ggplot() +
geom_density(aes(x = time, fill = condition), alpha = .8) + # you need to EDIT this line
theme_dark()
#| eval: false
#| include: false
# distribution of times by condition
d %>%
ggplot() +
geom_histogram(aes(x = avg_time), fill = "pink", colour = "purple") +
theme_classic()
#| eval: false
#| include: false
# filter out the high values
d_f <- # create a new object (or overwrite)
d %>% # original data object
filter(avg_time < 10)
# draw a new
d_f %>%
ggplot() +
geom_histogram(aes(x = avg_time),
fill = "yellow",
colour = "black",
bins = 20) +
theme_minimal()
#| eval: false
#| include: false
d_f %>%
group_by(condition) %>% # you need to EDIT this for Q1
summarise(stroop_mean = mean(time)) # you need to EDIT this for Q1
#| eval: false
#| include: false
# use filter to select two levels of the IV - Q3-5
stroop_compatible <-
d_f %>% # Edit this for the name of YOUR data object
filter(condition == "compatible")
# use filter to select two levels of the IV - Q3-5
stroop_control <-
d_f %>% # Edit this for the name of YOUR data object
filter(condition == "control")
# use filter to select two levels of the IV - Q3-5
stroop_incompatible <-
d_f %>% # Edit this for the name of YOUR data object
filter(condition == "incompatible")
# run the t-test comparing the means of these two levels
t.test(x = stroop_compatible$time, y = stroop_control$time, paired = TRUE)
t.test(x = stroop_incompatible$time, y = stroop_control$time, paired = TRUE)
t.test(x = stroop_compatible$time, y = stroop_incompatible$time, paired = TRUE)
#| eval: false
#| include: false
stroop_summary <-
d_f %>%
group_by(condition) %>% # you need to EDIT this for Q2
summarise(stroop_mean = mean(time),
stroop_SE = sd(time)/sqrt(n())) # you need to EDIT this for Q1
stroop_summary
#| eval: false
#| include: false
# let's first plot the means
stroop_summary %>%
ggplot(aes(x = condition, y = stroop_mean)) + # map variables to x and y for Q5
geom_point(size = 5) +
geom_errorbar(aes(ymin = stroop_mean - stroop_SE, # edit this for Q5
ymax = stroop_mean + stroop_SE), # edit this for Q5
width = .2)
#| eval: false
#| include: false
# let's first plot the means
stroop_summary %>%
ggplot(aes(x = condition, y = stroop_mean)) + # map variables to x and y for Q5
geom_col(size = 5) +
geom_errorbar(aes(ymin = stroop_mean - stroop_SE, # edit this for Q5
ymax = stroop_mean + stroop_SE), # edit this for Q5
width = .2)
setwd("D:/Repos/Research/year1/PSYC121")
#| eval: false
#| include: false
# filter out the high values
d_f <- # create a new object (or overwrite)
d %>% # original data object
filter(avg_time < 10)
#| eval: false
#| include: false
library(tidyverse)
d_raw <- read_csv("data/Week_6/PSYC121_survey_data.csv")
# set up the data for Task 3
d <-
d_raw %>%
select("stroop_control" = Q23,
"stroop_compatible" = Q24,
"stroop_incompatible" = Q25) %>%
slice(3:n()) %>%
mutate(across(contains("stroop"), as.numeric)) %>%
drop_na() %>%
mutate(pID = 1:n()) %>%
pivot_longer(cols = contains("stroop"),
names_to = "condition",
names_prefix = "stroop_",
values_to = "time") %>%
mutate(time = round(time,2)) %>%
group_by(pID) %>%
mutate(avg_time = round(mean(time),2))
write_csv(d, file = "data/Week_8/stroop_wk8_2024.csv")
#| eval: false
#| include: false
# filter out the high values
d_f <- # create a new object (or overwrite)
d %>% # original data object
filter(avg_time < 10)
# draw a new
d_f %>%
ggplot() +
geom_histogram(aes(x = avg_time),
fill = "yellow",
colour = "black",
bins = 20) +
theme_minimal()
#| eval: false
#| include: false
d_f %>%
group_by(condition) %>% # you need to EDIT this for Q1
summarise(stroop_mean = mean(time)) # you need to EDIT this for Q1
setwd("D:/Repos/Research/year1/PSYC121")
# Chunk 1: setup
library(tidyverse)
library(learnr)
library(kableExtra)
load("tidy_data.RData")
load("tidy_data.RData")
setwd("D:/Repos/Research/year1/PSYC121/data/Week_9")
library(tidyverse)
library(learnr)
library(kableExtra)
load("tidy_data.RData")
data <-
data %>%
mutate(z_stroop = scale(avg_stroop))
View(data)
data <-
data %>%
rowwise() %>%
mutate(avg_stroop = round(mean(c(stroop_control,stroop_compatible,stroop_incompatible), na.rm = TRUE),2),
across(facebook_days:twitter_days, ~str_sub(.x, 1,1))) %>%
ungroup() %>%
select(facebook_days, instagram_days, twitter_days, avg_stroop) %>%
drop_na()
data <-
data %>%
mutate(z_stroop = scale(avg_stroop))
data
# because we are assigning (<-) the changes to update "data",
# we write it again here to display the results
quarto::quarto_version()
library(tidyverse)
setwd("D:/Repos/Research/year1/PSYC121/data/Week_9")
data_wk9 <- read_csv("data/Week_9/data_wk9.csv")
d_stroop <- read_csv("data/Week_9/data_stroop_wk9.csv")
#| eval: false
#| include: false
summary(data_wk9)
#| eval: false
#| include: false
data_wk9 %>%
ggplot(aes(x = pop_est_muslim, y = pop_est_immigrants)) + # map two of the columns to x and y
geom_point() # you can change the size or colour of the points if you wish
#| eval: false
#| include: false
# use mutate and scale to create z-scores of immigration estimates
data_wk9_z <-
data_wk9 %>%
mutate(z_imm = scale(pop_est_immigrants))
data_wk9
data_wk9 %>%
ggplot()
data_wk9 %>%
ggplot() +
geom_point(aes(x = pop_est_muslim, y = pop_est_immigrants))
data_wk9 %>%
ggplot(aes(x = pop_est_muslim, y = pop_est_immigrants)) +
geom_point()
data_wk9 %>%
ggplot(aes(x = pop_est_muslim, y = pop_est_immigrants)) + # map two of the columns to x and y
geom_point() # you can change the size or colour of the points if you wish
data_wk9 %>%
ggplot(aes(x = pop_est_muslim, y = pop_est_immigrants)) + # map two of the columns to x and y
geom_point() # you can change the size or colour of the points if you wish
# check the mean and sd of the new column
mean(data_object_name$column_name)
#| eval: false
#| include: false
# check the mean of the new column
mean(data_wk9_z$z_imm)
sd(data_wk9_z$z_imm)
# check the mean of the new column
mean(data_wk9_z$z_imm)
sd(data_wk9_z$z_imm)
#| eval: false
#| include: false
data_wk9_z %>%
ggplot() +
geom_histogram(aes(x = z_imm),
bins = 40)
data_wk9_z %>%
ggplot() +
geom_histogram(aes(x = z_imm),
bins = 40)
#| eval: false
#| include: false
data_wk9_z %>%
ggplot() +
geom_histogram(aes(x = z_imm),
bins = 40)
View(data_wk9_z)
# use mutate and scale to create z-scores of immigration estimates
data_wk9_z <-
data_wk9 %>%
mutate(z_imm = scale(pop_est_immigrants)[1])
# use mutate and scale to create z-scores of immigration estimates
data_wk9_z <-
data_wk9 %>%
mutate(z_imm = scale(pop_est_immigrants)[])
# use mutate and scale to create z-scores of immigration estimates
data_wk9_z <-
data_wk9 %>%
mutate(z_imm = scale(pop_est_immigrants)[1])
#| eval: false
#| include: false
# add a filter command
data_wk9_f <-
data_wk9_z %>%
filter(z_imm > -2.5 & z_imm < 2.5)
# use mutate and scale to create z-scores of immigration estimates
new_data_object_name <-
data_object_name %>%
mutate(z_imm = scale(column_name)[])
# use mutate and scale to create z-scores of immigration estimates
data_wk9_z <-
data_wk9 %>%
mutate(z_imm = scale(pop_est_immigrants)[])
# use mutate and scale to create z-scores of immigration estimates
data_wk9_z <-
data_wk9 %>%
mutate(z_imm = scale(pop_est_immigrants)[1])
# use mutate and scale to create z-scores of immigration estimates
data_wk9_z <-
data_wk9 %>%
mutate(z_imm = scale(pop_est_immigrants)[,1])
# add a filter command
data_wk9_f <-
data_wk9_z %>%
filter(z_imm > -2.5 & z_imm < 2.5)
#| eval: false
#| include: false
# summary statistics
data_wk9_f %>%
group_by(home_location_in_UK) %>%
summarise(mean_imm_est = mean(pop_est_immigrants),
sample_size = n())
#| eval: false
# check if variances are unequal (p < .05) - Q3
var.test(pop_est_immigrants ~ home_location_in_UK, data = your_data_object_name)
#| eval: false
# run unrelated samples t-test
t.test(pop_est_immigrants ~ home_location_in_UK, data = your_data_object_name,
paired = FALSE,
var.equal = missing_value) # you'll need to set this to TRUE or FALSE depending on what you found in var.test
#| eval: false
# run unrelated samples t-test
t.test(pop_est_immigrants ~ home_location_in_UK, data = your_data_object_name,
var.equal = missing_value) # you'll need to set this to TRUE or FALSE depending on what you found in var.test
# check if variances are unequal (p < .05) - Q3
var.test(pop_est_immigrants ~ home_location_in_UK, data = data_wk9_f)
#| eval: false
#| include: false
# check if variances are unequal (p < .05) - Q3
var.test(pop_est_immigrants ~ home_location_in_UK, data = data_wk9_f)
# run unrelated samples t-test
t.test(pop_est_immigrants ~ home_location_in_UK,
data = data_wk9_f,
paired = FALSE,
var.equal = TRUE) # you'll need to set this to TRUE or FALSE depending on what you found in var.test
#| eval: false
#| include: false
# check if variances are unequal (p < .05) - Q3
var.test(pop_est_immigrants ~ home_location_in_UK, data = data_wk9_f)
# run unrelated samples t-test
t.test(pop_est_immigrants ~ home_location_in_UK,
data = data_wk9_f,
var.equal = TRUE) # you'll need to set this to TRUE or FALSE depending on what you found in var.test
#| eval: false
#| include: false
# check if variances are unequal (p < .05) - Q3
var.test(pop_est_christian ~ home_location_in_UK, data = data_wk9_f)
# run unrelated samples t-test
t.test(pop_est_christian ~ home_location_in_UK,
data = data_wk9_f,
var.equal = TRUE) # you'll need to set this to TRUE or FALSE depending on what you found in var.test
#| eval: false
#| include: false
# check if variances are unequal (p < .05) - Q3
var.test(pop_est_muslim ~ home_location_in_UK, data = data_wk9_f)
# run unrelated samples t-test
t.test(pop_est_muslim ~ home_location_in_UK,
data = data_wk9_f,
var.equal = TRUE) # you'll need to set this to TRUE or FALSE depending on what you found in var.test
#| eval: false
#| include: false
# check if variances are unequal (p < .05) - Q3
var.test(pop_est_age ~ home_location_in_UK, data = data_wk9_f)
# run unrelated samples t-test
t.test(pop_est_age ~ home_location_in_UK,
data = data_wk9_f,
var.equal = TRUE) # you'll need to set this to TRUE or FALSE depending on what you found in var.test
#| eval: false
#| include: false
library(effectsize)
library(pwr)
cohens_d(time ~ condition,
data = d_stroop,
paired = TRUE)
compatible_condition <-
d_stroop %>%
filter(condition == "compatible")
incompatible_condition <-
d_stroop %>%
filter(condition == "incompatible")
cohens_d(x = compatible_condition,
y = incompatible_condition
paired = TRUE)
cohens_d(x = compatible_condition,
y = incompatible_condition,
paired = TRUE)
cohens_d(x = compatible_condition$time,
y = incompatible_condition$time,
paired = TRUE)
pwr.t.test(d = 1.27, n = 20) #Q4
pwr.t.test(d = 1.27, power = .8) #Q4
pwr.t.test(d = 1.27, power = .8, type = paired) #Q5
pwr.t.test(d = value_from_step_3, n = 20, type = "paired-samples") #Q4
pwr.t.test(d = 1.27, power = .8, type = "paired-samples") #Q5
pwr.t.test(d = 1.27, power = .8, type = "paired") #Q5
pwr.t.test(d = 1.27, power = .8, type = "paired") #Q4
pwr.t.test(d = 1.27, n = 20, type = "paired") #Q4
pwr.t.test(d = 1.27, power = .8, type = "paired") #Q4
pwr.t.test(power = .8, n = 40, type = "paired") #Q5
quarto render
quarto render
install.packages("rmarkdown")
quarto render
install.packages("shiny")
install.packages("tidyverse")
install.packages("learnr")
install.packages("kableExtra")
install.packages("car")
install.packages("lsr")
install.packages("Hmisc")
install.packages("ggeffects")
install.packages("patchwork")
knitr::opts_chunk$set(echo = TRUE)
library(broom)
library(tidyverse)
setwd("~/year1/PSYC122/data/week11")
mh <- read_csv("MillderHadenData.csv")
setwd("~/year1/PSYC122/data/week11")
mh <- read_csv("MillderHadenData.csv")
setwd("~/year1/PSYC122/data/week11")
mh <- read_csv("MillderHadenData.csv")
download.file("https://github.com/lu-psy-r/year1/blob/main/PSYC122/data/week11/MillerHadenData.csv?raw=true", destfile = "MillerHadenData.csv")
mh <- read_csv("MillderHadenData.csv")
mh <- read_csv("MillerHadenData.csv")
ggplot (mh, aes(x="TV", y="Home")) +
geom_point()
scatterplot <- ggplot (mh, aes(x="TV", y="Home")) +
geom_point()
library(broom)
library(tidyverse)
download.file("https://github.com/lu-psy-r/year1/blob/main/PSYC122/data/week11/MillerHadenData.csv?raw=true", destfile = "MillerHadenData.csv")
mh <- read_csv("MillerHadenData.csv")
ggplot (mh, aes(x="TV", y="Home")) +
geom_point()
scatterplot
library(broom)
library(tidyverse)
download.file("https://github.com/lu-psy-r/year1/blob/main/PSYC122/data/week11/MillerHadenData.csv?raw=true", destfile = "MillerHadenData.csv")
mh <- read_csv("MillerHadenData.csv")
ggplot (mh, aes(x="TV", y="Home")) +
geom_point()
mh
ggplot (mh, aes(x=TV, y=Home)) +
geom_point()
mh <- read_csv("MillerHadenData.csv")
mh #just to have a quick look at the data
ggplot (mh, aes (x = TV, y = Home)) +
geom_point()
ggplot (mh, aes (x = TV, y = Home)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
ggplot (mh, aes (x = TV, y = Home)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
theme_bw() +
labs(x = "Time spend reading at home", y = "Time spend watching TV at home")
results <- cor.test(mh$Home,
mh$TV,
method = "pearson",
alternative = "two.sided") %>%
tidy()
results
pull(results, estimate)
r <- pull(results, estimate)
p <- pull(results, p.value)
df <- pull(results, parameter)
r <- pull(results, round(estimate,2)
r <- pull(results, round(estimate,2))
r <- results %>%
pull(estimate) %>%
round(2)
p <- results %>%
pull(p.value) %>%
round(3)
results
df <- results %>%
pull(parameter)
r-squared <- squared(r)
r-squared <- r*r
rsquared <- r*r
rsquared <- (r*r)*100
data <- read_csv("alcoholUse_Impulsivity.csv")
data
ggplot(data, aes(x = hau, y = imp)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
theme_bw() +
labs(x = "Hazardous Alcohol Use", y = "Impulsivity")
results <- cor.test(data$hau,
data$imp,
method = "pearson",
alternative = "two.sided") %>%
tidy()
results
r <- results %>%
pull(estimate) %>%
round(2)
df <- results %>%
pull(parameter)
pvalue <- results %>%
pull(p.value) %>%
round(3)
rsquared <- r*r
rsquaredPercent <- round(rsquared * 100, 0)
?broom
---
title: "122_wk12_labActivity2"
---
title: "122_wk12_labActivity2"
knitr::opts_chunk$set(echo = TRUE)
library(broom)
library(car)
library(lsr)
library(Hmisc)
library(tidyverse)
dat <- read_csv("VapingData.csv")
dat <- dat %>%
filter(IAT_BLOCK3_Acc < 1) %>%                            # 1) only keep participants with accuracy scores equal to or smaller than 1 from block 3
filter(IAT_BLOCK5_Acc < 1) %>%                            # 1) only keep participants with accuracy scores equal to or smaller than 1 from block 5
mutate(IAT_ACC = (IAT_BLOCK3_Acc + IAT_BLOCK5_Acc)/2) %>% # 2) calculate an average IAT accuracy score across blocks 3 and 5
filter(IAT_ACC > .8) %>%                                  # 2) only keep participants with an average score greater than .8.
mutate(IAT_RT = IAT_BLOCK5_RT - IAT_BLOCK3_RT)            # 3) compute the IAT_RT score
descriptives <- dat %>%
summarise(n = n(),
mean_IAT_ACC = mean(IAT_ACC),
mean_IAT_RT = mean(IAT_RT),
mean_VPQ = mean(VapingQuestionnaireScore, na.rm = TRUE))
descriptives
dat <- dat %>%
filter(!is.na(VapingQuestionnaireScore)) %>%
filter(!is.na(IAT_RT))
ggplot(dat, aes(x = VapingQuestionnaireScore)) +
geom_histogram(binwidth = 10) +
theme_bw()
qqPlot(x = dat$VapingQuestionnaireScore)
ggplot(dat, aes(x = IAT_RT)) +
geom_histogram(binwidth = 10) +
theme_bw()
qqPlot(x = dat$IAT_RT)
ggplot(dat, aes(x = IAT_RT, y = VapingQuestionnaireScore)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
theme_bw() +
labs(x = "Implicit attitude", y = "Explicit attitude")
results <- cor.test(dat$VapingQuestionnaireScore,
dat$IAT_RT,
method = "pearson") %>%
tidy()
results
correlation <- results %>%
pull(estimate) %>%
round(2)
df <- results %>%
pull(parameter) %>%
round(0)
pvalue <- results %>%
pull(p.value) %>%
round(3)
dat_matrix <- dat %>%
select(Age, IAT_RT, VapingQuestionnaireScore) %>%
as.data.frame() # Make sure tell R that dat is a data frame
pairs(dat_matrix)
intercor_results <- correlate(x = dat_matrix, # our data
test = TRUE, # compute p-values
corr.method = "spearman", # run a spearman test
p.adjust.method = "bonferroni") # use the bonferroni correction
intercor_results
