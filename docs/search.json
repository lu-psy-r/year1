[
  {
    "objectID": "PSYC121/Week4.html",
    "href": "PSYC121/Week4.html",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "",
    "text": "Watch Part 1\n\n\nWatch Part 2\n\n\nWatch Lecture Part 3\n\n\nWatch Lecture Part 4\n\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#customisation-of-data-plots",
    "href": "PSYC121/Week4.html#customisation-of-data-plots",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "3.1 - Customisation of data plots",
    "text": "3.1 - Customisation of data plots\nStep 1. Set up a folder for Week 4 and set the working directory.\nStep 2. Bring the week_4_2025.zip file into R Studio server. Like last week, upload the zip file. Launch the week_4 R script.\nIf you’ve done Step 1 & 2 already as a lab preparation, super! Pat yourself on the back, skip these steps, and move on…\nStep 3. Once again, we’re gong to be using several commands from the tidyverse library (the pipe operator is one example) so we need to ensure that it’s active. Run the command\nlibrary(tidyverse)\nStep 4. Read in the “wages.csv” data. There’s already a read_csv() script line for this, you just need to change the file name (like we’ve done in previous weeks).\nStep 5. Customize you graph work. We’ve provided some suggestions about adding titles and labels for your graph. Edit and play with the script lines to make them useful to you and to understand how they work. Note that the ggplot instructions have a similar structure / grammar to the group_by() instructions that we used: piping a data frame to a (here, plotting) function and piping that to an output or summarisation format.\n\nTry change the text, the colours, and so on of the graphs.\nAdd comments for yourself about what the different commands do. The idea is to learn by trying different things out (changing values, taking out elements of the command, putting other is) and record for yourself.\nIf you are struggling or not sure, try look at help files.",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#z-scores",
    "href": "PSYC121/Week4.html#z-scores",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "z-scores",
    "text": "z-scores\n\nHint / Reminder: Sketch a normal (z score) distribution and mark the mean/mode, and mark off the relevant parts of the question so you know what you are trying to achieve and how to interpret any calculations you make.\n\n\nHint/ Guide 2. For questions 6 & 7, typically in psychology we use the 5% level as a cutoff to decide, in broadly described terms, whether something is extreme or unlikely vs. at least somewhat plausible or likely.\n\n\nz-scores basics\nz-score distributions\nQ1. What is the relationship between the sign of a z-score and its position in a distribution?\nQ2. If a distribution has a mean of 100 and a standard deviation of 10, what is the raw score equivalent to a z-score of 1.96?\nQ3. If a distribution has a mean of 157 and a standard deviation of 19, what is the raw score equivalent to a z-score of 1?\n\n\nUsing z-score tables\nQ4. What proportion of scores lie between the mean and a z-score of 0.5?\nQ5. What is the combined proportion of scores lying between z=-1.2 and z=.85?\n\n\nApplying z-scores to inferential problems\nQ6. A Neuropsychologist has presented a test of face recognition to 200 neurotypical participants and finds that the scores are normally distributed with a mean of 85 and the standard deviation of 12. Two brain-damaged patients are also given the test. The one with right hemisphere brain damage scored 58 and the one with left hemisphere damage scored 67.\n\nWhat is the z score of the right hemisphere patient when compared to the neurotypical group?\nWhat proportion of neurotypical participants score lower than this patient?\nIs this patient likely to belong to the population of neurotypical participants? (justify your answer)\nWhat is the z score of the left hemisphere patient when compared to the neurotypical group?\nWhat proportion of neurotypical participants score lower than this patient?\nIs this patient likely to belong to the population of neurotypical participants? (justify your answer)\n\n\n\nFinal z-score challenge\nCome back to this afterwards for some extra practice if you want:\nQ7. Tom Bunion has completed a huge research study and measured the foot size of men and women and found each to be normally distributed. The men have a mean size of 55 with a standard deviation of 5 and the women a mean of 33 and a standard deviation of 5. Joanna Toes has foolishly measured two individuals but forgotten to note their gender. These have foot sizes of 37 and 47. To which gender is each more likely to belong? What evidence is there for this?",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#week-4-lecture",
    "href": "PSYC121/Week4.html#week-4-lecture",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "",
    "text": "Watch Part 1\n\n\nWatch Part 2\n\n\nWatch Lecture Part 3\n\n\nWatch Lecture Part 4\n\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#pre-lab-work",
    "href": "PSYC121/Week4.html#pre-lab-work",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "Pre-lab work",
    "text": "Pre-lab work\n\nComplete materials from sessions in previous week. Consolidate what we have already covered.\nThis week - there’s a new learnr tutorial to follow and help prep for what we are covering: You can find it here.\n\nMake sure you download the zip file for the RStudio tasks.\nCreate a new folder and upload the file into RStudio, ideally before the lab class.",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#r-studio-tasks",
    "href": "PSYC121/Week4.html#r-studio-tasks",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "R Studio tasks",
    "text": "R Studio tasks\nLast week we introduced two different ways to get descriptive information about a numerical variable (column) as a function of a categorical variable. We discussed how this was extremely powerful, as we can look at the effect of an IV on the DV, which gets us a long way in the analysis of our psychological experiments!\nIn the labs students were showing great skills in utilising each of these tools:\naggregate(x = DV, by = list(IV), FUN = mean)\nand\ndataframe %&gt;% group_by(IV) %&gt;% summarise(label_header = mean(DV))\nThis week we’ll first look at some more features of our powerful graphing tool, ggplot().\n\nCustomisation of data plots\nStep 1. Set up a folder for Week 4 and set the working directory.\nStep 2. Bring the week_4_2025.zip file into R Studio server. Like last week, upload the zip file. Launch the week_4 R script. This will give you 3 files.\nIf you’ve done Step 1 & 2 already as a lab preparation, super! Pat yourself on the back, skip these steps, and move on…\nStep 3. Once again, we’re gong to be using several commands from the tidyverse library (the pipe operator is one example) so we need to ensure that it’s active. Run the command\nlibrary(tidyverse)\nStep 4. Read in the “wages.csv” data. There’s already a read_csv() script line for this, you just need to change the file name and the object name (like we’ve done in previous weeks).\nStep 5. Draw a graph of the screen time data, with the phone type on the x axis and the usage data on the y axis. You’ve done this last week, so it should be straightforward.\nStep 6. Customize you graph work. We’ve provided some suggestions about adding titles and labels for your graph. Edit and play with the script lines to make them useful to you and to understand how they work. Note how we are using the piping command, %&gt;%, to send the data into the ggplot() commands. When we build up layers of the graph we + each layer to the graph (we’ll learn more about this in future weeks).\n\nTry to change the text, the colours, and so on of the graphs.\nAdd comments for yourself about what the different commands do. The idea is to learn by trying different things out (changing values, taking out elements of the command, putting other is) and record for yourself.\n\nStep 7. Let’s add a boxplot over the top of our violin plots. The code for a boxplot is geom_boxplot() and remember that you’ll need to add a + to the line before to add this new layer.\nStep 8. Let’s use the next block of code to draw a simple histogram of the salary estimates. You’ll just need to add the object name to do this.\nStep 9. We’ll now use a new tool to split the data up by the “family_position” variable. This technique is called “faceting”. facet_wrap() makes a long ribbon of panels according to the variable(s) you specify. This is useful if you have a single variable with many levels and want to arrange the plots in a more space efficient manner.\n\n# remember to add a + to the end of the graph command\nfacet_wrap(vars(family_position))\n\nYou should have a plot with 4 different graphs, each showing a histogram for each value in the family position variable.\nStep 10. We’ve included the screentime.csv data again this week and given you some code to work with. Try out some of the techniques you’ve learnt in this week’s tutorial as well as practicing the technqiues you’ve learnt in previous weeks. You’ll see we’ve included a new type of plot: geom_density() which is another great way to plot distributions of data. As always, make sure you add comments to your ode.",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#rstudio-and-stats-humour",
    "href": "PSYC121/Week4.html#rstudio-and-stats-humour",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "RStudio and stats humour",
    "text": "RStudio and stats humour\nFor a bit of fun… The following are parody music videos about stats. Now that you have a few weeks’ experience with R Studio and also, an introduction to hypothesis testing, you might appreciate the following\nThe R Inferno Song (Teenage Dirtbag Parody) filmed largely on campus at Maynooth University, Ireland with stats students and staff:\n\nHypothesis testing and p values (plus bunny rabbits and a dog)",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#extra-external-r-resources",
    "href": "PSYC121/Week4.html#extra-external-r-resources",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "Extra external R resources",
    "text": "Extra external R resources\nSome students have asked for a pointer to additional R resources so they can structure some time exploring the R system. There are lots, but this is good and very compatible with the teaching we provide: R for data science",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html",
    "href": "PSYC121/Week2.html",
    "title": "2. Descriptive statistics in RStudio",
    "section": "",
    "text": "Below you will see the embedded lecture videos. As a reminder these lectures in PSYC121 are designed to provide an explanation of the conceptual and computational work involved in some core analytic material. The lab classes are designed (a) to put these ideas into practice (b) to work with R Studio in practicing with data, as a tool that helps you to explore data. So the lectures and lab complement each other, and we will continue to tackle this approach across the module.\nWatch Part 1\n\n\nWatch Part 2\n\n\nWatch Part 3\n\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#week-2-lecture",
    "href": "PSYC121/Week2.html#week-2-lecture",
    "title": "2. Descriptive statistics in RStudio",
    "section": "",
    "text": "Below you will see the embedded lecture videos. As a reminder these lectures in PSYC121 are designed to provide an explanation of the conceptual and computational work involved in some core analytic material. The lab classes are designed (a) to put these ideas into practice (b) to work with R Studio in practicing with data, as a tool that helps you to explore data. So the lectures and lab complement each other, and we will continue to tackle this approach across the module.\nWatch Part 1\n\n\nWatch Part 2\n\n\nWatch Part 3\n\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#pre-lab-work",
    "href": "PSYC121/Week2.html#pre-lab-work",
    "title": "2. Descriptive statistics in RStudio",
    "section": "Pre-lab work",
    "text": "Pre-lab work\nLast week, among other things, we asked you to\n\nRoll some dice, carry out some (relatively) straightforward ‘hand’ calculations of central tendency\nConnect to the RStudio server, create a folder and get used to the different “panes”\nWithin RStudio upload and run a script (a set of instructions), and explore annotations\nAdapt the script commands to perform calculations on the dice rolls within RStudio\nComplete a survey so that we can collect data for analysis teaching\n\nYour progress was great! We start with small steps and build up - but this is a nice start and as staff we’re pleased how things went!\nBefore the lab, make sure you have worked through the material in the week2 learnr tutorial. The link is here",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#lab-activities",
    "href": "PSYC121/Week2.html#lab-activities",
    "title": "2. Descriptive statistics in RStudio",
    "section": "Lab activities",
    "text": "Lab activities\n\nR Studio recap and setup\nFor a reminder of how to start RStudio, see week 1’s instructions\n(remember: off campus, you will need to be on the VPN)\n\nA word of advice (from David Howell’s statistics book: One more word of advice I can’t resist adding what is perhaps the best advice I have. If there is something that you don’t understand, just remember that “Google is your friend.” She certainly is mine. (Well, maybe Google is getting a bit pushy, but there are many other search sites.) If you don’t understand what Fisher’s Exact Test is, or you don’t like my explanation, go to Google and type in Fisher’s Exact Test. I just did that and had 260,000 hits. You can’t tell me that there isn’t going to be something useful in there.)\n\nIn week 1, we had a tiny dataset (relatively speaking) that we entered into R through a script line. That worked for what is was. But it’s going to become painful and tedious when (a) we want to work with larger datasets (b) we have data more complex than a 1-dimensional list of numbers (think about some 2-dimensional data sheets you might have encountered in excel for example)\nR can handle data files, and this week we’re going to explore them. Within R, we can specify ‘data frames’ which can have, essentially, multiple columns of data, and we can link data files to data frames for processing\nTo make things straightforward, each week we’ll provide students with a “zip” file that contains the script to start from (which you can expand and annotate etc, and save on your file space). We’ll also provide a data file(s) for you to use in the zip file. R can then import these files into the RStudio environment. So when you upload the zip file, you can import the data AND you can open up the script\n\n\nWorking with multiple columns\nSome years ago, a large group of participants gave an estimate of the weight of Penelope the cow. Just over 17,000 guesses. And the distribution of guesses was something like this: \nWhat we can see from this graph is that:\n\nGuesses formed a roughly normal distribution. There is a bit of a skew with a right-hand tail, but this is inevitable as a weight of less than 0 is physically impossible, but there is no limit of the semantics of a large guess.\nThe mean guess weight (1,287 lbs) is very close to the actual (true) weight of the cow (1,355 lbs). So even though lots of people were inaccurate, a central tendency measure has a pretty good alignment with the true weight. This is known as the Wisdom of Crowds phenomenon, first identified by Galton in 1907 (though he suggested using the median weight). The concept of the wisdom of crowds continues to be used and investigated in psychology today (see for example here and here)\n\nLet’s look at (a sample of) the PSYC121 student data collected on guessing the weight of Penelope, and ask whether it resembles the properties of this large dataset.\n\n\nCreate a folder and set the “working directory”\nLast week you should have created a “PSYC121” folder. This week, we’ll organise our work a little more. Follow these steps to organise your scripts and data:\n\nWithin the PSYC121 folder, create a “Week_1” folder and a “Week_2” folder (you don’t have to use the underscores, but it’s good practice)\nMove the Week 1 script (the .R file) into the Week 1 folder. To do this, tick the box next to the Week 1 R file, select “More” and then “Move”. Find the Week 1 folder and select it.\nNavigate to the Week 2 folder and set it as the “working directory”. To do this, click “More” and “Set as Working Directory”. What does this mean? Well, RStudio is a bit unusual, in that you need to tell it where you want to work. So even if you navigate to a different directory, RStudio doesn’t assume that’s where it should look for files. So when you’re sure you’re in the correct folder you want to work in, make sure you Set as Working Directory.\nDownload and then upload the zip file into RStudio. You’ll remember from last week that we need to download the zip file from this page and then upload it into [RStudio]. This week we’re using a file named week_2_2025.zip. Click on this link to download the zip file. Follow the same process as last week to achieve this step.\n\n\n\nLibrary() and read_csv()\nLet’s start working with our data, by opening up (clicking on) the script “Week_2.R” file.\nThe first command is to load a library of functions:\n\nlibrary(tidyverse)\n\nTo run this, simply click anywhere on line 1 of the R script to put the cursor there, and press ctrl+enter (cmd+enter on a mac) or click the button called run. You will see a number of messages appear in the console. Don’t worry about these, or worry too much about what exactly this command is doing. Essentially this is giving us some useful tools for our analysis. We will introduce the features of the tidyverse gradually during this course.\nIf you have followed all the instructions to this point, you should see a “csv” file in the Week 2 folder. “csv” is a filetype that means “comma separated values” - it’s a very typical way to store data in very small files. While the data exist in this file, we actually need to get them into an R object so we can explore the data on the RStudio server. To do this, run the next line of code:\n\ncows &lt;- read_csv(\"penelope22.csv\")\n\nWhat this command accomplished was to read the data file called ‘penelope22.csv’ into an object in R called cows. You could use any object label - it doesn’t have to be ‘cows’- but it’s important to then keep that alternate name consistent in what you do next.\nWe can use the following command to view the data in the spreadsheet format:\n\nView(cows)\n\nThis presents the data in a window of RStudio. As you look through the data, note that “NA” means not available or missing data. Does this file structure start to make some sense to you?\n\n\nFinding the mean and median estimates\nUse the data to answer the following questions…\n\nWhat is the mean weight estimates?\nWhat is the standard deviation of the estimates?\nWhat is the median weight of the estimates?\nWhich of these central tendency measures is the more accurate measure of the true cow weight? (make a judgement)\nWhat is the mean weight estimate (and standard deviation) for female respondents and non-female (male / non-binary /prefer not to say) respondents?\n\nYou may be thinking, how do I possibly do any of this?! Well this week most of the commands you need are contained in the R script you have downloaded. Also, remember from last week, we explored the R command:\n\nmean(week_1_lecture_data)\n\nThat gave us the mean of the small dataset week_1_lecture_data. This time, we want to explore the penelope dataset. But also, the lecture_data was just a single list of numbers. The penelope22 object is a 2D datasheet: it has rows and columns. So we need to tell R Studio which column we are interested in. RStudio uses the format object_name$column_name. So run the following lines in the script:\n\nmean(cows$estimate) \n\n\nsd(cows$estimate)\n\nSo from this, can you work out what you would do to get the median value? Part of the command is given to you, can you change the text so that it works?\n\n\nCalculations from a range of columns\nWe have seen that:\n\nmean(cows$estimate) \n\nwill provide a mean of the column “estimate”. In the third column, named “female_estimate”, we have the estimates of just the female respondents. In the fourth column, named “other_estimate”, we have the estimates of the “other” respondents (i.e., males and non-binary and those that prefer not to say).\nCan you now figure out how you might get information about the estimate from the female data (only) or the non-female data? Try this out and compare with others on your table to check you are getting the same answers. You will certainly find that the result of the this command produces an “NA” result. This means that the answer is “Not Available”, or in other words, is a “missing value”. This is because some of the values in this column are NA, and the mean of a column with NAs will always lead to the result NA.\nBut thankfully there is a solution. Try changing the script so it looks like this:\n\nmean(cows$female_estimate, na.rm = TRUE )\n\nAny different? The na.rm = TRUE instruction tells RStudio that missing data can be ignored in this mean calculation. (in technical language, na.rm is a parameter of the function mean that removes the NAs if set to TRUE)\n\n\nSimple graphs\nRStudio can be used to create graphical data plots that can help interpret datasets\nThe first thing we can do is create a histogram distribution of guesses from the sample student data to compare with the previous large sample study (i.e. the 17,000 guesses):\n\nhist(cows$estimate)\n\nOne way to alter or adjust the histogram is to change the width of the bars, the intervals, between each plot section. Try run this line from the script\n\nhist(cows$estimate, breaks = *MISSING*)\n\nDoes it work? No? What you need to do is replace the two question marks in the script (or better still, create a new instruction line in which you amend this to have a numerical value representing the number of different plot bars. Try at least 3 different values. Look at and think about how this affects the visual distribution.\nWe can also create a “box and whisker plot”. Here’s a general simple description of a box-and-whisker plot as a graphical representation of data:\n\n\n\nExtension and practice. Apply your new skils to a different dataset\nIn the zip file, we also provide data on the estimates of the percentage of immigrants in the UK. This will allow you to explore this variable, create visualisations of the data and its spread. We’ll be looking at a version of this variable in week 3: but for now, can you apply the analysis of the penelope data to the immigration data (report descriptive statistics)? Write some new script lines to investigate this additional dataset, and annotate those new script lines.",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#data-collection-exercise",
    "href": "PSYC121/Week2.html#data-collection-exercise",
    "title": "2. Descriptive statistics in RStudio",
    "section": "Data collection exercise",
    "text": "Data collection exercise\nIn order to learn about psychology and data analysis techniques, we need data! Rather than rely too much on artificial data (certainly it is sometimes useful to say “Here are a bunch of numbers and this is what we can do with them” – think about the R Studio example for this week’s lab) for the most part, we prefer to draw on datasets that are a bit more engaging and meaningful that you have a stake in yourself! By using a common data set, that we can return to over the year, we can also build up familiarity and confidence in the data and remove a potential obstacle to thinking about the more important analysis part.\nSo a key task will be for everyone to have a go at taking our online survey, and contribute to a dataset that can be used throughout the year.\nThe survey runs by following this link",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#post---lab-recap-the-slides-we-used",
    "href": "PSYC121/Week2.html#post---lab-recap-the-slides-we-used",
    "title": "2. Descriptive statistics in RStudio",
    "section": "Post - lab recap: The slides we used",
    "text": "Post - lab recap: The slides we used\nWant to see again the introduction slides that we used in the Levy lab? They are available here",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/index.html",
    "href": "PSYC121/index.html",
    "title": "Statistics for Psychologists I",
    "section": "",
    "text": "Welcome\nWelcome to the PSYC121 lab material for 2025!\nIn this module we will provide you with an introduction to data handling, data processing and data visualisation. What that means is that, by the end of the this module (at Christmas time), you will be able to take a set of data, look at some basic statistics (e.g., the mean value), filter and process the data in order to answer basic questions about it, and present the data in an appealing way with different graphs. On top of this, you will be able to apply some of your knowledge of the basic “inferential” statistical tests that we will introduce in the lecture series (e.g., “t-test”).\nIn Week 1 we will introduce you to the software that we use to do all this useful work in statistics: “R” and “RStudio”. This is a coding language, and you will be taught the basics of how to write code in order to do all of the above key steps in data analysis. This tuition will continue in Term 2, and in your statistics modules in Year 2. Coding is challenging, but we know from experience that those students who attend classes, who work through the exercises carefully, and who seek help when they need it, do very well on these modules.\nMost of all, it’s important that you recognise that data analysis (statistics) is a critical aspect of the study of psychology. When we want to understand behaviour, we take measurements of that behaviour, which the majority of the time will result in quantitative (numerical) data. In order to understand the behaviour in a meaningful way, we need to conduct all of the above steps in our data analysis workflow (view data, conduct basic stats, draw graphs, etc). In summary, we cannot investigate psychological processes without the toolbox of statistics and data analysis techniques you will learn in PSYC121/122.\n\n\nWorking at your own pace and seeking help\nWe have carefully prepared and refined the lab materials in this course over several years, and we feel that the pace of the materials is just about right for our students. Some students will complete them more quickly, and others more slowly - both of these scenarios are absolutely fine. You should work at the pace that suits you best, making sure you understand the materials before you move forward.\nIt is fairly inevitable that you will get stuck on the lab materials in this module at some point. This might be in Week 1, Week 2, or later. When you do, it’s important you reach out for help:\n\nAsk your friends on your table. We’ve designed this teaching space to help collaborative work. You are encouraged to work with other students. Make sure you ask others to explain how they’ve solved an exercise. Make sure you help out others where you can. Always make sure you understand the code and the exercise; don’t simply be satisfied that you’ve got the right answer.\nAsk a GTA or Lecturer. Our Graduate Teaching Assistants are there to help you. There are no “stupid questions” in statistics, so just ask the GTAs any question about what you’re doing. Likewise, ask the Lecturer.\nAsk on the Discussion Forum. On the PSYC121 moodle page you will find a Discussion Forum. This is a great way to ask a question outside of the lab sessions. It might seem scary to ask a question in the forum, but please don’t be afraid to do this. If you have a question, you can bet 30+ other students also have the same question! So by asking the question on the forum, you help out many more people on the module. A friendly GTA or Lecturer will be along to answer the question as soon as possible (we aim for within 48 hours during the working week).\nAsk on the module Q & A session. Each week we hold a “Q&A” online session where we will try and resolve any general queries and problems. It’s an ideal time to discuss things that students are struggling with or confused about, and can share ideas and answers. You can ask on the discussion forum above and then we might be able to pick up the issues and discuss them, but also you can ask in the session itself.\n\nAsking good questions - it’s really important that you give us as much information as you can when you ask your questions (in class and especially on the forum). It’s so much harder to help respond to “I can’t do Exercise 5 in Week 7” (because we don’t know why it is that you can’t do it) than for example “In Exercise 5 of Week 7, I’ve managed to read in the data, put the graph looks quite odd. Here is the code I’m using…”\n\n\nCourse Contacts\nIf you have something that needs to be private, then please feel free to email the academic staff at the email addresses below:\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley (Coordinator)\nt.beesley at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html",
    "title": "PSYC121: Week 9 Lab",
    "section": "",
    "text": "For this week’s online tutorial we have data on how much students use social media platforms and people’s mean reading times from the stroop task. We will use these data to look at how we create new variables (columns) using mutate(). We will also expand what we have learnt about filtering data based on conditions, using the filter() function. Let’s take a look at the data we have using the summary(), to get useful statistics. You can also use head() to look at the first few rows of data\n\n\nsummary(data)\nhead(data)"
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html#exploring-the-data",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html#exploring-the-data",
    "title": "PSYC121: Week 9 Lab",
    "section": "",
    "text": "For this week’s online tutorial we have data on how much students use social media platforms and people’s mean reading times from the stroop task. We will use these data to look at how we create new variables (columns) using mutate(). We will also expand what we have learnt about filtering data based on conditions, using the filter() function. Let’s take a look at the data we have using the summary(), to get useful statistics. You can also use head() to look at the first few rows of data\n\n\nsummary(data)\nhead(data)"
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html#mutating-new-variables",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html#mutating-new-variables",
    "title": "PSYC121: Week 9 Lab",
    "section": "“Mutating” new variables",
    "text": "“Mutating” new variables\nIn this data set, all of the variables are numerical. Sometimes we may want to recode a variable to turn it from a continuous numerical variable, to a categorical/nominal variable. For example, maybe we want to compare students who are high users of facebook, to those who are low users of facebook. We could do that in the following way using the mutate() function:\n\n\ndata %&gt;% \n  mutate(facebook_use = facebook_days &gt;= 4)\n\n\n\n\n\nWhen you run this code you’ll see that mutate() has created a new column on the end, which tells us whether the person uses facebook for at least 4 days a week. Note that the new column is called “facebook_use” - this works in exactly the same way as the summarise() commands you have been practising, where you tell it the new variable name you want to create. The values “TRUE” and “FALSE” are not especially informative here - we probably want to be a bit clearer in the names we give to these levels of the new variable. To do that we can modify this code a little to use an if_else(), which checks if the “conditional statement” (facebook_days &gt;= 4) is TRUE or FALSE, and specifies the values to use in each case:\n\n\ndata %&gt;% \n  mutate(facebook_use = if_else(facebook_days &gt;= 4,true = \"high\",false = \"low\"))\n\n\n\n\n\n\nJust like with summarise(), we can have multiple new columns created within the one mutate() command. Each of these needs to be separated by a comma, and it’s good practice to put each one on a new line. Try copying and pasting the if_else command, and then modifying it to make two new variables to code for “high” and “low” instagram and twitter use:\n\n\ndata &lt;- \n  data %&gt;% \n  mutate(facebook_use = if_else(facebook_days &gt;= 4,true = \"high\",false = \"low\"),\n         instagram_use = ,\n         twitter_use = )\n\ndata\n# because we are assigning (&lt;-) the changes to update \"data\", \n# we write it again here to display the results\n\n\n\n\n\ndata &lt;- \n  data %&gt;% \n  mutate(facebook_use = if_else(facebook_days &gt;= 4,true = \"high\",false = \"low\"),\n         instagram_use = if_else(instagram_days &gt;= 4,true = \"high\",false = \"low\"),\n         twitter_use = if_else(twitter_days &gt;= 4,true = \"high\",false = \"low\"))\n\ndata\n# because we are assigning (&lt;-) the changes to update \"data\", \n# we write it again here to display the results\n\n\n\n\n\nLet’s now look at our avg_stroop variable, and first plot the data in a box and whisker plot:\n\n\ndata %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y = avg_stroop))\n\n\n\n\n\n\nWe can see here that we have a number of points that lie outside the “whiskers” on the plot. One thing we can do to identify potential outliers is to create a new variable that transforms the data to a z distribution. This is easy to do in R using the scale() function:\n\n\ndata &lt;- \n  data %&gt;% \n  mutate(z_stroop = scale(avg_stroop))\n\ndata\n# because we are assigning (&lt;-) the changes to update \"data\", \n# we write it again here to display the results\n\n\n\n\n\n\nYou may remember that a distribution of z scores has a mean of 0 and a standard deviation of 1. Let’s check that here:\n\n\nmean()\nsd()\n\n\n\n\n\nmean(data_set$variable) # example\nsd(data_set$variable) # example\n\n\n\n\nmean(data$z_stroop)\nsd(data$z_stroop)\n\n\nYou’ll notice that the value for the mean is not quite 0 (but it is very very small!). This is probably due to a rounding of values in the scale() function. To get a value of 0 we could encolse the statement in the round() function, e.g. `round(mean(),digits = 2).\n\n\n\nA good way to get a sense of the range of values in our z-scores is to use arrange() to sort them in order. We can then use head() and tail() to see the values at each end of the distribution (the first ‘n’ rows, and the last ‘n’ rows)\n\n\ndata &lt;- \n  data %&gt;% \n  arrange(desc(z_stroop)) # arrange in descending order\n\nhead(data, n = 10)\ntail(data, n = 10)\n\n\n\nYou can see that we’ve got some quite extreme values here. First, we’ve got some very slow participants, showing average reading times of over 10 seconds. Secondly, we’ve got some extremely fast participants. The fastest participant of all is particularly unusual - perhaps they put in incorrect values into the survey?"
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html#filtering-data",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html#filtering-data",
    "title": "PSYC121: Week 9 Lab",
    "section": "Filtering data",
    "text": "Filtering data\nAs you can see, we quite often want to filter our data to select or remove some of the rows we are working with. To do this, we can use the filter() command.\nTo use filter(), we simply specify the data first, and then we need to use an expression to state how we want the data to be filtered. For example:\n\n\ndata %&gt;% \n  filter(z_stroop &lt;= 2) \n# find all those people who have z_stroop values lower than positive 2\n\n\n\n\nCommon expressions\nThe following table gives some examples of very common expressions used in filtering data:\n\n\n\n\n\nOperator\nMeaning\nExample\n\n\n\n\n==\nis the same as\nfilter(dataQ, age==27)\n\n\n&lt;\nis less than\nfilter(dataQ, age&lt;25)\n\n\n&gt;\nis greater than\nfilter(dataQ, age&gt;30)\n\n\n!=\nis not equal to\nfilter(dataQ, gender != 'female')\n\n\n&\nand\nfilter(dataQ, age&lt;30 & gender == 'female')\n\n\n|\nor\nfilter(dataQ, gender == 'male' | gender == 'non-binary')\n\n\n\n\n\n\n\nIt’s particularly important to note the difference between “==” and “=” in R. “=” is used as an assignment operator - you’ve used it several times already inside functions (e.g., na.rm = TRUE, mu = 29600). You can think of “=” as meaning “set this to”. In contrast the double equals operator, “==”, asks a question: “is this thing the same as this other thing?” In the above example, z_stroop &lt;= 2, it looks for all rows in the data where z_stroop is the same as, or less than, the value of 2. In programming terms, the expression returns a boolean value, which reports whether the statement is TRUE or FALSE (and when used in the filter, it finds all rows where it is TRUE). You can see this in the results of the following “conditional expressions”:\n\n\n2 == 3\n\"blah\" == \"blah\"\n\"blah\" == \"BLAH\"\n\"John\" == \"rock star\"\nmean(c(3,4,5,6)) == 4.5\nTRUE == FALSE # this is getting meta...\n\n\n\n\n\nPractice filtering\nPractice writing your own filter commands in the box below. Try to filter the data to match the following queries:\n\nData for those people who are “high” facebook users (hint: facebook_use == )\nData for those people who use instagram for 7 days, and have a z_stroop score of less than -1 (hint: use &)\nData for those people who are “high” users of at least one social media platform (hint: this needs two “or”: | )\n\n[Each hint here gives the solution to each query]\n\n\ndata %&gt;% \n  filter()\n\n\n\n\n\n# Query 1 solution\ndata %&gt;% \n  filter(facebook_use == \"high\")\n\n\n\n\n# Query 2 solution\ndata %&gt;% \n  filter(instagram_days == 7 & z_stroop &lt; -1)\n\n\n\n\n# Query 3 solution\ndata %&gt;% \n  filter(facebook_use == \"high\" | instagram_use == \"high\" | twitter_use == \"high\")"
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html#section-6",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html#section-6",
    "title": "PSYC121: Week 9 Lab",
    "section": "",
    "text": "End of tutorial\nThis is now the end of the online tutorial on filter() and mutate(). Please return to the tasks in the lab worksheet."
  },
  {
    "objectID": "PSYC121/Week1.html",
    "href": "PSYC121/Week1.html",
    "title": "1. Introduction to PSYC121",
    "section": "",
    "text": "PLEASE NOTE: Prof. John Towse is on sabbatical (research leave) this year and so will not be answering emails on his content. All questions should be posted on the discussion forum and will be answered by Dr Tom Beesley.\nWatch the introduction: Lecture Part 1\n\nWatch Lecture Part 2\n\nWatch Lecture Part 3\n\nWatch Lecture Part 4\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.1---check-in-with-the-university-attendance-register",
    "href": "PSYC121/Week1.html#task-3.1---check-in-with-the-university-attendance-register",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.1 - check-in with the University attendance register",
    "text": "Task 3.1 - check-in with the University attendance register\nWhen you arrive, make sure you have checked-in to your Analysis session in the Levy lab. All students are required by the University to confirm attendance at taught session\nStaff will remind you of this in your class.",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.2---getting-dicy",
    "href": "PSYC121/Week1.html#task-3.2---getting-dicy",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.2 - Getting dicy",
    "text": "Task 3.2 - Getting dicy\n\n\n\n\n\nHere’s a simple task for you to complete as a group around each of the workstations;\nYou will be given a pair of dice\n\nWorking in pairs, one person rolls both dice.\nAdd up the total on each of them and have someone record that total (if you don’t have some spare paper or a pen, use your computer)\nRepeat those steps 20 times.\nThen swap over your roles (the person rolling the dice, the person recording the outcome)\nOnce everyone at the workstation has had a turn at this, each person should attempt to work out (a) the mean and (b) the median of their dice roll total.\nCheck each others working, and discuss any differences or problems you have.\n\nAre all your answers the same? Why / why not? If not, are they very different or very similar?",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.3---using-rstudio",
    "href": "PSYC121/Week1.html#task-3.3---using-rstudio",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.3 - Using RStudio",
    "text": "Task 3.3 - Using RStudio\n\nIntroducing R Studio\nR and RStudio is the software that we will be using to explore and learn about analysis in your Psychology degree. It’s a computational engine: a very snazzy calculator that you should see as your friend and ally in the journey to understand and appreciate psychology. It sits alongside what we teach about the concepts and interpretation of statistical analysis.\nR is the core software, RStudio is the interface for interacting with it. Put another way, R is the engine, RStudio is the cockpit.\nLike even a simplest calculator, it just does what you ask (at least when you ask nicely!) but it requires the user to know what they want from it and to understand what it is telling you. A calculator can’t help a kid get the right answer to a multiplication problem if they don’t know the difference between multiplication and division and addition etc. And whilst a calculator is brilliant at doing the number crunching (and as a bonus, R Studio can help with turning the numbers into beautiful graphs and images too), even a calculator requires a thoughtful person to take the answers and make sensible interpretations from them.\nTherefore, we need to learn both about the concepts of statistical analysis on the one hand, and the processing of statistical information -through R- on the other. The lectures will provide the starting point and the direction for statistical concepts, whilst these analysis labs provide the more practical experiences in how to use R, and how to make R your ally. Over the next year, in these labs we will increasingly be using RStudio to focus on the latter, processing side, which will allow you to focus your energies on the conceptual side and its relevance for appreciating psychology.\n\n\nGetting started with RStudio\nWe will be learning about R Studio through a simple but powerful web server architecture. That is, through the power of the internet, you can access and use R Studio by logging into a free account that we have provided and we will maintain for your use.\n\nHere’s a little secret: There are several different ways to access RStudio. For example, you can download a copy of the software onto your computer, or use a Virtual Machine set up to run a copy. There’s nothing to stop you having your local copy, but please note - we can’t support your own version through lab classes. We’re using the web server to make sure everyone has the same, controlled experience.\n\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n \nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\n\n\nWhat does RStudio look like?\nWhen RStudio starts, it will look something like this: \nRStudio has three panels or windows: there are tabs for Console (taking up the left hand side), Environment (and History top right) , Current file (bottom right). You will also see a 4th window for a script or set of commands you develop, also (on the left hand side).\n\n\nLet’s get started!\nThe first thing we want to do in RStudio is to create a folder for this week so that we can put the relevant material there and keep it tidy.\nFrom the lower-right panel of RStudio, click the files tab.\nSelect the “new folder” option and create a new folder (eg “week 1”)\nClick on that folder to open it\nNext, we’ve prepared some instructions for RStudio to use - this is called a “script”. So we need to get this script into the server for you to explore and play with\n\nDownload the “zip” file by clicking this link\nFind the location of the file on your computer and check it is saved as a “.zip” file\nReturn to RStudio\nClick “Upload”\nClick choose file and find the file on your computer.\nSelect the file and click “Open”. Click “OK”\n\nYou should now see the files extracted in the directory.\nYou should now have the script available in RStudio.\nUse “Save…As” to create a new version of the script. By doing this, you’ll be able to have a “before” and “after” version of the script and can go back over the changes\nIn the script, select or highlight the first line of text, which is this one:\n\nRun your first ever R instruction!\n\n5 + 5\n\nand “run” this line. That tells RStudio to carry out the instruction.\nYou should see that in the console tab, RStudio calculates the answer to this incredibly hard maths challenge! (amazing huh? OK, maybe not *that* amazing…).\n\n\nModify your first ever R instruction!\nUse your imagination – add a new line to the script and ask a different simple arithmetic question of your own choosing! What happens?\n\n\nCalculate descriptive stats in R for the first time!\nIn this week’s analysis lecture, we looked at measures of central tendency and how to calculate them. So let’s get R to do these calculations also!\nFirst, we tell R about the data used in the lecture. We’ve already created the instruction that will do exactly this and it is in the script, so run this line from the script\n\nweek_1_lecture_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nThis creates an “object” called week_1_lecture_data. We can then perform calculations on this object. For example, we can find the mean by running the following command (use the script to do this)\n\nmean(week_1_lecture_data)\n\nCheck the answer is the same we found in the lecture (it should be 6!).\nNext, let’s ask for the median by running this line from the script:\n\nmedian(week_1_lecture_data)\n\nThis also should be the answer from the lecture (7)\nR doesn’t have a built in function for the mode (in fact be careful, mode() does something very different to the mode command that we want). So we will use a library() command to bring in a new set of functions, one of which is a handy mode function called mfv(). Try highlighting the text “mvf” and press F1 to view the help on this function. Alternatively you can type ?mfv in the console. and press return.\n\nlibrary(modeest)\nmfv()\n\nWhat does R say the mode is?\n\n\nYour challenge\nHow can you get RStudio to verify / check the dice calculations that you attempted earlier? Think about how you might solve this problem, on the basis of what we have covered so far.\nWe will discuss this in class and attempt to get RStudio to check your answers. In doing so, annotate the script (add notes for you - not RStudio) using the “#” command",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#before-you-finish",
    "href": "PSYC121/Week1.html#before-you-finish",
    "title": "1. Introduction to PSYC121",
    "section": "Before you finish",
    "text": "Before you finish\n\nMake sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\nEnd your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\nThere is a red “power” button near the top right of the R studio window (do ask for help if you can’t find it). It’s a good habit to get into to turn the session off\n\n\nExtra content for outside the lab class\nIn your own time and think about the following:\nIn R, the leftward pointing arrow, “&lt;-”, is called the assignment operator. For example we used it in the command here:\n\nPSYC121_week_1_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nIt’s important to stop and think about what this does. When we use “&lt;-”, we create the variable label on the left (Analysis_week1_data) and we give it those numbers on the right. The nameAnalysis_week1_data is largely arbitrary: try use a variable of your own naming (your own name?) instead - and then use that alternative name for the other commands. QUICK TIP: there is a keyboard shortcut, alt+hyphen, which will give you the assignment arrow in R. Get used to this as it will save you lots of time in the long run.\n\nAnother tip: Throughout this year, we’ll use the convention of the “underscore” to separate words in labels (it_makes_them_easier_to_read than ifyoudidn’thaveanyspaces)",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.4-review-the-learnr-sample-practice-questions",
    "href": "PSYC121/Week1.html#task-3.4-review-the-learnr-sample-practice-questions",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.4 – Review the learnr sample / practice questions",
    "text": "Task 3.4 – Review the learnr sample / practice questions\nAfter every block of teaching in part-1 analysis (specifically, we mean in week 5, week 10, week 15 and week 20) there will be a class test. This will assess your knowledge and your understanding of the material that has been covered.\nThe class test will comprise a set of Multiple Choice Questions (and the set of questions will be different for each student, as the test will involve random selection from a larger pool) under timed conditions.\nIn order to help you get (a) a broad or basic feel for the sort of questions you might get in the class test (b) self-review your progress through the term, we will provide MCQs each week for you to attempt.\nSo these are for your benefit… you can take the questions when you choose to, and the learnr quiz will provide feedback on the answers your provide. Just bear in mind:\n\nWe place a set of questions at the end of the learnr pages so that you can attempt these at the end of each week, after you’ve completed lab activities, follow-up work, weekly Q&As etc. But it’s up to you when you answer the questions\nThese are meant as indicative questions. There’s no point in learning/ memorising these questions (they won’t be on the quiz!) and our advice is to reflect on how the teaching and content links to the sorts of questions that get posed.",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#data-collection-exercise",
    "href": "PSYC121/Week1.html#data-collection-exercise",
    "title": "1. Introduction to PSYC121",
    "section": "Data collection exercise",
    "text": "Data collection exercise\nIn order to learn about psychology and data analysis techniques, we need data! Rather than rely too much on artificial data (certainly it is sometimes useful to say “Here are a bunch of numbers and this is what we can do with them” – think about the R Studio example for this week’s lab) for the most part, we prefer to draw on datasets that are a bit more engaging and meaningful that you have a stake in yourself! By using a common data set, that we can return to over the year, we can also build up familiarity and confidence in the data and remove a potential obstacle to thinking about the more important analysis part.\nSo a key task will be for everyone to have a go at taking our online survey, and contribute to a dataset that can be used throughout the year.\nThe survey runs by following this link",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#post---lab-recap-the-slides-we-used",
    "href": "PSYC121/Week1.html#post---lab-recap-the-slides-we-used",
    "title": "1. Introduction to PSYC121",
    "section": "Post - lab recap: The slides we used",
    "text": "Post - lab recap: The slides we used\nWant to see again the introduction slides that we used in the Levy lab? They are available here",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html",
    "href": "PSYC121/Week3.html",
    "title": "3. DVs and IVs in RStudio",
    "section": "",
    "text": "Watch Part 1\n\n\nWatch Part 2\n\n\nWatch Lecture Part 3\n\n\nWatch Lecture Part 4\n\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html#week-3-lecture",
    "href": "PSYC121/Week3.html#week-3-lecture",
    "title": "3. DVs and IVs in RStudio",
    "section": "",
    "text": "Watch Part 1\n\n\nWatch Part 2\n\n\nWatch Lecture Part 3\n\n\nWatch Lecture Part 4\n\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html#pre-lab-work",
    "href": "PSYC121/Week3.html#pre-lab-work",
    "title": "3. DVs and IVs in RStudio",
    "section": "Pre-lab work",
    "text": "Pre-lab work\nLast week we asked you to\n\nUse a script to run instructions in RStudio\nPut data into RStudio form a data file and explore how to run descriptive stats and basic visualisations\n\nThis week - again, there’s a learnr tutorial to follow and help prep for this week’s activities. You can find it here",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html#lab-activities",
    "href": "PSYC121/Week3.html#lab-activities",
    "title": "3. DVs and IVs in RStudio",
    "section": "Lab activities",
    "text": "Lab activities\nFor the data concerning the weight of a cow data (week 2), we provided you with columns that reflected people’s estimates, and you were able to generate descriptive statistics for those estimates (if you haven’t done this, please go back and work through that part of the week 2 activity again).\nIn order to find the estimates separately for the different groups of respondents (female and non-female), we needed to have a column for each gender category. Whilst that worked, it could get cumbersome over time always to work with data created like that.\nThere is a better way… In this first task we will look at how you can analyse the numerical weight estimates as a function of the categorical data in the identity column.\n\nTask 1 - More with the penelope22 data\nStep 1. Create a new folder for week 3, and set this as the working directory. This is covered in the learnr tutorial and we covered it in Week 2 as well.\nStep 2. Bring the Week_3_2025.zip file into R Studio server. Like last week, upload the zip file, and launch the R script. You can get the file here\nStep 3. This week, we again want to explore commands from the tidyverse library (toolkit) which can help us do more powerful things more elegantly. So let’s get R to work again with the tidyverse library by running the code line\nlibrary(tidyverse)\nStep 4. Explore help() commands. R can give you more information about how it works.\nStep 5. Read the penelope22.csv data into R:\n\nwhat_a_really_terrible_data_object_name &lt;- read_csv(*MISSING*) \n# use your own data object name and specify the file you want to read in\n\nNote that you will need to edit this line (and ensure you are in the correct working directory) for this to be successful. View the new data object using the View() command in the script or clicking on the object in the environment.\nThis week we’ll for the mean of the estimate data as a function of the different gender identities:\n\naggregate(x = *MISSING*$estimate, by = list(*MISSING*$identity), FUN = mean)\n\nThere’s a lot going on in this code, but first, let’s try replacing data object name where it says MISSING and then running the code. What do you get? Does this match what we did last week when we calculated the mean for the female and for the other (i.e., non-female) group?\nOnce you’ve run this, let’s consider all the things that are going on in the code:\naggregate This is a command to call for descriptive statistics\nx= This defines what column we are analyzing\nby=list Now we tell R how to group the estimate data, and which column does that\nFUN=mean Specifies which descriptive function is being asked for. Feel free to try functions for other descriptive statistics that you’ve used in previous weeks.\n\n\nusing group_by() to calculate means for each group\nLike most things in R, there are multiple ways to do the same thing. Here’s another way to group scores by a (nominal/categorical) variable and calculate the values we need for each level of that variable. We explored this already in the learnr tutorial, and that should help you to complete the code below to get the weight estimates broken down by gender identity. You need to first define the data frame for the estimates data, and then specify the gender IV in the group_by command and the estimates DV in the summarise command:\n\n*MISSING* %&gt;% group_by(*MISSING*) %&gt;% summarise(mean_estimate = mean(*MISSING*))\n\nIf you edit this code correctly and run it, you should get a result that is quite similar to that provided by the aggregate command (though the format of the output may be slightly different)\nA note about: %&gt;%\nThis is called a “pipe operator”, basically take the output from the left and feed it into the requests on the right. Summarise Provide summary statistics information for the specified variable as specified (whether mean, median etc)\n\n\n3.1.3 The assignment operator\nAs well as learning about the pipe operator, we want to remind you /draw attention explicitly to another important element of the R command line syntax: the assignment operator. Using a command such as\ncows &lt;- read_csv(\"penelope22.csv\")\nlooks for the csv datafile called ‘penelope22’, reads that data in, and assigns it to an object called ‘cows’\nWe could create any object name we wanted (within broad limits of names that RStudio allows). The arrow symbol isn’t just for reading in data files, we can perform a whole range of functions and assign them to an object.\n\nTask 2 - New salary data\nUsing aggregate and summarise may not seem like much progress, because they are just replicating what we had already done with mean() is week 2. However (a) this emphasizes that there are often several ways to get at the same thing in R (b) now we know about doing calculations on grouped data, and about working with 2-dimensional data. These are big steps - we can now start to do much more efficient and informative things with our data.\nNow, let’s turn to the guesses made about median salary in the UK. We will read in some of the data that PSYC121 students provided in the file wages2024.csv (you will need to adapt the code we used above for the weight estimation file so that it will load in the wages data, and in what follows the assumption is your new variable name is called wages)\nLet’s take a peek at the dataset with:\nglimpse(wages)\nGlimpse pretty much does what you might think from the meaning of the word: it just gives us some quick statistics on the different column (handy because this is a much bigger dataset). We can see that we have 3 columns; uk_region (where someone lives, note ‘other’ probably equals Ireland, Europe, China, etc), family_position (age relationship with siblings), and salary (estimate).\nWe will be analysing how people assessed the median income in the UK. According to government statistics, the median income in 2023 was approximately £34,963 see this link\nYour task:\n\nWithin the script, use the working “aggregate” commands from task 1 with the penelope weight data, can you find out the mean salary estimates as a function of where someone lives? That is, can you adapt that code you used earlier for this problem?\nCan you use the aggregate command to find out mean salary estimates as a function of the different family relationships? (if you are the youngest child maybe you have older siblings earning money that changes your evaluation?)\nCan you get a breakdown of salary estimates as a function of BOTH UK region AND family relationship together? You may well need some help with this, but have a guess at which bit of code would need to change to do it.\nCan you use the group by() and summarise() command to display salary guesses as a function of where someone lives? Check this gives you the same answer.",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html#task-3.2---new-phone-use-data",
    "href": "PSYC121/Week3.html#task-3.2---new-phone-use-data",
    "title": "3. DVs and IVs in RStudio",
    "section": "Task 3.2 - New phone use data",
    "text": "Task 3.2 - New phone use data\n\nRead in the dataset of phone screen time usage, screentime2024.csv, into a new object. For this task we’ll focus on the group_by() and summarise() commands to further explore the data and consolidate your skills. Use copy and paste to adapt the existing script lines from the above tasks so that this time you calculate screen time usage as a function of the type of phone.\nUse RStudio to figure out the (overall) mean screen time estimate and the standard deviation. Using these values, can you calculate by hand what screen time estimate value would reflect z scores of z=-1.5 and z = +2?",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html#task-3.3---final-challenge-visualisation",
    "href": "PSYC121/Week3.html#task-3.3---final-challenge-visualisation",
    "title": "3. DVs and IVs in RStudio",
    "section": "Task 3.3 - Final challenge: visualisation",
    "text": "Task 3.3 - Final challenge: visualisation\nCan you find a way to visualise the screentime usage data that you have been working with above? The script provides two ways to consider doing this - basic boxplots (which we have looked at last week) and ggplot, which we have spent less time with but is an extremely powerful engine for creating plots. We’ve provided the start of the code in each case, leaving you to work out the specifics.",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html",
    "href": "PSYC121/Week6.html",
    "title": "6. Sampling, probability and binomial tests",
    "section": "",
    "text": "Please note: Lecture recordings do not seem to show properly using Safari browser (Mac). Please try Chrome and if problem persists, report on the Moodle Discussion Forum.\nWatch Part 1\n\n\nWatch Part 2\n\n\nWatch Part 3\n\n\nWatch Part 4\n\n\nYou can download the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#week-6-lecture",
    "href": "PSYC121/Week6.html#week-6-lecture",
    "title": "6. Sampling, probability and binomial tests",
    "section": "",
    "text": "Please note: Lecture recordings do not seem to show properly using Safari browser (Mac). Please try Chrome and if problem persists, report on the Moodle Discussion Forum.\nWatch Part 1\n\n\nWatch Part 2\n\n\nWatch Part 3\n\n\nWatch Part 4\n\n\nYou can download the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#reading",
    "href": "PSYC121/Week6.html#reading",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Reading",
    "text": "Reading\nChapter 8 of Howell.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#pre-lab-work",
    "href": "PSYC121/Week6.html#pre-lab-work",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Pre-lab work",
    "text": "Pre-lab work\n\nEnsure you have watched the above lecture content for Week 6.\nComplete the short learnr tutorial which will introduce you to running the binomial tests in R. You can find it here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#working-with-r-markdown-documents-.rmd",
    "href": "PSYC121/Week6.html#working-with-r-markdown-documents-.rmd",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Working with “R Markdown” documents (.Rmd)",
    "text": "Working with “R Markdown” documents (.Rmd)\nToday we will take a look at a different type of document file called “R Markdown” (the filetype is .Rmd). It’s like a .R script, only a bit more fancy! Today we’ll introduce a couple of basic features of these files and use this document for our tasks below.\n\nMake sure you have a Week 6 folder\nOnce in that folder in the files pane, click “more” and “set as working directory”\nThen click the new file button and select “R Markdown”\n\n\n\nYou’ll be asked to name this file. Leave the other options as they are and click OK.\nWhen the new R Markdown file appears, try “knitting” it (the icon at the top with the ball of wool). You should get a nice output of the default R Markdown document.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#adding-code-chunks",
    "href": "PSYC121/Week6.html#adding-code-chunks",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Adding Code chunks",
    "text": "Adding Code chunks\nR Markdown documents allow you to freely type in text in the main body of the file. You are no longer restricted to putting comments after the “#” sign. Instead, when you want to put in R code you will need to create a “code chunk” within the document. These are places where you put your R code you want to run.\nThe easiest way to add a code chunk is to use the dedicated button in the editor pane (the shortcut is ctrl+alt+i):\n\nSelect “R” as that is the type of code you want to write. With your template document, try running the very first code chunk (click the green arrow on the right). This will run the code and display the output (in the document and in the console).\n\n\n\n\n\n\n.Rmd and .R differences\n\n\n\nIn a way, R Markdown files function in the opposite way to .R files:\n\n.Rmd: you write normal text in the main part (like you’d do in a word processor), but you create a special “code chunk” for your code.\n.R: you write code in the main body, and use the “#” to write normal text as a comment",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#source-mode-vs.-visual-mode",
    "href": "PSYC121/Week6.html#source-mode-vs.-visual-mode",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Source mode vs. Visual mode",
    "text": "Source mode vs. Visual mode\nOne of the nice things about working with R Markdown documents is that you can switch from the “coding” view (called “Source”) to something that is much more like a word processor (called “Visual”). You’ll see these two options in the top left corner of the document. Try flicking between them. Visual mode allows you to edit the document like a word-processor, making it easy to make formatting changes, add titles, insert images and tables, etc.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#creating-an-r-markdown-for-your-lab-work",
    "href": "PSYC121/Week6.html#creating-an-r-markdown-for-your-lab-work",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Creating an R Markdown for your lab work",
    "text": "Creating an R Markdown for your lab work\n\nDelete all of the script except the first title (## R Markdown)\nChange this title to something more relevant: (e.g., ## Task 1 - Card Sampling task)\nKnit your document to check it is still working!",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#card-sampling-task",
    "href": "PSYC121/Week6.html#card-sampling-task",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Card sampling task",
    "text": "Card sampling task\n\nIn the first task this week we will look at the sampling of events and we will apply the basic statistical test of the binomial test: binom.test()\nEach table has a set of cards. These will be 13 red cards and 13 black cards - please check your set to ensure you have the right number of each colour (it doesn’t matter what suit the cards are).\nWe are going to play a game in which one person chooses a set of these cards and biases the deck towards either red or black. The other members of the table have to try and work out which way the set is biased. To do this, they will draw samples from the deck.\n\n\n\n\n\n\nThe Experiment\n\n\n\nThink of this as an experiment: there is something real out there in the world in our “population” (the cards). As an experimenter we are trying to estimate what is true about the world, and in order to do this we need to take samples. When you can only see the backs of the cards, that data is unobserved. But as we draw samples, we start to understand how the “world” is - whether it is biased towards red or black.\nSo each time you draw a card, you are observing one data point from the population, and based on the data you collect (your samples) you are going to draw an inference about what is true about the population.\n\n\n\nSet up and instructions:\n\nOne person on each table should act as the “world” (the person who biases the cards). Congratulations, you are God! This person will determine what is true about the state of things in the world. This means they control what is contained in the deck of cards.\nFor each experiment, this person secretly looks at the cards, and removes some cards to use in the experiment. For example, from the set of 26 cards, they might choose to remove 4 black cards. The deck is now biased towards red (13 red; 9 black).\nIt’s important that no one sees what the cards are (the ones you’ve kept, or the ones removed). Shuffle the cards so they are ready to be sampled.\nThe remaining people (1 or more) will act as the experimenters. Your job is to draw samples and work out whether you think the deck is biased or not towards either red or black.\nNow copy the following text and code to your R Markdown document to record your work:\n\n### Experiment 1\n\nNumber of samples:\nTotal red cards found:\nTotal black cards found:\nConclusion: \nThe true bias was: \n\n\n```{r}\n\nbinom.test(x, n) # x is the number or red or black; n is the sample size\n\n```\n\n\nRunning each experiment\nDo these steps for each experiment:\n\nThe “World” removes some cards from the full deck (the number and colour of the cards removed is up to them). They shuffle the chosen cards ready to start the “experiment”.\nThe “Experimenters” pre-register their sample size. That is, they state how many cards they are going to draw.\nDraw samples one at a time (each experimenter can take one card, to speed things up).\n\n\n\n\n\n\n\nImportant!\n\n\n\nMake sure you replace all the cards each time you draw samples. The world/dealer should also give the pack a quick shuffle.\n\n\n\nDo step 3 until you have collect the sample size you chose\nOnce you have all the samples, the experimenters should draw a conclusion based initially on their own “gut feeling” about the data. Do you think the deck was biased towards red, black, or was it unbiased?\nChange the binom.test() code to provide a statistical result. Note the “p value”. Was this result unusual? How likely were the data given the null hypothesis?\nThe “world” can then reveal the hidden cards. Was the deck actually biased or not? How does this sit with a) your initial conclusions, and b) the result of the binomial test?\nComplete your record log in the .Rmd file. Feel free to Write a short statement about what you found in this experiment.\n\nRepeat all of the above steps (1-8) for a new experiment, making sure that you try different parameters for the experiment. So vary a) how many cards are removed from the deck, b) the combination of cards removed from the deck, and c) the pre-registered sample size. Feel free to swap the roles around.\nOnce you’ve conducted a few experiments, discuss on your table the results you found. It might be useful to think about the following things:\n\nwere there times when your intuitions were different to the statistical result? For example, you were sure there was a bias, but in fact the statistics told you this was not that unusual (p was &gt; .05)?\nwere there times when the deck was actually biased, but you failed to prove this with your experiment (you failed to see p &lt; .05)? Do you remember what this type of error is called?\nwere there times when the deck was not biased, but the test result suggested it was (p &lt; .05)? Do you remember what type of error this is called?",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#risky-and-safe-decisions",
    "href": "PSYC121/Week6.html#risky-and-safe-decisions",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Risky and safe decisions",
    "text": "Risky and safe decisions\nFor the second exercise today we will look at data from the survey on “risky and safe decisions”. You may remember that you were asked the following question:\n\n\n\n\n\n\nGain\n\n\n\nImagine you are on a game-show and you win £300. The presenter gives you a choice: receive an additional £100 for sure, or take a gamble offering a 50% chance to gain £200 and a 50% chance to gain nothing. Which would you choose?\n\nreceive £100 for sure\ntake the gamble of a 50% chance to gain £200 and a 50% chance to gain nothing\n\n\n\nWe then asked you a similar question:\n\n\n\n\n\n\nLoss\n\n\n\nImagine you are on a different game-show and you win £500. Here you are given a choice between either losing £100 from your winnings for sure, or taking a gamble offering a 50% chance to lose nothing and a 50% chance to lose £200. Which would you choose?\n\nlose £100 for sure\ntake the gamble of a 50% chance to lose nothing and a 50% chance to lose £200\n\n\n\nWe’ve called these “gain” and “loss”, because in the first scenario you’re being asked about a chance to gain money, while in the second, it’s about a chance to lose money. Note that the “expected utility” of the choices is equivalent:\n\n\n\n\n\n\n“The expected utility of an act is a weighted average of the utilities of each of its possible outcomes, where the utility of an outcome measures the extent to which that outcome is preferred, or preferable, to the alternatives. The utility of each outcome is weighted according to the probability that the act will lead to that outcome.” see here\n\n\n\nThat’s to say, on average, you’ll end up with +£100 for the two cases in the gain scenario, or -£100 for the two cases in the loss scenario.\nBut what do people actually pick? Well people tend to be risk-averse, choosing the safe option overall. But interestingly, the safe option is picked far less when the scenario is presented as a loss. People seem to want to take the risk of potentially not losing anything (but maybe losing more).\nLet’s look to see if you showed the same pattern!\n\nCreate a new section of your markdown, giving it a suitable header\nCreate a new code chunk by clicking the “Code” menu, then “Insert Chunk”\nYou can download the data from this link, then upload it into the server.\nAdd a read_csv() command to read the data into the environment.\n\n\n\n\n\n\n\nReading CSVs\n\n\n\n\nRemember to set the working directory so R knows where the file is\nRemember to assign (&lt;-) to a new data object and give this a sensible name\n\n\n\n\nView the data, and see that the columns represent the gain and loss scenarios. The values represent the choices people made.\nUse the count() function to count the number of “safe” and “risky” choices that were made for our sample\n\n\ncount(my_data_object_name, gain)\n\n\nWe can now tell whether, in our sample, people tended to play it safe or take the risky choice. Did the sample have a meaningful bias towards one type of decision? If they didn’t we’d expect it to be a 50/50 split between safe and risky (people might make their choice at random). Use the binom.test() to look at whether the result would be expected by chance, noting the p value that is found.\nWrite a sentence or two after your code to explain what this result means.\nRepeat for the data from the loss column. Was the p value &lt; .05 here? Again, write a sentence or two to explain what this means.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#week-6-quiz",
    "href": "PSYC121/Week6.html#week-6-quiz",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Week 6 Quiz",
    "text": "Week 6 Quiz\nYou can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week5.html",
    "href": "PSYC121/Week5.html",
    "title": "5. Class test",
    "section": "",
    "text": "Formulae for weeks 1-4\nHere you can download a list of the formulae used in the first half of this module.\nThere are no other materials for this week!\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC121",
      "5. Class test"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html",
    "href": "PSYC121/Week7.html",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "",
    "text": "Please note: Lecture recordings do not seem to show properly using Safari browser (Mac). Please try Chrome and if problem persists, report on the Moodle Discussion Forum.\nWatch Part 1\n\nWatch Part 2\n\nWatch Part 3\n\nWatch Part 4\n\nDownload the lecture slides here\n\n\nChapter 12 of Howell\nToday we will look in a bit more detail at people’s estimates of the average UK salary. We will first plot this data using geom_histogram() and also geom_boxplot(). When we do this, we’ll see that there are some unusual values, and we’ll need to do a bit of data wrangling to remove them, using the filter() command. We’ll then turn to the conceptual ideas of the lecture - how can we tell if the mean of our sample is unusual, or whether we would actually expect this mean value under the null hypothesis? Finally, we’ll continue to develop our skills in data visualisation by exploring geom_density() plots.",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html#reading",
    "href": "PSYC121/Week7.html#reading",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "",
    "text": "Chapter 12 of Howell\nToday we will look in a bit more detail at people’s estimates of the average UK salary. We will first plot this data using geom_histogram() and also geom_boxplot(). When we do this, we’ll see that there are some unusual values, and we’ll need to do a bit of data wrangling to remove them, using the filter() command. We’ll then turn to the conceptual ideas of the lecture - how can we tell if the mean of our sample is unusual, or whether we would actually expect this mean value under the null hypothesis? Finally, we’ll continue to develop our skills in data visualisation by exploring geom_density() plots.",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html#sample-size-size-of-effect-and-the-one-sample-t-test",
    "href": "PSYC121/Week7.html#sample-size-size-of-effect-and-the-one-sample-t-test",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "Sample size, size of effect, and the one sample t-test",
    "text": "Sample size, size of effect, and the one sample t-test\nIn the lecture this week, Tom used an application to show the process of sampling data. You can access this application at the link below. There are three “parameters” you can change in this:\n\nThe true mean of the effect: Think of this like the bias that was set up in your deck of cards last week. There is some true state out there in the world, and we are going to draw samples from a distribution of data that has a mean that equals this value. If you make this 100, then the true mean is equal to that under the null hypothesis (there is no effect).\nThe standard deviation of the data: This sets how variable the data are in this population. If the data are more variable, then our samples will produce estimations that are less accurate of the true mean value.\nThe sample size: How many observations are drawn in the sample. These are represented by the yellow circles in the plot.\n\nEach time you draw a sample the data points are plotted in yellow and the mean of the sample is marked with the red line. The application also runs a one-sample t-test against the expected mean under the null hypothesis, of 100. The null hypothesis is also represented by the static distribution presented in grey, centred on 100.\nThings to try:\n\nStart with a sample size of 10, and a mean of the effect of 110 (SD = 15). How often do you get a significant result (p &lt; .05) when you draw a new sample?\nNow try changing the mean of the effect to 120. Does this increase or decrease the likelihood of getting significant results? What about changing to 130?\nNow keep the mean effect constant (say 110), but increase the sample size. Try 5, then 10, 15, and so on. Does this increase or decrease the likelihood of getting significant results?\nSet the mean of the effect to 100 and the sample size to 10. Keep drawing new samples, noting each time the p value. You will evenutally get a p value of &lt; .05. What type of error is this?\n\nClick here for the one-sample t-test application",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html#week-7-quiz",
    "href": "PSYC121/Week7.html#week-7-quiz",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "Week 7 Quiz",
    "text": "Week 7 Quiz\nYou can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html",
    "href": "PSYC121/Week8.html",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "Watch Part 1\n\nWatch Part 2\n\nWatch Part 3\n\nWatch Part 4\n\nDownload the lecture slides here\nTo play around with the t distribution simulator that Tom uses in the lecture, go here: (on campus, or VPN required)\n\n\nChapter 13 of Howell\nToday we will take a look at summarising means and standard errors (SEs) from our data. We will look at how we plot these together on the one graph (using ggplot() commands that allow us to share mappings between different geoms. We will explore our data on the famous “Stroop Task” and we will use a related-samples t-test to examine the differences between the means of our different conditions in this task.\n\n\n\nOnline tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\n\nCreate a folder for Week 8 and download the Week_8 csv file file and upload it into this new folder in RStudio Server.\n\n\n\n\n\n\nThe “Stroop Effect” is a classic demonstration of automaticity of behaviour. Participants have to say the colour a word is printed in, which is an easy task for a “compatible” stimulus like GREEN, and a much more difficult task for an “incompatible” stimulus like BLUE. We can’t help but read the text - it has seemingly become an automatic process.\n In this task we will calculate the means and standard errors of the means and then we will then plot them using ggplot(). First though, we’ll need to inspect the data and maybe do a bit of data wrangling by using our filter() command.\n\nCreate a new Rmarkdown document. If you’re unsure about this step, see the instructions from Week 6 (or 7).\nAs usual, add a code chunk with library(tidyverse) and a read_csv command (see above for the link to the csv). Assign the result to a new data object, and call your data something meaningful (perhaps data_w8 or data_stroop but maybe not bestest_most_fantastic_data_on_the_stroop_test_eva_init)\nView the data with View(data_object_name). You will see that the data are a little different from the data we have worked with previously. We have an pID variable, which gives a unique number for each person, but each person has 3 rows of data. This is because the different conditions of the Stroop task reflect a within-subjects variable (related samples). For data like this it is often useful to have them arranged in what is referred to as “long format”, with multiple rows for each response the participant provides. For the current data that means we have a variable called condition, which is our IV, and one called time which is our DV. We also have a column labelled avg_time, which is the average of the 3 time values for each participant (the data is duplicated, which is both normal and necessary with long format data).\nLet’s look at the distribution of time (our DV) as a function of condition. Add another chunk of code and include the following code:\n\n\n# distribution of times by condition\nyour_data_object %&gt;% \n  ggplot() +\n  geom_density(aes(x = missing_column_name, \n                   fill = missing_column_name), \n               alpha = .8) + # you need to EDIT this for Q4\n  theme_dark()\n\n\nYou’ll need to “map” x to time and fill to condition for our geom_density() plot. You can play around with the alpha parameter (which sets the transparency of the elements of the graph), setting it to a value between 0 and 1. Note that this is done OUTSIDE of the aes() command.\nFrom the density plot, it does seem like we have some outlier values. It’s probably best if we remove data for the whole participant if their average time is unusual. To do that, we’ll look at the data using the avg_time column. Add the following code for a geom_histogram() to plot the distribution of values in the new avg_time column.\n\n\n# distribution of average times\nyour_data_object %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = missing_column_name), fill = \"pink\") + # you need to EDIT this line\n  theme_classic()\n\n\nLet’s use the filter command we learned last week to remove these high values. Like last week, we will do this (for now) in a fairly unprincipled manner, by “eyeballing” the data (next week we’ll consider something a bit more “scientific”). Complete the filter command so that it keeps only the responses for people that had an avg_time less than 10 seconds. Remember that you need to think about how you are storing the result of this filter process. Do you want to create a new object, or overwrite the existing object?\n\n\n# filter out the high values\nnew_data_object &lt;- # create a new object (or overwrite)\n  your_data_object %&gt;% # original data object \n  filter(insert_an_expression_here)\n\n\n\n\n\n\n\nCheck your result!\n\n\n\nIf you’ve done this correctly, you should now have a data object that has 189 rows (data for 63 participants, with 3 responses each).\n\n\n\nAdd and edit the following code to plot a histogram of the filtered data.\n\n\n# draw a histogram of the filtered data \nnew_data_object %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = missing_columns_name), \n                 fill = \"pink\", # try some different colours?\n                 colour = \"purple\", # and here?\n                 bins = 3) #  # adjust the bins? \n# you could also add (+) a theme to this plot! \n# for a list of themes, type: ?theme_classic\n\n\nFinally, copy the code for the original geom_density() plot that you drew in step 4. Paste it, and edit the code so that it now plots the filtered set of data (from step 7) for each of the three conditions in the stroop task.\n\n\n\n\nWe have seen in our density plots that the reaction times (DV) look different in the three different Stroop conditions (our IV). But now we need to look at whether there are statistically significant differences between the means of the three conditions.\nTo do this, we will first summarise the mean time taken by each condition in the Stroop task. In Week 3 we learnt how to use group_by() and summarise() to get summary stats (e.g., mean, sd) at each level of the IV. That’s what we want to do now:\n\nCopy the code below into your R Markdown and edit the group_by() line to specify the IV and the summarise() line to calculate the mean() of our DV. If you do this correctly, you’ll get three values - a mean value for each level (condition) of our IV. Do these means reflect what you would expect in the Stroop task? Do they match the central tendency of the distributions you plotted?\n\n\nname_of_data_object %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q1\n  summarise(stroop_mean = mean(name_of_DV_column)) # you need to EDIT this for Q1\n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nYou should have 3 values: 4.81, 6.09, 7.80\nIf all of your values are the same, you’ve analysed the wrong column.\n\n\n\n\nNext we need to test if these differences between our means are real. To do that, we can run a related samples t-test. We use this test because each level of the IV in this experiment came from the same person. First though, we must use a filter() to restrict the data to just two levels of the IV. The IV is the condition column/variable in the data. The related samples t-test looks at the difference between two means (and only two), so the column we use for the t-test needs to have just two levels of the IV (two of the conditions).\nCopy the code below into your R Markdown. Edit the first filter command to select the data for the compatible condition, and the second filter command to get the data for the control condition. HINT: you’ll want to use a “==” here - this allows us to select the data that is the same as (==) a particular value (i.e., the name of that condition).\n\n\n# use filter to select the compatible level of the IV\nstroop_compatible &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# use filter to select the control level of the IV\nstroop_control &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# use filter to select the incompatible level of the IV\nstroop_incompatible &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# run the t-test comparing the means of two of the levels\nt.test(x = stroop_compatible$time, y = stroop_control$time, paired = TRUE)\n\n# add two more t-tests for the other two comparisions\n\n\nRun the t-test on this selection of data to compare the means from these two levels of the IV. Is the result significant? Note the t-value and the p-value.\nWrite out a statement in your R Markdown document to express this result. Here’s a template you can use, where you need to edit the bits in the []:\n\n\n“There [was a / was no] significant difference between the [describe the variables that were compared], t([degrees of freedom here]) = [t value here], p &lt; [p value here].”\n\n\nWith 3 levels (L1,L2,L3) to the IV condition there are 3 possible comparisons we can make (L1 vs. L2; L1 vs. L3; L2 vs. L3). Complete all three tests, by copying and pasting the code chunk twice more, editing each filter expression to select the relevant data. For each pair of conditions/levels, run the t-test. Write out a reporting statement (Q5) for each of your comparisons.\n\n\n\n\n\n\n\nCheck your results!\n\n\n\n\nUsing the filtered data (189 rows) you should get the following t statistics: (+/-)7.5688, (+/-)7.4054, (+/-)12.191\nRemember that it’s the magnitude that’s important, not whether it’s positive or negative. The sign simply depends which way round they are compared in the t calculations (mean_1 - mean_2 or mean_2 - mean_1)\n\n\n\n\n\n\n\nIn Task 2 you calculated the means for each condition in the Stroop task. We’ve seen in lectures that “standard error” provides an estimate of how variable that mean will be across the samples we collect. A very typical way to plot a mean value is to plot it with the standard error of the mean (SEM):\n\n\n\nThe code from Task 2, Question 1 will give the mean. We will now add a second line of this code to give the standard error values:\n\n\nstroop_summary &lt;- \n  name_of_data_object %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q2\n  summarise(stroop_mean = mean(name_of_DV_column),\n            stroop_SE = sd(name_of_DV_column)/sqrt(n())) # you need to EDIT this for Q1\n\n\nAdd this code to your document and the correct column (DV) to both the sd() and the mean() commands. Note that you don’t need to put anything in n(), as this simply calculates how many rows there are.\nView the new summary object you have created. Check that the means and SEs are different for the 3 conditions. If they are the same, you probably summarised the wrong column!\n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nThe SE values should be: 0.180, 0.168, 0.274\n\n\n\n\nWe will now plot these 3 mean values in a figure. Let’s use geom_point() so that our means and SEs look a bit like the figure above. Complete the ggplot command to plot our summarised value called stroop_mean (y), as a function of the IV, condition (x):\n\n\n# let's first plot the means\nname_of_data_object %&gt;%\n  ggplot(aes(x = name_of_IV_column, y = name_of_DV_column)) + # map variables to x and y for Q5\n  geom_point(size = 5)\n\n\nNow we need to add some “error bars” which provide a visual guide as to how much uncertainty we have in our mean value. Edit the code below for the ggplot() command to plot both geom_point() (same as Q5) and geom_errorbar. You will need to calculate a ymin and a ymax value.\n\n\n\n\n\n\n\nPlotting the error bars\n\n\n\nUse the illustration of the error bars above to work out how to combine the mean value and the SE value (hint: you’ll need to either ADD or SUBTRACT for the two statements) to create the right ymin and ymax. You need to put this in the “missing_equation” bit of the code below:\n\n\n\n# let's first plot the means\nname_of_data_object %&gt;%\n  ggplot(aes(x = name_of_IV_column, y = name_of_DV_column)) + # map variables to x and y for Q5\n  geom_point(size = 5) +\n  geom_errorbar(aes(ymin = missing_equation, # edit this for Q5\n                    ymax = missing_equation), # edit this for Q5\n                width = .2) \n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nThe correct result will have 3 points, and an error bar around each mean point. These 3 error bars should all be different sizes (as per the 3 SEs you calculated in steps 2-4)\n\n\n\nEXTRA: These next steps can be completed to practice customising your plot\n\nAdd a labs() layer to the plot to change the axis titles, and the title of the plot.\nChange the theme of the plot (e.g., theme_classic() or theme_dark())\nMap the colour aesthetic to the variable condition. You can do this for geom_point or geom_errorbar or both at once by putting it in the aes() within the ggplot() command.\nTry changing your geom_point() to geom_col.\n\n\n\n\nLet’s try knitting the document. If you’ve done everything right, then the knitting process will work and you’ll get a nice output (in html, or PDF, whichever you choose). If something goes wrong, here’s a few things you can check\n\nDid you keep all your code in the code chunks?\nCheck all your code blocks run.\nAre there any red cross symbols next to your lines of code? These indicate a code error and need to be fixed before it will knit.\n\nWhen you knit the document, you will probably see the code you have written in the output. You can decide whether you want to present the code or not using the options for each code chunk:\n\nClick the cog, then select the type of output you want each code to produce.\nKnitting the document is a great way to see how your work looks as an actual report. Go back and add more description between your code chunks to describe all the steps you have performed in your analysis.\n\n\n\n\nYou can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html#reading",
    "href": "PSYC121/Week8.html#reading",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "Chapter 13 of Howell\nToday we will take a look at summarising means and standard errors (SEs) from our data. We will look at how we plot these together on the one graph (using ggplot() commands that allow us to share mappings between different geoms. We will explore our data on the famous “Stroop Task” and we will use a related-samples t-test to examine the differences between the means of our different conditions in this task.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html#pre-lab-work-online-tutorial",
    "href": "PSYC121/Week8.html#pre-lab-work-online-tutorial",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "Online tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\n\nCreate a folder for Week 8 and download the Week_8 csv file file and upload it into this new folder in RStudio Server.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html#rstudio-tasks",
    "href": "PSYC121/Week8.html#rstudio-tasks",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "The “Stroop Effect” is a classic demonstration of automaticity of behaviour. Participants have to say the colour a word is printed in, which is an easy task for a “compatible” stimulus like GREEN, and a much more difficult task for an “incompatible” stimulus like BLUE. We can’t help but read the text - it has seemingly become an automatic process.\n In this task we will calculate the means and standard errors of the means and then we will then plot them using ggplot(). First though, we’ll need to inspect the data and maybe do a bit of data wrangling by using our filter() command.\n\nCreate a new Rmarkdown document. If you’re unsure about this step, see the instructions from Week 6 (or 7).\nAs usual, add a code chunk with library(tidyverse) and a read_csv command (see above for the link to the csv). Assign the result to a new data object, and call your data something meaningful (perhaps data_w8 or data_stroop but maybe not bestest_most_fantastic_data_on_the_stroop_test_eva_init)\nView the data with View(data_object_name). You will see that the data are a little different from the data we have worked with previously. We have an pID variable, which gives a unique number for each person, but each person has 3 rows of data. This is because the different conditions of the Stroop task reflect a within-subjects variable (related samples). For data like this it is often useful to have them arranged in what is referred to as “long format”, with multiple rows for each response the participant provides. For the current data that means we have a variable called condition, which is our IV, and one called time which is our DV. We also have a column labelled avg_time, which is the average of the 3 time values for each participant (the data is duplicated, which is both normal and necessary with long format data).\nLet’s look at the distribution of time (our DV) as a function of condition. Add another chunk of code and include the following code:\n\n\n# distribution of times by condition\nyour_data_object %&gt;% \n  ggplot() +\n  geom_density(aes(x = missing_column_name, \n                   fill = missing_column_name), \n               alpha = .8) + # you need to EDIT this for Q4\n  theme_dark()\n\n\nYou’ll need to “map” x to time and fill to condition for our geom_density() plot. You can play around with the alpha parameter (which sets the transparency of the elements of the graph), setting it to a value between 0 and 1. Note that this is done OUTSIDE of the aes() command.\nFrom the density plot, it does seem like we have some outlier values. It’s probably best if we remove data for the whole participant if their average time is unusual. To do that, we’ll look at the data using the avg_time column. Add the following code for a geom_histogram() to plot the distribution of values in the new avg_time column.\n\n\n# distribution of average times\nyour_data_object %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = missing_column_name), fill = \"pink\") + # you need to EDIT this line\n  theme_classic()\n\n\nLet’s use the filter command we learned last week to remove these high values. Like last week, we will do this (for now) in a fairly unprincipled manner, by “eyeballing” the data (next week we’ll consider something a bit more “scientific”). Complete the filter command so that it keeps only the responses for people that had an avg_time less than 10 seconds. Remember that you need to think about how you are storing the result of this filter process. Do you want to create a new object, or overwrite the existing object?\n\n\n# filter out the high values\nnew_data_object &lt;- # create a new object (or overwrite)\n  your_data_object %&gt;% # original data object \n  filter(insert_an_expression_here)\n\n\n\n\n\n\n\nCheck your result!\n\n\n\nIf you’ve done this correctly, you should now have a data object that has 189 rows (data for 63 participants, with 3 responses each).\n\n\n\nAdd and edit the following code to plot a histogram of the filtered data.\n\n\n# draw a histogram of the filtered data \nnew_data_object %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = missing_columns_name), \n                 fill = \"pink\", # try some different colours?\n                 colour = \"purple\", # and here?\n                 bins = 3) #  # adjust the bins? \n# you could also add (+) a theme to this plot! \n# for a list of themes, type: ?theme_classic\n\n\nFinally, copy the code for the original geom_density() plot that you drew in step 4. Paste it, and edit the code so that it now plots the filtered set of data (from step 7) for each of the three conditions in the stroop task.\n\n\n\n\nWe have seen in our density plots that the reaction times (DV) look different in the three different Stroop conditions (our IV). But now we need to look at whether there are statistically significant differences between the means of the three conditions.\nTo do this, we will first summarise the mean time taken by each condition in the Stroop task. In Week 3 we learnt how to use group_by() and summarise() to get summary stats (e.g., mean, sd) at each level of the IV. That’s what we want to do now:\n\nCopy the code below into your R Markdown and edit the group_by() line to specify the IV and the summarise() line to calculate the mean() of our DV. If you do this correctly, you’ll get three values - a mean value for each level (condition) of our IV. Do these means reflect what you would expect in the Stroop task? Do they match the central tendency of the distributions you plotted?\n\n\nname_of_data_object %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q1\n  summarise(stroop_mean = mean(name_of_DV_column)) # you need to EDIT this for Q1\n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nYou should have 3 values: 4.81, 6.09, 7.80\nIf all of your values are the same, you’ve analysed the wrong column.\n\n\n\n\nNext we need to test if these differences between our means are real. To do that, we can run a related samples t-test. We use this test because each level of the IV in this experiment came from the same person. First though, we must use a filter() to restrict the data to just two levels of the IV. The IV is the condition column/variable in the data. The related samples t-test looks at the difference between two means (and only two), so the column we use for the t-test needs to have just two levels of the IV (two of the conditions).\nCopy the code below into your R Markdown. Edit the first filter command to select the data for the compatible condition, and the second filter command to get the data for the control condition. HINT: you’ll want to use a “==” here - this allows us to select the data that is the same as (==) a particular value (i.e., the name of that condition).\n\n\n# use filter to select the compatible level of the IV\nstroop_compatible &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# use filter to select the control level of the IV\nstroop_control &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# use filter to select the incompatible level of the IV\nstroop_incompatible &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# run the t-test comparing the means of two of the levels\nt.test(x = stroop_compatible$time, y = stroop_control$time, paired = TRUE)\n\n# add two more t-tests for the other two comparisions\n\n\nRun the t-test on this selection of data to compare the means from these two levels of the IV. Is the result significant? Note the t-value and the p-value.\nWrite out a statement in your R Markdown document to express this result. Here’s a template you can use, where you need to edit the bits in the []:\n\n\n“There [was a / was no] significant difference between the [describe the variables that were compared], t([degrees of freedom here]) = [t value here], p &lt; [p value here].”\n\n\nWith 3 levels (L1,L2,L3) to the IV condition there are 3 possible comparisons we can make (L1 vs. L2; L1 vs. L3; L2 vs. L3). Complete all three tests, by copying and pasting the code chunk twice more, editing each filter expression to select the relevant data. For each pair of conditions/levels, run the t-test. Write out a reporting statement (Q5) for each of your comparisons.\n\n\n\n\n\n\n\nCheck your results!\n\n\n\n\nUsing the filtered data (189 rows) you should get the following t statistics: (+/-)7.5688, (+/-)7.4054, (+/-)12.191\nRemember that it’s the magnitude that’s important, not whether it’s positive or negative. The sign simply depends which way round they are compared in the t calculations (mean_1 - mean_2 or mean_2 - mean_1)\n\n\n\n\n\n\n\nIn Task 2 you calculated the means for each condition in the Stroop task. We’ve seen in lectures that “standard error” provides an estimate of how variable that mean will be across the samples we collect. A very typical way to plot a mean value is to plot it with the standard error of the mean (SEM):\n\n\n\nThe code from Task 2, Question 1 will give the mean. We will now add a second line of this code to give the standard error values:\n\n\nstroop_summary &lt;- \n  name_of_data_object %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q2\n  summarise(stroop_mean = mean(name_of_DV_column),\n            stroop_SE = sd(name_of_DV_column)/sqrt(n())) # you need to EDIT this for Q1\n\n\nAdd this code to your document and the correct column (DV) to both the sd() and the mean() commands. Note that you don’t need to put anything in n(), as this simply calculates how many rows there are.\nView the new summary object you have created. Check that the means and SEs are different for the 3 conditions. If they are the same, you probably summarised the wrong column!\n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nThe SE values should be: 0.180, 0.168, 0.274\n\n\n\n\nWe will now plot these 3 mean values in a figure. Let’s use geom_point() so that our means and SEs look a bit like the figure above. Complete the ggplot command to plot our summarised value called stroop_mean (y), as a function of the IV, condition (x):\n\n\n# let's first plot the means\nname_of_data_object %&gt;%\n  ggplot(aes(x = name_of_IV_column, y = name_of_DV_column)) + # map variables to x and y for Q5\n  geom_point(size = 5)\n\n\nNow we need to add some “error bars” which provide a visual guide as to how much uncertainty we have in our mean value. Edit the code below for the ggplot() command to plot both geom_point() (same as Q5) and geom_errorbar. You will need to calculate a ymin and a ymax value.\n\n\n\n\n\n\n\nPlotting the error bars\n\n\n\nUse the illustration of the error bars above to work out how to combine the mean value and the SE value (hint: you’ll need to either ADD or SUBTRACT for the two statements) to create the right ymin and ymax. You need to put this in the “missing_equation” bit of the code below:\n\n\n\n# let's first plot the means\nname_of_data_object %&gt;%\n  ggplot(aes(x = name_of_IV_column, y = name_of_DV_column)) + # map variables to x and y for Q5\n  geom_point(size = 5) +\n  geom_errorbar(aes(ymin = missing_equation, # edit this for Q5\n                    ymax = missing_equation), # edit this for Q5\n                width = .2) \n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nThe correct result will have 3 points, and an error bar around each mean point. These 3 error bars should all be different sizes (as per the 3 SEs you calculated in steps 2-4)\n\n\n\nEXTRA: These next steps can be completed to practice customising your plot\n\nAdd a labs() layer to the plot to change the axis titles, and the title of the plot.\nChange the theme of the plot (e.g., theme_classic() or theme_dark())\nMap the colour aesthetic to the variable condition. You can do this for geom_point or geom_errorbar or both at once by putting it in the aes() within the ggplot() command.\nTry changing your geom_point() to geom_col.\n\n\n\n\nLet’s try knitting the document. If you’ve done everything right, then the knitting process will work and you’ll get a nice output (in html, or PDF, whichever you choose). If something goes wrong, here’s a few things you can check\n\nDid you keep all your code in the code chunks?\nCheck all your code blocks run.\nAre there any red cross symbols next to your lines of code? These indicate a code error and need to be fixed before it will knit.\n\nWhen you knit the document, you will probably see the code you have written in the output. You can decide whether you want to present the code or not using the options for each code chunk:\n\nClick the cog, then select the type of output you want each code to produce.\nKnitting the document is a great way to see how your work looks as an actual report. Go back and add more description between your code chunks to describe all the steps you have performed in your analysis.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html#week-8-quiz",
    "href": "PSYC121/Week8.html#week-8-quiz",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "You can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  }
]