[
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "",
    "text": "In Week 18, we aim to further develop skills in working with the linear model.\nWe do this to learn how to answer research questions like:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\nThese kinds of research questions can be answered using methods like the linear model.\nWhen we do these analyses, we need to think about how we report the results:\n\nwe usually need to report information about the kind of model we specify;\nand we will need to report the nature of the association estimated in our model.\n\nWe usually need to decide:\n\nis the association significant?\ndoes the association reflect a positive or negative relationship between outcome and predictor?\nand is the association we see in our sample data relatively strong or weak?\n\nWe will consolidate and extend learning on data visualization:\n\nfocusing on how we edit ggplot() code to produce professional looking plots.\n\n\n\nI will format dataset names like this:\n\nstudy-two-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}.\n\n\n\nIn this activity, we use data from a second 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-two-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#naming-things",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#naming-things",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "",
    "text": "I will format dataset names like this:\n\nstudy-two-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}."
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#the-data-we-will-be-using",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#the-data-we-will-be-using",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "",
    "text": "In this activity, we use data from a second 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-two-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-1-set-up",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-1-set-up",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "Step 1: Set-up",
    "text": "Step 1: Set-up\nTo begin, we set up our environment in R.\n\nTask 1 – Run code to empty the R environment\n\nrm(list=ls())                            \n\n\n\nTask 2 – Run code to load relevant libraries\n\nlibrary(\"ggeffects\")\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-2-load-the-data",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-2-load-the-data",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "Step 2: Load the data",
    "text": "Step 2: Load the data\n\nTask 3 – Read in the data file we will be using\nThe data file is called:\n\nstudy-two-general-participants.csv\n\nUse the read_csv() function to read the data file into R:\n\nstudy.two.gen &lt;- read_csv(\"study-two-general-participants.csv\")\n\nRows: 172 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data file in, give the data object you create a distinct name e.g. study.two.gen.\n\n\nTask 4 – Inspect the data file\nUse the summary() or head() functions to take a look.\n\nhead(study.two.gen)\n\n# A tibble: 6 × 12\n  participant_ID mean.acc mean.self study     AGE SHIPLEY  HLVA FACTOR3 QRITOTAL\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 studytwo.1        0.411      6.07 studyt…    26      27     6      50        9\n2 studytwo.10       0.607      8.5  studyt…    38      24     9      58       15\n3 studytwo.100      0.875      8.93 studyt…    66      40    13      60       20\n4 studytwo.101      0.964      8.5  studyt…    21      31    11      59       14\n5 studytwo.102      0.714      7.07 studyt…    74      35     7      52       18\n6 studytwo.103      0.768      5.07 studyt…    18      40    11      54       15\n# ℹ 3 more variables: GENDER &lt;chr&gt;, EDUCATION &lt;chr&gt;, ETHNICITY &lt;chr&gt;\n\nsummary(study.two.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:172         Min.   :0.4107   Min.   :3.786   Length:172        \n Class :character   1st Qu.:0.6786   1st Qu.:6.411   Class :character  \n Mode  :character   Median :0.7679   Median :7.321   Mode  :character  \n                    Mean   :0.7596   Mean   :7.101                     \n                    3rd Qu.:0.8393   3rd Qu.:7.946                     \n                    Max.   :0.9821   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :29.00  \n 1st Qu.:25.00   1st Qu.:32.75   1st Qu.: 7.750   1st Qu.:47.00  \n Median :32.50   Median :36.00   Median : 9.000   Median :51.00  \n Mean   :35.37   Mean   :35.13   Mean   : 9.064   Mean   :51.24  \n 3rd Qu.:44.00   3rd Qu.:39.00   3rd Qu.:11.000   3rd Qu.:56.25  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:172         Length:172         Length:172        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :14.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.88                                                           \n 3rd Qu.:16.00                                                           \n Max.   :20.00                                                           \n\n\nEven though you have done this before, you will want to do it again, here, and pay particular attention to:\n\nsummary information about the numeric variables;\nsummary information about variables of class: character."
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-3-use-a-linear-model-to-to-answer-the-research-questions-one-predictor",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-3-use-a-linear-model-to-to-answer-the-research-questions-one-predictor",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "Step 3: Use a linear model to to answer the research questions – one predictor",
    "text": "Step 3: Use a linear model to to answer the research questions – one predictor\n\nRevise: practice to strengthen skills\n\n\nRevise: We start by revising how to use lm() with one predictor\nOne of our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe can address this question by examining whether someone’s rated evaluation of their own understanding matches their performance on a test of that understanding, and by investigating what variables predict variation in mean self-rated accuracy.\n\nNote that ratings of accuracy are ordinal data but that, here, we may choose to examine the average of participants’ ratings of their own understanding of health information to keep our analysis fairly simple.\nFor these data, participants were asked to respond to questions about health information to get mean.acc scores and then were asked to rate their own understanding of the same information.\n\nIf you can evaluate your own understanding then ratings of understanding should be associated with performance on tests of understanding\n\n\nTask 5 – Estimate the relation between outcome mean self-rated accuracy (mean.self) and tested accuracy of understanding (mean.acc)\n\n\nhint: Task 5\nFor these data, participants were asked to respond to questions about health information to get mean.acc scores and were asked to rate their own understanding of the same information.\n\n\nhint: Task 5\nWe can use lm() to estimate whether the ratings of accuracy actually predict the outcome tested accuracy levels.\n\nmodel &lt;- lm(mean.self ~ mean.acc, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.self ~ mean.acc, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.47926 -0.62782  0.02038  0.65403  2.37788 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.8725     0.5037   5.703 5.12e-08 ***\nmean.acc      5.5670     0.6550   8.499 9.36e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.032 on 170 degrees of freedom\nMultiple R-squared:  0.2982,    Adjusted R-squared:  0.2941 \nF-statistic: 72.24 on 1 and 170 DF,  p-value: 9.356e-15\n\n\nIn R analysis code, we write method(outcome ~ predictor) so:\nlm(mean.self ~ mean.acc, data = study.two.gen)\n\ngets us an analysis of whether or how mean.self predicts variation in outcome mean.acc.\n\nIf you look at the model summary you can answer the following questions.\n\nQuestions: Task 5\n\nQ.1. What is the estimate for the coefficient of the effect of the predictor mean.acc on the outcome mean.self in this model?\nA.1. 5.5670\nQ.2. Is the effect significant?\nA.2. It is significant, p &lt; .05\nQ.3. What are the values for t and p for the significance test for the coefficient?\nA.3. t = 8.499, p = 9.36e-15\nQ.4. What do you conclude is the answer to the research question, given the linear model results?\nA.4. The model slope estimate suggests that higher levels of tested understanding can predict higher levels of rated understanding so, yes: it does appear that people can evaluate their own understanding.\nQ.5. What is the F-statistic for the regression? Report F, DF and the p-value.\nA.5. F-statistic: 72.24 on 1 and 170 DF, p-value: 9.356e-15\nQ.6. Is the regression significant?\nA.6. Yes: the regression is significant.\nQ.7. What is the Adjusted R-squared?\nA.7. Adjusted R-squared: 0.2941\nQ.8. Explain in words what this R-squared value indicates?\nA.8. The R-squared suggests that about 30% of outcome variance can be explained by the model"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-4-use-a-linear-model-to-to-answer-the-research-questions-multiple-predictors",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-4-use-a-linear-model-to-to-answer-the-research-questions-multiple-predictors",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "Step 4: Use a linear model to to answer the research questions – multiple predictors",
    "text": "Step 4: Use a linear model to to answer the research questions – multiple predictors\n\nIntroduce: make some new moves\nOne of our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe have already looked at this question by asking whether ratings of understanding predict performance on tests of understanding.\nBut there is a problem with that analysis – it leaves open the question:\n\nWhat actually predicts ratings of understanding?\n\nWe can look at this follow-up question, next.\n\n\nTask 6 – Examine the relation between outcome mean self-rated accuracy (mean.self) and multiple predictors\nHere, the predictors will include:\n\nhealth literacy (HLVA);\nvocabulary (SHIPLEY);\nage in years (AGE);\nreading strategy (FACTOR3);\nas well as average accuracy of the tested understanding of health information (mean.acc).\n\n\nhint: Task 6 – We use lm(), as before, but now specify each variable listed here by variable name\n\nmodel &lt;- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc, \n            data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc, \n    data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.72027 -0.49118 -0.00177  0.55561  2.00134 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.561110   0.700632   0.801   0.4244    \nHLVA         0.041272   0.034833   1.185   0.2378    \nSHIPLEY     -0.046125   0.018701  -2.466   0.0147 *  \nFACTOR3      0.063689   0.010747   5.926 1.74e-08 ***\nAGE          0.025570   0.005472   4.673 6.12e-06 ***\nmean.acc     4.763278   0.708166   6.726 2.69e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8805 on 166 degrees of freedom\nMultiple R-squared:  0.5014,    Adjusted R-squared:  0.4864 \nF-statistic: 33.39 on 5 and 166 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nQuestions: Task 6\nIf you look at the model summary you can answer the following questions.\n\nQ.9. What predictors are significant in this model?\nA.9. Vocabulary (SHIPLEY), reading strategy (FACTOR3), AGE, and performance on tests of accuracy of understanding (mean.acc) all appear to significantly predict variation in mean ratings of understanding (mean.self).\nQ.10. What is the estimate for the coefficient of the effect of the predictor mean.acc in this model?\nA.10. 4.763278\nQ.11. Is the effect significant?\nA.11. It is significant, p &lt; .05\nQ.12. What are the values for t and p for the significance test for the coefficient?\nA.12. t = 6.726, p = 2.69e-10\nQ.13. What do you conclude is the answer to the follow-up question, what actually predicts ratings of understanding?\nA.13. Ratings of understanding appear to be predicted by performance on tests of accuracy of understanding, together with variation in age, vocabulary knowledge, health literacy and reading strategy"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-5-understanding-linear-model-predictions-by-comparing-one-outcome-predictor-relation",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-5-understanding-linear-model-predictions-by-comparing-one-outcome-predictor-relation",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "Step 5: Understanding linear model predictions by comparing one outcome-predictor relation",
    "text": "Step 5: Understanding linear model predictions by comparing one outcome-predictor relation\nNext, we focus in on whether mean.self predicts mean.acc or, in reverse, whether mean.acc predicts mean.self?\n\nNote that a comparison between these models teaches us something important about what it is that linear models predict.\nQ.14. Why do you think it appears that the slope coefficient estimate is different if you compare :\n\n\nThe model mean.acc ~ mean.self versus\nThe model mean.self ~ mean.acc?\n\n\nhint: Q.14. You want to fit two simple models here, using the verbal description in the Q.14 wording.\n\n1. The model mean.acc ~ mean.self\n\nmodel.1 &lt;- lm(mean.acc ~ mean.self, data = study.two.gen)\nsummary(model.1)\n\n\nCall:\nlm(formula = mean.acc ~ mean.self, data = study.two.gen)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.293692 -0.069200  0.005422  0.080041  0.215240 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.379182   0.045415   8.349 2.31e-14 ***\nmean.self   0.053566   0.006303   8.499 9.36e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1013 on 170 degrees of freedom\nMultiple R-squared:  0.2982,    Adjusted R-squared:  0.2941 \nF-statistic: 72.24 on 1 and 170 DF,  p-value: 9.356e-15\n\n\n2. The model mean.self ~ mean.acc\n\nmodel.2 &lt;- lm(mean.self ~ mean.acc, \n              data = study.two.gen)\nsummary(model.2)\n\n\nCall:\nlm(formula = mean.self ~ mean.acc, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.47926 -0.62782  0.02038  0.65403  2.37788 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.8725     0.5037   5.703 5.12e-08 ***\nmean.acc      5.5670     0.6550   8.499 9.36e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.032 on 170 degrees of freedom\nMultiple R-squared:  0.2982,    Adjusted R-squared:  0.2941 \nF-statistic: 72.24 on 1 and 170 DF,  p-value: 9.356e-15\n\n\n\nhint: Q.14. You may benefit here by reflecting on the lm-intro lecture and practical materials, especially where they concern predictions.\nA.14. Linear models are prediction models. We use them to predict variation in outcomes given some set of predictor variables. Predictions will necessarily be scaled in the same way as the outcome variable.\n\nSo, to expand on that explanation a bit more, to help understanding – the answer is:\n\nIf we have the model, mean.acc ~ mean.self then this means that the outcome is mean.acc.\n\n\nSo if we are predicting change in outcome mean.acc, which is scaled 0-1, then we are looking at coefficients that will lie somewhere on the same scale (also 0-1).\nHere: the model estimate suggests that each unit change in values of the variable mean.self predicts an increase of 0.053566 in mean.acc.\n\n\nWhereas if we have the model, mean.self ~ mean.acc then this means that the outcome is mean.self.\n\n\nSo if we are predicting change in outcome mean.self, which is scaled 1-9 , then we are looking at coefficients that will lie somewhere on the same scale (also 1-9).\nHere: the model estimate suggests that unit change in mean.acc predicts increase of 5.5670 in mean.self.\n\nNote that:\n\nWhere we reference model estimates, here, we are looking at the values in the Estimate column of the lm() model summary.\nThese estimates give us the expected or predicted change in the outcome, given change in the predictor variable named on that row.\n\nRemember that:\n\nmean.acc is scaled from 0 to 1 because it represents the average accuracy of the responses made by study participants to questions about health texts. This average has to have a minimum of 0 (no responses correct) and a maximum of 1 (all responses correct). The average is calculated by adding up all the correct answers and dividing by the number of questions answered by each participant.\nmean.self is scaled from 1 to 9 bcause it represents the average self-rated accuracy of understanding. Participants are asked to rate on a scale form 1 (not all) to 9 (very well) how well they think they understand a health information text. The average is calculated by adding up all the ratings and dividing by the number of texts responded to by each participant.\n\nThe important lesson, here, is that estimates of predictor effects are scaled in terms of predicted change in the outcome, so whatever scale the outcome measurement is in determines how big or small the predictor coefficient estimates can be.\nWe can visualize this to see what it means in practice.\n\nQ.15. Can you plot the predictions from each model?\nA.15. Here is the code to plot the predictions from both models.\n\nFirst fit the models.\n\nRemember to give each model object distinct names.\n\n\nmodel.1 &lt;- lm(mean.acc ~ mean.self, data = study.two.gen)\nsummary(model.1)\n\n\nCall:\nlm(formula = mean.acc ~ mean.self, data = study.two.gen)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.293692 -0.069200  0.005422  0.080041  0.215240 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.379182   0.045415   8.349 2.31e-14 ***\nmean.self   0.053566   0.006303   8.499 9.36e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1013 on 170 degrees of freedom\nMultiple R-squared:  0.2982,    Adjusted R-squared:  0.2941 \nF-statistic: 72.24 on 1 and 170 DF,  p-value: 9.356e-15\n\nmodel.2 &lt;- lm(mean.self ~ mean.acc, \n            data = study.two.gen)\nsummary(model.2)\n\n\nCall:\nlm(formula = mean.self ~ mean.acc, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.47926 -0.62782  0.02038  0.65403  2.37788 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.8725     0.5037   5.703 5.12e-08 ***\nmean.acc      5.5670     0.6550   8.499 9.36e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.032 on 170 degrees of freedom\nMultiple R-squared:  0.2982,    Adjusted R-squared:  0.2941 \nF-statistic: 72.24 on 1 and 170 DF,  p-value: 9.356e-15\n\n\nSecond get the predictions:\n\ndat.1 &lt;- ggpredict(model.1, \"mean.self\")\ndat.2 &lt;- ggpredict(model.2, \"mean.acc\")\n\nThird make the prediction plots:\n\nPredictions from the model mean.acc ~ mean.self\n\n\nplot(dat.1)\n\n\n\n\n\n\n\n\n\nPredictions from the model mean.self ~ mean.acc\n\n\nplot(dat.2)\n\n\n\n\n\n\n\n\n\nQ.16. Look at the two plots: what do you see?\nhint: Q.16. Look at changes in height of the prediction line, given changes in x-axis position of the line\nA.16. A side-by-side comparison shows that:\n\n\nFor model mean.acc ~ mean.self increases in mean.self from about 4 to 9 are associated with a change in mean.acc from about .6 to about .85;\nFor model mean.self ~ mean.acc increases in mean.acc from about 0.4 to 1.0 are associated with a change in mean.self from about 5 to about 9."
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-6-estimate-the-effects-of-factors-as-well-as-numeric-variables",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-workbook-answers.html#step-6-estimate-the-effects-of-factors-as-well-as-numeric-variables",
    "title": "2023-24-PSYC122-w18-workbook-answers",
    "section": "Step 6: Estimate the effects of factors as well as numeric variables",
    "text": "Step 6: Estimate the effects of factors as well as numeric variables\n\nConsolidation: build your skills\nWe have not yet included any categorical or nominal variables as predictors but we can, and should: lm() can cope with any kind of variable as a predictor.\nThere are different ways to do this, here we ask you to use the R default method.\n\n\nTask 7 – Fit a linear model to examine what variables predict outcome mean self-rated accuracy of mean.self\n\nhint: Task 7 – Include as predictors both numeric variables and categorical variables\nHere, our model includes predictors that are both numeric like:\n\nhealth literacy (HLVA);\nvocabulary (SHIPLEY);\nAGE;\nreading strategy (FACTOR3);\naccuracy mean.acc\n\nAs well as a categorical or nominal variable like\n\nEDUCATION.\n\nNote: EDUCATION is different because participants are classified by what education category (higher education, further education, secondary school) they report themselves as having received.\n\nmodel &lt;- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc +\n                        EDUCATION, \n            data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc + \n    EDUCATION, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.70987 -0.50037  0.01988  0.55965  2.01412 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.487753   0.702049   0.695   0.4882    \nHLVA                0.047100   0.034915   1.349   0.1792    \nSHIPLEY            -0.044132   0.018719  -2.358   0.0196 *  \nFACTOR3             0.061918   0.010771   5.749 4.29e-08 ***\nAGE                 0.023997   0.005595   4.289 3.06e-05 ***\nmean.acc            4.912833   0.712381   6.896 1.10e-10 ***\nEDUCATIONHigher    -0.082217   0.146390  -0.562   0.5751    \nEDUCATIONSecondary  0.346161   0.266030   1.301   0.1950    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8783 on 164 degrees of freedom\nMultiple R-squared:  0.5099,    Adjusted R-squared:  0.489 \nF-statistic: 24.38 on 7 and 164 DF,  p-value: &lt; 2.2e-16\n\n\n\nQ.17. Can you report the overall model and model fit statistics?\nA.17.\n\n\nWe fitted a linear model with mean self-rated accuracy as the outcome and with the predictors: health literacy (HLVA), vocabulary (SHIPLEY), reading strategy (FACTOR3), AGE, as well as mean accuracy (mean.acc) and education level (EDUCATION). The model is significant overall, with F(7, 164) = 24.38, p &lt; .001, and explains 49% of variance (adjusted R2 = 0.489).\n\n\nQ.18. Can you plot the predicted effect of EDUCATION given your model?\nhint: Q.18. We first fit the model, including EDUCATION.\n\n\nmodel &lt;- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc + EDUCATION, \n            data = study.two.gen)\n\n\nhint: Q.18. We then use the ggpredict() function to get the prediction for the effect of EDUCATION differences on outcome mean.self.\n\n\ndat &lt;- ggpredict(model, \"EDUCATION\")\n\nSome of the focal terms are of type `character`. This may lead to\n  unexpected results. It is recommended to convert these variables to\n  factors before fitting the model.\n  The following variables are of type character: `EDUCATION`\n\nplot(dat)\n\n\n\n\n\n\n\n\n\nQ.19. The plot should give you dot-and-whisker representations of the estimated mean.self outcome for different levels of EDUCATION. What is the difference in the estimated mean.self between the groups?\nhint: Q.19. The effect or prediction plot will show you dot-and-whisker representations of predicted outcome mean.self. In these plots, the dots represent the estimated mean.self while the lines (whiskers) represent confidence intervals.\nA.19. The difference in the estimated mean.self between these groups is small: the groups vary between ratings of about 7, 7.10 and 7.5.\nQ.20. Compare the difference in the estimated mean.self between these groups, given the plot, with the coefficient estimate from the model summary: what do you see?\nA.20. The effect of EDUCATION is presented in the summary as two estimates:\nEDUCATIONHigher    -0.082217\nEDUCATIONSecondary  0.346161\n\nThe reference level for EDUCATION is Further.\nThe estimates therefore show that people with Higher education have mean.self scores about -.08 lower than mean.self for people with Further education.\nPeople with Secondary education have mean.self scores about .35 higher than mean.self for people with Further education.\nWe are learning some new things here so it is useful to explain them:\n\nCategorical variables or factors and reference levels.\n\n\nIf you have a categorical variable like EDUCATION then when you use it in an analysis, R will look at the different categories (called levels) e.g., here,higher education, further education, secondary school` and it will pick one level to be the reference or baseline level.\nThe reference is the the level against which other levels are compared.\nHere, the reference level is Further (education) simply because, unless you tell R otherwise, it picks the level with a category name that begins earlier in the alphabet as the reference level.\n\n\nDot and whisker plots show estimates with confidence intervals.\n\n\nDot and whisker plots are a nice way to present a concise visual summary about the estimates we get from prediction models.\nHere, the plots show the coefficient estimates from our model (the dots) plus confidence intervals (the lines or “whiskers”).\n\n\nConfidence intervals are often misunderstood but they are helpful.\n\n\nEssentially, a confidence interval tells us about we might expect to see using our analysis procedure (Hoekstra et al., 2014).\n\n\nIf we were to repeat the experiment over and over, then 95 % of the time the confidence intervals contain the true mean.\n\n\nAnd you can read more about this here\n\nHoekstra, R., Morey, R. D., Rouder, J. N., & Wagenmakers, E. J. (2014). Robust misinterpretation of confidence intervals. Psychonomic Bulletin & Review, 21, 1157-1164."
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "",
    "text": "In Week 16, we aim to develop skills in visualizing and testing the associations between variables in psychological data.\nWe do this to learn how to answer research questions like:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\nThese kinds of research questions can be answered using methods like correlation and linear models.\nWe will consolidate and extend learning on data visualization:\n\nUse histograms to examine the distributions of variables;\nUse scatterplots to examine the relationships we may observe or predict.\n\n\n\nI will format dataset names like this:\n\nstudy-two-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}.\n\n\n\nIn this activity, we use data from a second 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-two-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#naming-things",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#naming-things",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "",
    "text": "I will format dataset names like this:\n\nstudy-two-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}."
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#the-data-we-will-be-using",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#the-data-we-will-be-using",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "",
    "text": "In this activity, we use data from a second 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-two-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-1-set-up",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-1-set-up",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "Step 1: Set-up",
    "text": "Step 1: Set-up\nTo begin, we set up our environment in R.\n\nTask 1 – Run code to empty the R environment\n\nrm(list=ls())                            \n\n\n\nTask 2 – Run code to load relevant libraries\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-2-load-the-data",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-2-load-the-data",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "Step 2: Load the data",
    "text": "Step 2: Load the data\n\nTask Task 3 – Read in the data file we will be using\nThe data file is called:\n\nstudy-two-general-participants.csv\n\nUse the read_csv() function to read the data file into R:\n\nstudy.two.gen &lt;- read_csv(\"study-two-general-participants.csv\")\n\nRows: 172 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data file in, give the data object you create a clear name e.g. study.two.gen.\n\n\nTask 4 – Inspect the data file\nUse the summary() or head() functions to take a look\n\nsummary(study.two.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:172         Min.   :0.4107   Min.   :3.786   Length:172        \n Class :character   1st Qu.:0.6786   1st Qu.:6.411   Class :character  \n Mode  :character   Median :0.7679   Median :7.321   Mode  :character  \n                    Mean   :0.7596   Mean   :7.101                     \n                    3rd Qu.:0.8393   3rd Qu.:7.946                     \n                    Max.   :0.9821   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :29.00  \n 1st Qu.:25.00   1st Qu.:32.75   1st Qu.: 7.750   1st Qu.:47.00  \n Median :32.50   Median :36.00   Median : 9.000   Median :51.00  \n Mean   :35.37   Mean   :35.13   Mean   : 9.064   Mean   :51.24  \n 3rd Qu.:44.00   3rd Qu.:39.00   3rd Qu.:11.000   3rd Qu.:56.25  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:172         Length:172         Length:172        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :14.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.88                                                           \n 3rd Qu.:16.00                                                           \n Max.   :20.00                                                           \n\n\n\nQ.1. What is the median of AGE?\nA.1. 32.50\nQ.2. What class is the variable ETHNICITY?\nA.2. character\nQ.3. Does the summary indicate if any variable has missing values (NAs)?\nQ.3. No\n\n\n\nTask 5 – Change the class or type of the variable ETHNICITY to factor\nYou can use the as.factor() function you have used before:\n\nstudy.two.gen$ETHNICITY &lt;- as.factor(study.two.gen$ETHNICITY)\n\n\nQ.4. After you have done this, what information does summary() give you about the variable ETHNICITY?\n\n\nsummary(study.two.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:172         Min.   :0.4107   Min.   :3.786   Length:172        \n Class :character   1st Qu.:0.6786   1st Qu.:6.411   Class :character  \n Mode  :character   Median :0.7679   Median :7.321   Mode  :character  \n                    Mean   :0.7596   Mean   :7.101                     \n                    3rd Qu.:0.8393   3rd Qu.:7.946                     \n                    Max.   :0.9821   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :29.00  \n 1st Qu.:25.00   1st Qu.:32.75   1st Qu.: 7.750   1st Qu.:47.00  \n Median :32.50   Median :36.00   Median : 9.000   Median :51.00  \n Mean   :35.37   Mean   :35.13   Mean   : 9.064   Mean   :51.24  \n 3rd Qu.:44.00   3rd Qu.:39.00   3rd Qu.:11.000   3rd Qu.:56.25  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION         ETHNICITY  \n Min.   : 6.00   Length:172         Length:172         Asian: 15  \n 1st Qu.:12.00   Class :character   Class :character   Black:  5  \n Median :14.00   Mode  :character   Mode  :character   Mixed:  7  \n Mean   :13.88                                         White:145  \n 3rd Qu.:16.00                                                    \n Max.   :20.00                                                    \n\n\n\nA.4. We can see that ETHNICITY lists observations following UK Office National Statistics ethnicity grouping:\nAsian: 15\nBlack: 5\nMixed: 7\nWhite: 145"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-3-use-histograms-to-examine-the-distributions-of-variables",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-3-use-histograms-to-examine-the-distributions-of-variables",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "Step 3: Use histograms to examine the distributions of variables",
    "text": "Step 3: Use histograms to examine the distributions of variables\n\nTask 6 – Draw histograms to examine the distributions of variables\n\nHint: Task 6\nUse ggplot() with geom_histogram().\nThe first time we do this, we take things step-by-step.\nHere’s an example: run the line of code and see the result in the Plots window in R-Studio.\n\nggplot(data = study.two.gen, aes(x = mean.acc)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThese are the steps, set out one at a time:\n\nggplot(...) you tell R you want to make a plot using the ggplot() function\nggplot(data = study.two.gen ...) you tell R you want to make a plot with the study.two.gen data\nggplot(..., aes(x = mean.acc)) you tell R that you want to make a plot with the variable mean.acc – here, you specify the aesthetic mapping, x = mean.acc\nggplot(...) + geom_histogram() you tell R you want to plot values of mean.acc as a histogram\n\n\n\nQuestions: Task 6\n\nQ.5. Did you get a message in the Console window in R-Studio: what does it say?\nA.5. The message says:\nstat_bin() using bins = 30. Pick better value with binwidth.\nQ.6. Draw two different histograms to examine the distributions of two different variables: SHIPLEY and HLVA\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nQ.7. Now re-do both plots: can you change the binwidth in geom_histogram() to make the bars wider?\n\nIf you are going to change binwidth the number you use needs to be a number larger than\nthe minimum and smaller than the maximum for the variable.\nRemember, min and max values are given for each numeric variable in summary().\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA)) + geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\nWe adjust binwidth typically to improve the appearance of the plot.\nThis is a bit subjective so try different numbers and see how you feel about the changes in appearance.\nWe want histograms that show us enough detail about the frequency of occurrence of groupings (bins) of values for each variable.\nBut we do not want histograms that show us so much detail it is difficult to see the pattern for the distribution.\n\nQ.8 – How would you describe the distributions – in a sentence – of the distributions of the SHIPLEY and HLVA variable values for our sample?\nA.8. The SHIPLEY values lie between about 25 and 40, and are skewed towards high scores.\nA.8. The HLVA values lie between 4 and about 14, and peak in the middle (near 7)."
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-4-edit-your-plots-to-make-them-look-good",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-4-edit-your-plots-to-make-them-look-good",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "Step 4: Edit your plots to make them look good",
    "text": "Step 4: Edit your plots to make them look good\n\nTask 7 – Edit the appearance of a histogram plot for one numeric variable\nNote that ggplot() code does not all have to be on the same line.\nYou can create a new plot for each edit so you can see what difference your edits make.\n\nQ.9. Edit the appearance of the bars using binwidth\n\n\nggplot(data = study.two.gen, aes(x = HLVA)) + \n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\nQ.10. Edit the colour of the background using theme_bw()\n\n\nggplot(data = study.two.gen, aes(x = HLVA)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nQ.11. Edit the appearance of the labels using labs()\n\n\nggplot(data = study.two.gen, aes(x = HLVA)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"count\")\n\n\n\n\n\n\n\n\nNotice how, if you are doing edits in steps, one line at a time, each line in your code except the last one ends in a +.\nWhat we are doing is telling R we want this + this + this … Each line then adds an extra step.\nYou can break this code by not adding a + at the end of each bit (except the last line).\nNotice that how to break the code, and how to figure out how to fix the break, are discussed in the how-to .R"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-5-now-draw-scatterplots-to-examine-associations-between-variables",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-5-now-draw-scatterplots-to-examine-associations-between-variables",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "Step 5: Now draw scatterplots to examine associations between variables",
    "text": "Step 5: Now draw scatterplots to examine associations between variables\n\nTask 8 – Create a scatterplot to examine the association between some variables\nWe are working with geom_point() and you need x and y aesthetic mappings\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis plot shows the possible association between x-axis variable HLVA and y-axis variable mean.acc.\nThe plot code moves through the following steps:\n\nggplot(...) make a plot\nggplot(data = study.two.gen, ...) with the study.two.gen dataset\nggplot(...aes(x = HLVA, y = mean.acc)) using two aesthetic mappings:\n\n\nx = HLVAmapHLVA` values to x-axis (horizontal, left to right) positions\ny = mean.accmapmeann.acc` values to y-axis (vertical, bottom to top) positions\n\n\ngeom_point() show the mappings as points\n\n\n\nTask 9 – Now do scatterplots with every numeric predictor variable in the study.two.gen dataset\nYou always want to use as the y-axis variable the outcome mean.acc so:\n\ny = mean.acc\n\nThen you can use each numeric predictor variable as the x-axis variable so:\n\nx = mean.self\n\nRemember what we saw with summary(): not every variable consists of numbers\nIf the summary() does not show you a mean for a variable, then R does not think that variable is numeric\nIt can be hard to decide what an association looks like:\n\nDraw the plot, answer the question\n\nggplot(data = study.two.gen, aes(x = mean.self, y = mean.acc)) +\n  geom_point()  \n\n\n\n\n\n\n\n\n\nQ.12. What is the shape (direction) of the association between mean.self and mean.acc?\nA.12. Increase in mean.self is associated with increase in mean.acc\n\n\n\nDraw the plot, answer the question\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\nQ.13. What is the shape (direction) of the association between AGE and mean.acc?\nA.13. There is no clear association between AGE and mean.acc\n\n\n\nDraw the plot, answer the question\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()  \n\n\n\n\n\n\n\n\n\nQ.14. What is the shape (direction) of the association between SHIPLEY and mean.acc?\nA.14. Increase in SHIPLEY is associated with increase in mean.acc\n\n\n\nDraw the plot, answer the question\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\nQ.15. – What is the shape (direction) of the association between HLVA and mean.acc?\nA.15. Increase in HLVA is associated with increase in mean.acc\n\n\n\nDraw the plot, answer the question\n\nggplot(data = study.two.gen, aes(x = FACTOR3, y = mean.acc)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\nQ.16. What is the shape (direction) of the association between FACTOR3 and mean.acc?\nA.16. Increase in FACTOR3 is associated with increase in mean.acc\n\n\n\nDraw the plot, answer the question\n\nggplot(data = study.two.gen, aes(x = QRITOTAL, y = mean.acc)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\nQ.17. What is the shape (direction) of the association between QRITOTAL and mean.acc?\nA.17. Increase in QRITOTAL is associated with increase in mean.acc"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-6-edit-the-scatterplots-to-make-them-look-good",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-6-edit-the-scatterplots-to-make-them-look-good",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "Step 6: Edit the scatterplots to make them look good",
    "text": "Step 6: Edit the scatterplots to make them look good\n\nTask 10 – Edit the appearance of one plot step-by-step\n\nHint: Task 10 – We are going to edit:\n\nthe appearance of the points using alpha, size and colour;\nthe colour of the background using theme_bw();\nthe appearance of the labels using labs().\n\nJust like with geom_histogram() there is ggplot reference information for the geom you can use here – take a look:\nhttps://ggplot2.tidyverse.org/reference/geom_point.html\nYou can find some example code and come back here if you are unsure what to do\n\n\nQuestions: Task 10\n\nQ.18. Change the appearance of the points using alpha, size and colour:\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\") \n\n\n\n\n\n\n\n\n\nQ.19. Edit the colour of the background using theme_bw()\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\")   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nQ.20. Edit the appearance of the labels using labs()\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\")   +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\nQ.21. Can you find the ggplot reference page?\n\nDo a search with the keywords “ggplot reference geom_point”\n\nA.21. – It is here:\n\nhttps://ggplot2.tidyverse.org/reference/geom_point.html"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#now-you-experiment",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#now-you-experiment",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "Now you: experiment!",
    "text": "Now you: experiment!"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-7-use-correlation-to-to-answer-the-research-questions",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-workbook-answers.html#step-7-use-correlation-to-to-answer-the-research-questions",
    "title": "2023-24-PSYC122-w16-workbook-answers",
    "section": "Step 7: Use correlation to to answer the research questions",
    "text": "Step 7: Use correlation to to answer the research questions\n\nTask 11 – Examine the correlation between mean accuracy (mean.acc) and some numeric predictor variables\nWe use cor.test()\n\nQ.22. What is r (given as cor in the output) for the correlation between HLVA and mean.acc?\n\n\ncor.test(study.two.gen$HLVA, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$HLVA and study.two.gen$mean.acc\nt = 7.5288, df = 170, p-value = 2.866e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3787626 0.6044611\nsample estimates:\n      cor \n0.5000559 \n\n\n\nA.22. r = 0.5000559\nQ.23. Is the correlation significant?\nA.23. r is significant\nQ.24. What are the values for t and p for the significance test for the correlation?\nA.24. t = 7.5288, p = 2.866e-12\nQ.25. What do you conclude, given the correlation results? (Maybe draw a scatterplot to examine the shape of the association.)\nA.25. HLVA and mean.acc are positively correlated suggesting that as HLVA scores increase so also do mean.acc scores\nQ.26. What is r (given as cor in the output) for the correlation between mean.self and mean.acc?\n\n\ncor.test(study.two.gen$mean.self, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$mean.self and study.two.gen$mean.acc\nt = 8.4991, df = 170, p-value = 9.356e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4317217 0.6431596\nsample estimates:\n      cor \n0.5460792 \n\n\n\nA.26. r = 0.5460792\nQ.27. Is the correlation between AGE and mean.acc significant?\n\n\ncor.test(study.two.gen$AGE, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$AGE and study.two.gen$mean.acc\nt = 0.30121, df = 170, p-value = 0.7636\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1269774  0.1721354\nsample estimates:\n       cor \n0.02309589 \n\n\n\nA.27. r is not significant\nQ.28. What are the values for t and p for the significance test for the correlation between QRITOTAL and mean.acc?\n\n\ncor.test(study.two.gen$QRITOTAL, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$QRITOTAL and study.two.gen$mean.acc\nt = 6.4711, df = 170, p-value = 9.993e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3159538 0.5571417\nsample estimates:\n    cor \n0.44457 \n\n\n\nA.28. t = 6.4711, p = 9.993e-10\nQ.29. What do you conclude, given the correlation results, about the association between SHIPLEY and mean.acc?\n\n\ncor.test(study.two.gen$SHIPLEY, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$SHIPLEY and study.two.gen$mean.acc\nt = 6.8493, df = 170, p-value = 1.299e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3390103 0.5746961\nsample estimates:\n      cor \n0.4650537 \n\n\n\nA.29. SHIPLEY and mean.acc are positively correlated suggesting that as HLVA scores increase so also do mean.acc scores"
  },
  {
    "objectID": "PSYC122/data/week11/122_wk11_labActivities2_3.html#lab-activity-3---hazardous-alcohol-use-and-impulsivity",
    "href": "PSYC122/data/week11/122_wk11_labActivities2_3.html#lab-activity-3---hazardous-alcohol-use-and-impulsivity",
    "title": "122_wk11_labActivities2_3",
    "section": "Lab activity 3 - Hazardous alcohol use and impulsivity",
    "text": "Lab activity 3 - Hazardous alcohol use and impulsivity"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "",
    "text": "In Week 17, we aim to further develop skills in visualizing and testing the associations between variables in psychological data.\nWe do this to learn how to answer research questions like:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\nThese kinds of research questions can be answered using methods like the linear model.\nWhen we do these analyses, we need to think about how we report the results:\n\nwe usually need to report information about the kind of model we specify;\nand we will need to report the nature of the association estimated in our model;\nwe usually need to decide, is the association significant?\ndoes the association reflect a positive or negative relationship between outcome and predictor?\nand is the association we see in our sample data relatively strong or weak?\n\nWe will consolidate and extend learning on data visualization:\n\nfocusing on how we edit ggplot() code to produce professional looking plots.\n\n\n\nI will format dataset names like this:\n\nstudy-two-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}.\n\n\n\nIn this activity, we use data from a second 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-two-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#naming-things",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#naming-things",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "",
    "text": "I will format dataset names like this:\n\nstudy-two-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}."
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#the-data-we-will-be-using",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#the-data-we-will-be-using",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "",
    "text": "In this activity, we use data from a second 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-two-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-1-set-up",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-1-set-up",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "Step 1: Set-up",
    "text": "Step 1: Set-up\nTo begin, we set up our environment in R.\n\nTask 1 – Run code to empty the R environment\n\nrm(list=ls())                            \n\n\n\nTask 2 – Run code to load relevant libraries\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-2-load-the-data",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-2-load-the-data",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "Step 2: Load the data",
    "text": "Step 2: Load the data\n\nTask 3 – Read in the data file we will be using\nThe data file is called:\n\nstudy-two-general-participants.csv\n\nUse the read_csv() function to read the data file into R:\n\nstudy.two.gen &lt;- read_csv(\"study-two-general-participants.csv\")\n\nRows: 172 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data file in, give the data object you create a distinct name e.g. study.two.gen.\n\n\nTask 4 – Inspect the data file\nUse the summary() or head() functions to take a look.\n\nhead(study.two.gen)\n\n# A tibble: 6 × 12\n  participant_ID mean.acc mean.self study     AGE SHIPLEY  HLVA FACTOR3 QRITOTAL\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 studytwo.1        0.411      6.07 studyt…    26      27     6      50        9\n2 studytwo.10       0.607      8.5  studyt…    38      24     9      58       15\n3 studytwo.100      0.875      8.93 studyt…    66      40    13      60       20\n4 studytwo.101      0.964      8.5  studyt…    21      31    11      59       14\n5 studytwo.102      0.714      7.07 studyt…    74      35     7      52       18\n6 studytwo.103      0.768      5.07 studyt…    18      40    11      54       15\n# ℹ 3 more variables: GENDER &lt;chr&gt;, EDUCATION &lt;chr&gt;, ETHNICITY &lt;chr&gt;\n\nsummary(study.two.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:172         Min.   :0.4107   Min.   :3.786   Length:172        \n Class :character   1st Qu.:0.6786   1st Qu.:6.411   Class :character  \n Mode  :character   Median :0.7679   Median :7.321   Mode  :character  \n                    Mean   :0.7596   Mean   :7.101                     \n                    3rd Qu.:0.8393   3rd Qu.:7.946                     \n                    Max.   :0.9821   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :29.00  \n 1st Qu.:25.00   1st Qu.:32.75   1st Qu.: 7.750   1st Qu.:47.00  \n Median :32.50   Median :36.00   Median : 9.000   Median :51.00  \n Mean   :35.37   Mean   :35.13   Mean   : 9.064   Mean   :51.24  \n 3rd Qu.:44.00   3rd Qu.:39.00   3rd Qu.:11.000   3rd Qu.:56.25  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:172         Length:172         Length:172        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :14.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.88                                                           \n 3rd Qu.:16.00                                                           \n Max.   :20.00                                                           \n\n\nEven though you have done this before, you will want to do it again, here, and pay particular attention to what you see, for the numeric variables, in the information about minimum (Min.) and maximum (Max.) values."
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-3-use-histograms-to-examine-the-distributions-of-variables",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-3-use-histograms-to-examine-the-distributions-of-variables",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "Step 3: Use histograms to examine the distributions of variables",
    "text": "Step 3: Use histograms to examine the distributions of variables\n\nRevise: practice to strengthen skills\n\n\nTask 5 – Draw histograms to examine the distributions of variables\n\n\nhint: Task 5\nUse ggplot() with geom_histogram().\nWhen we create a plot, we take things step-by-step.\nHere’s an example you have seen before: run the lines of code and see the result in the Plots window in R-Studio.\n\nggplot(data = study.two.gen, aes(x = mean.acc)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThese are the steps, set out one at a time:\n\nggplot(...) you tell R you want to make a plot using the ggplot() function\nggplot(data = study.two.gen ...) you tell R you want to make a plot with the study.two.gen data\nggplot(..., aes(x = mean.acc)) you tell R that you want to make a plot with the variable mean.acc – here, you specify the aesthetic mapping, x = mean.acc\nggplot(...) + geom_histogram() you tell R you want to plot values of mean.acc as a histogram\n\nNotice that the code works the same whether we have the different bits of code on the same line or in a series of lines.\n\n\nRevise: make sure you are confident about doing these things\n\n\nTask 6 – Practice editing the appearance of a histogram plot step-by-step\nStart by constructing a basic histogram.\n\nhint: Task 6 – Choose whichever numeric variable from the study.two.gen dataset you please\n\n\nhint: Task 6 – Use the line-by-line format to break the plot code into steps\nIt will make it easier to read, and it will make it easier to add edits e.g.\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe are going to revise editing:\n\nThe appearance of the bars using binwidth;\nThe colour of the background using theme_bw();\nThe appearance of the labels using `labs().\n\nThen we are going to try some new moves:\n\nSetting the x-axis limits to reflect the full range of possible scores on the x-axis variable;\nAdding annotation – here, a vertical line – indicating the sample average for a variable.\n\n\n\nQuestions: Task 6\n\nQ.1. Edit the appearance of the bars by specifying a binwidth value.\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\nQ.2. Then add an edit to the appearance of the background using theme_bw().\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nQ.3. Then add an edit to the appearance of the labels using labs().\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"frequency count\")\n\n\n\n\n\n\n\n\n\n\n\nIntroduce: make some new moves\n\nQ.4. Now add an edit by setting the x-axis limits using x.lim().\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\nQ.5. Then add an edit to draw a vertical line to show the mean value of the variable you are plotting.\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40) +\n  geom_vline(xintercept = mean(study.two.gen$SHIPLEY), colour = \"red\", size = 1.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\nQ.6. Can you find information on how to define the limits on the x-axis and on the y-axis?\nhint: Q.6. You can see the information in this week’s how-to but try a search online for “ggplot reference xlim”.\nA.6. See ggplot reference information on setting limits here:\n\nhttps://ggplot2.tidyverse.org/reference/lims.html\n\nQ.7. Can you find information on how to a reference line?\nhint: Q.7. You can see the information in this week’s how-to but try a search online for “ggplot reference vline”.\nA.7. See ggplot reference information on setting limits here:\n\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-4-now-draw-scatterplots-to-examine-associations-between-variables",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-4-now-draw-scatterplots-to-examine-associations-between-variables",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "Step 4: Now draw scatterplots to examine associations between variables",
    "text": "Step 4: Now draw scatterplots to examine associations between variables\n\nConsolidation: should be no surprises here\n\n\nTask 7 – Create a scatterplot to examine the association between some variables\nBetween the outcome mean.acc and each of three numeric potential predictor variables SHIPLEY, HLVA and AGE.\n\nhint: Task 7 – We are working with geom_point() and you need x and y aesthetic mappings.\n\n\nhint: Task 7 – The outcome variable mean.acc has to be mapped to the y-axis using ...y = ...\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nRevise: make sure you are confident about doing these things\n\n\nTask 8 – Edit the appearance of each plot step-by-step\n\nhint: Task 8 – You may want to use the same plot appearance choices for all plots\nBecause a consistent appearance is generally neater and easier for your audience to process.\n\n\nhint: Task 8 – You can find links to reference information on options in the how-to guide\nUse the information to make the plots pleasing in appearance to you.\n\n\nhint: Task 8 – Do not be afraid to copy then paste code you re-use.\nBut be careful that things like axis values are sensible for each variable.\n\n\nQuestions: Task 8\n\nQ.8. – First, edit the appearance of the points using alpha, size, shape, and colour:\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')\n\n\n\n\n\n\n\n\n\nQ.9. – Then edit the colour of the background using theme_bw():\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nQ.10. – Then edit the appearance of the labels using labs():\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"Age (Years)\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\n\n\nIntroduce: make some new moves\n\nQ.11. – Then set the x-axis and y-axis limits to the minimum-maximum ranges of the variables you are plotting\nhint: Q.11. – For these plots the y-axis limits will be the same because the outcome stays the same\nhint: Q.11. – But the x-axis limits will be different for each different predictor variable\nhint: Q.11. – The minimum value will always be 0\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\") +\n  xlim(0, 16) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"Age (Years)\", y = \"mean accuracy\") +\n  xlim(0, 80) + ylim(0, 1)"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-5-use-correlation-to-to-answer-the-research-questions",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-5-use-correlation-to-to-answer-the-research-questions",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "Step 5: Use correlation to to answer the research questions",
    "text": "Step 5: Use correlation to to answer the research questions\n\nRevise: make sure you are confident about doing these things\nOne of our research questions is:\n\nWhat person attributes predict success in understanding?\n\n\n\nTask 9 – Examine the correlations between the outcome variable and predictor variables\n\nhint: Task 9 – We use cor.test()\n\n\nhint: Task 9 – You can look at the how-to guide for more advice\nYou need to run three separate correlations:\n\nbetween mean accuracy and SHIPLEY\nbetween mean accuracy and HLVA and\nbetween mean accuracy and AGE\n\n\ncor.test(study.two.gen$SHIPLEY, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$SHIPLEY and study.two.gen$mean.acc\nt = 6.8493, df = 170, p-value = 1.299e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3390103 0.5746961\nsample estimates:\n      cor \n0.4650537 \n\n\n\ncor.test(study.two.gen$HLVA, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$HLVA and study.two.gen$mean.acc\nt = 7.5288, df = 170, p-value = 2.866e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3787626 0.6044611\nsample estimates:\n      cor \n0.5000559 \n\n\n\ncor.test(study.two.gen$AGE, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$AGE and study.two.gen$mean.acc\nt = 0.30121, df = 170, p-value = 0.7636\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1269774  0.1721354\nsample estimates:\n       cor \n0.02309589 \n\n\nNow use the results from the correlations to answer the following questions.\n\nQ.12. – What is r, the coefficient for the correlation between mean.acc and SHIPLEY?\nA.12. – r = 0.4650537\nQ.13. – Is the correlation between mean.acc and HLVA significant?\nA.13. – r is significant, p &lt; .05\nQ.14. – What are the values for t and p for the significance test for the correlation between mean.acc and AGE?\nA.14. – t = 0.30121, p = 0.7636\nQ.15. – For which pair of outcome-predictor variables is the correlation the largest?\nA.15. – The correlation is the largest between mean.acc and HLVA.\nQ.16. – What is the sign or direction of each of the correlations?\nA.16. – All the correlations are positive."
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-6-use-a-linear-model-to-to-answer-the-research-questions",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-6-use-a-linear-model-to-to-answer-the-research-questions",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "Step 6: Use a linear model to to answer the research questions",
    "text": "Step 6: Use a linear model to to answer the research questions\n\nIntroduce: Make some new moves\nOne of our research questions is:\n\nWhat person attributes predict success in understanding?\n\n\n\nTask 10 – Examine the relation between outcome mean accuracy (mean.acc)\nAnd each of the predictors: SHIPLEY, HLVA and AGE\n\nhint: Task 10 – Use lm()\n\n\nhint: Task 10 – Run three separate lm() analyses\nAll with mean.acc as the outcome but each with one predictor variable\n\n\nhint: Task 10 – See the how-to guide for example code\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = study.two.gen)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.285771 -0.076662  0.002099  0.079416  0.257793 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.308027   0.066426   4.637 7.01e-06 ***\nSHIPLEY     0.012854   0.001877   6.849 1.30e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.107 on 170 degrees of freedom\nMultiple R-squared:  0.2163,    Adjusted R-squared:  0.2117 \nF-statistic: 46.91 on 1 and 170 DF,  p-value: 1.299e-10\n\n\n\nmodel &lt;- lm(mean.acc ~ HLVA, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27457 -0.06777  0.01474  0.08025  0.23146 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.522016   0.032544  16.040  &lt; 2e-16 ***\nHLVA        0.026207   0.003481   7.529 2.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1047 on 170 degrees of freedom\nMultiple R-squared:  0.2501,    Adjusted R-squared:  0.2456 \nF-statistic: 56.68 on 1 and 170 DF,  p-value: 2.866e-12\n\n\n\nmodel &lt;- lm(mean.acc ~ AGE, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ AGE, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.34684 -0.08464  0.01084  0.08323  0.21968 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.7519965  0.0267206  28.143   &lt;2e-16 ***\nAGE         0.0002136  0.0007092   0.301    0.764    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1208 on 170 degrees of freedom\nMultiple R-squared:  0.0005334, Adjusted R-squared:  -0.005346 \nF-statistic: 0.09073 on 1 and 170 DF,  p-value: 0.7636\n\n\nNotice that we do the linear model in the steps:\n\nmodel &lt;- lm(...) fit the model using lm(...), and give the model a name, here, we call it model;\n...lm(mean.acc ~ SHIPLEY...) tell R you want a model of the outcome mean.acc predicted ~ by the predictor SHIPLEY\n...data = study.two) tell R that the variables you name in the formula live in the study.two dataset.\nsummary(model) ask R for a summary of the model you called model.\n\nNotice that R has a general formula syntax:\noutcome ~ predictor *or* y ~ x\n\nand uses the same format across a number of different functions;\neach time, the left of the tilde symbol ~ is some output or outcome;\nand the right of the tilde ~ is some input or predictor or set of predictors.\n\n\n\nQuestions: Task 10\nIf you look at the model summary you can answer the following questions.\n\nQ.17. What is the estimate for the coefficient of the effect of the predictor HLVA on mean.acc?\nA.17. 0.026207\nQ.18. Is the effect significant?\nA.18. It is significant, p &lt; .05\nQ.19. What are the values for t and p for the significance test for the coefficient?\nA.19. t = 7.529, p = 2.87e-12\nQ.20. How would you describe in words the shape or direction of the association between HLVA and mean.acc?\nA.20. The slope coefficient – and a scatterplot (draw it) – suggest that as HLVA scores increase so also do mean accuracy scores.\nQ.21. How how would you describe the relations apparent between the predictor and outcome in all three models?\nA.21. It is possible to see, given coefficient estimates, that the association between predictor and outcome is positive for each model: mean accuracy appears to increase for increasing values of SHIPLEY vocabulary, HLVA health literacy, and age."
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-7-use-a-linear-model-to-generate-predictions",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-workbook-answers.html#step-7-use-a-linear-model-to-generate-predictions",
    "title": "2023-24-PSYC122-w17-workbook-answers",
    "section": "Step 7: Use a linear model to generate predictions",
    "text": "Step 7: Use a linear model to generate predictions\n\nIntroduce: make some new moves\n\n\nTask 11 – We can use the model we have just fitted to plot the model predictions\n\nhint: Task 11 – We are going to draw a scatterplot and add a line\nThe line will show the model predictions, given the model intercept and effect coefficient estimates.\nFirst fit a model and get a summary: model the relationship between mean.acc and HLVA\n\nmodel &lt;- lm(mean.acc ~ HLVA, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27457 -0.06777  0.01474  0.08025  0.23146 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.522016   0.032544  16.040  &lt; 2e-16 ***\nHLVA        0.026207   0.003481   7.529 2.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1047 on 170 degrees of freedom\nMultiple R-squared:  0.2501,    Adjusted R-squared:  0.2456 \nF-statistic: 56.68 on 1 and 170 DF,  p-value: 2.866e-12\n\n\n\nQ.22. What is the coefficient estimate for the intercept?\nA.22. 0.522016\nQ.23. What is the coefficient estimate for the slope of HLVA (see earlier)?\nA.23. 0.026207\n\nSecond, use the geom_point() to draw a scatterplot and geom_abline() function to draw the prediction line representing the association between this outcome and predictor\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  geom_abline(intercept = 0.522016, slope = 0.026207, colour = \"red\", size = 1.5) +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\") +\n  xlim(0, 15) + ylim(0, 1)\n\n\n\n\n\n\n\n\nYou can see reference information here:\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html"
  },
  {
    "objectID": "PSYC122/data/week12/122_wk12_labAct2.html#variable-types",
    "href": "PSYC122/data/week12/122_wk12_labAct2.html#variable-types",
    "title": "122_wk12_labActivity2",
    "section": "Variable types",
    "text": "Variable types\nQuestions 5a: What are the variable types for the implicit (IAT_RT) and the explicit (VapingQuestionnaireScore) attitude variables?\nBoth can be considered continuous variables and at least at interval level."
  },
  {
    "objectID": "PSYC122/data/week12/122_wk12_labAct2.html#missing-data",
    "href": "PSYC122/data/week12/122_wk12_labAct2.html#missing-data",
    "title": "122_wk12_labActivity2",
    "section": "Missing data",
    "text": "Missing data\n\ndat &lt;- dat %&gt;% \n  filter(!is.na(VapingQuestionnaireScore)) %&gt;% \n  filter(!is.na(IAT_RT))\n\nQuestion 5b How many people had missing data?\nBefore we removed participants with missing data, we had 104 observations, now we have 96. So there must have been 8 participants without a score on one or the other variable."
  },
  {
    "objectID": "PSYC122/data/week12/122_wk12_labAct2.html#normality",
    "href": "PSYC122/data/week12/122_wk12_labAct2.html#normality",
    "title": "122_wk12_labActivity2",
    "section": "Normality",
    "text": "Normality\n\nggplot(dat, aes(x = VapingQuestionnaireScore)) + \n  geom_histogram(binwidth = 10) +\n  theme_bw()\n\n\n\n\n\n\n\nqqPlot(x = dat$VapingQuestionnaireScore)\n\n\n\n\n\n\n\n\n[1] 35 95\n\nggplot(dat, aes(x = IAT_RT)) + \n  geom_histogram(binwidth = 10) +\n  theme_bw()\n\n\n\n\n\n\n\nqqPlot(x = dat$IAT_RT)\n\n\n\n\n\n\n\n\n[1] 25 54\n\n\nQuestion 5c What do you conclude from the histograms and the qq-plots? Are the VapingQuestionnaireScore and the IAT_RT normally distributed?\nYes. Both histograms resemble a normal distribution (bell curve) and the open circles in the qq-plots fall within the blue stripy lines."
  },
  {
    "objectID": "PSYC122/data/week12/122_wk12_labAct2.html#linearity-and-homoscedasticity",
    "href": "PSYC122/data/week12/122_wk12_labAct2.html#linearity-and-homoscedasticity",
    "title": "122_wk12_labActivity2",
    "section": "Linearity and homoscedasticity",
    "text": "Linearity and homoscedasticity\n\nggplot(dat, aes(x = IAT_RT, y = VapingQuestionnaireScore)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  labs(x = \"Implicit attitude\", y = \"Explicit attitude\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nQuestion 5d What do you conclude from the scatterplot in terms of the homoscedasticity of the data and the linearity, direction and strength of the relationship? What does the scatterplot tell you about possible issues (outliers, range restrictions)?\nThe data look like a cloud without a clear direction. This suggests the relationship might might be weak. In terms of linearity, the scatterplot doesn’t suggest any curvilinear relationships. Variance seems quite constant, but there do seem to be few people with negative IAT_RT (Implicit attitude) scores, suggesting few people held the view that vaping is positive."
  },
  {
    "objectID": "PSYC122/data/week12/122_wk12_labAct2.html#create-a-new-data-frame-that-only-includes-the-relevant-variables",
    "href": "PSYC122/data/week12/122_wk12_labAct2.html#create-a-new-data-frame-that-only-includes-the-relevant-variables",
    "title": "122_wk12_labActivity2",
    "section": "Create a new data frame that only includes the relevant variables",
    "text": "Create a new data frame that only includes the relevant variables\n\ndat_matrix &lt;- dat %&gt;%\n  select(Age, IAT_RT, VapingQuestionnaireScore) %&gt;%\n  as.data.frame() # Make sure tell R that dat is a data frame"
  },
  {
    "objectID": "PSYC122/data/week12/122_wk12_labAct2.html#create-a-matrix-of-scatterplots",
    "href": "PSYC122/data/week12/122_wk12_labAct2.html#create-a-matrix-of-scatterplots",
    "title": "122_wk12_labActivity2",
    "section": "Create a matrix of scatterplots",
    "text": "Create a matrix of scatterplots\n\npairs(dat_matrix)\n\n\n\n\n\n\n\n\nQuestion 8a What do you conclude from the scatterplots?\nThe scatterplots with age suggest that age is highly skewed with only a few participants older than 25. For now, let’s say we’ll therefore calculate Spearman’s rho, rather than Pearson’s r. That is ok for now, but if you were analysing these data for a research project, you’d want to have a closer look at the age variable (think histogram, qq-plot, and think about either collecting more data from older participants or transforming the variable (more about that next year)."
  },
  {
    "objectID": "PSYC122/data/week12/122_wk12_labAct2.html#conduct-intercorrelation-multiple-correlations",
    "href": "PSYC122/data/week12/122_wk12_labAct2.html#conduct-intercorrelation-multiple-correlations",
    "title": "122_wk12_labActivity2",
    "section": "Conduct intercorrelation (multiple correlations)",
    "text": "Conduct intercorrelation (multiple correlations)\n\nintercor_results &lt;- correlate(x = dat_matrix, # our data\n                          test = TRUE, # compute p-values\n                          corr.method = \"spearman\", # run a spearman test \n                          p.adjust.method = \"bonferroni\") # use the bonferroni correction\nintercor_results\n\n\nCORRELATIONS\n============\n- correlation type:  spearman \n- correlations shown only when both variables are numeric\n\n                            Age    IAT_RT    VapingQuestionnaireScore   \nAge                           .     0.156                      -0.086   \nIAT_RT                    0.156         .                      -0.022   \nVapingQuestionnaireScore -0.086    -0.022                           .   \n\n---\nSignif. codes: . = p &lt; .1, * = p&lt;.05, ** = p&lt;.01, *** = p&lt;.001\n\n\np-VALUES\n========\n- total number of tests run:  3 \n- correction for multiple testing:  bonferroni \n- WARNING: cannot compute exact p-values with ties\n\n                           Age IAT_RT VapingQuestionnaireScore\nAge                          .  0.384                    1.000\nIAT_RT                   0.384      .                    1.000\nVapingQuestionnaireScore 1.000  1.000                        .\n\n\nSAMPLE SIZES\n============\n\n                         Age IAT_RT VapingQuestionnaireScore\nAge                       96     96                       96\nIAT_RT                    96     96                       96\nVapingQuestionnaireScore  96     96                       96\n\n\nQuestion 8b What do you conclude from the results of the correlation analysis?\nNo significant correlation with age was found."
  },
  {
    "objectID": "PSYC122/data/week13/122_wk13_labAct2.html",
    "href": "PSYC122/data/week13/122_wk13_labAct2.html",
    "title": "122_wk13_labActivity2",
    "section": "",
    "text": "Question 1a If our hypothesis is correct, what type of correlation (if any) should we observe between students’ mean anxiety levels and the variable n_weeks? A negative correlation"
  },
  {
    "objectID": "PSYC122/data/week13/122_wk13_labAct2.html#lab-activity-2---statistics-anxiety-and-engagement-in-module-activities",
    "href": "PSYC122/data/week13/122_wk13_labAct2.html#lab-activity-2---statistics-anxiety-and-engagement-in-module-activities",
    "title": "122_wk13_labActivity2",
    "section": "",
    "text": "Question 1a If our hypothesis is correct, what type of correlation (if any) should we observe between students’ mean anxiety levels and the variable n_weeks? A negative correlation"
  },
  {
    "objectID": "PSYC122/index.html",
    "href": "PSYC122/index.html",
    "title": "Statistics for Psychologists II",
    "section": "",
    "text": "Welcome\nWelcome to PSYC122!\nThis module builds on the knowledge and skills acquired in Statistics for Psychologists 1 (PSYC121). You will continue to practise data handling, data processing and data visualisation, using R and R Studio. In addition, you will learn about statistical methods to test whether two (or more) variables are associated and how to implement those methods in R and R Studio.\nWatch the video below (~ 5 minutes) to get a short overview of the topics we will cover in weeks 11 to 15.\n\nThis page gives you access to all the materials that you will need. You will have timetabled lab classes during which you are expected to work through a series of exercises to practise with the week’s material. Before you come to your lab session, you should watch the lectures for that week, read the relevant book chapter and complete the pre-lab activities. Following all that and to check that you’ve understood the week’s materials, you can complete a quick quiz.\nThere will be class tests in weeks 15 and 20. These take place, in person, during your regular lab session.\n\n\nAsking for help\nWe have carefully prepared and refined the lab materials in this course over several years, and we feel that the pace of the materials is just right for our students. Some students will complete them more quickly, and others more slowly - both of these scenarios are absolutely fine. You should work at the pace that suits you best, making sure you understand the materials before you move forward.\nIt is fairly inevitable that you will get stuck on the lab materials in this module at some point. This might be in Week 11, Week 12, or later. When you do, it’s important you reach out for help:\n\nAsk your friends on your table. We’ve designed this teaching space to help collaborative work. You are encouraged to work with other students. Make sure you ask others to explain how they’ve solved an exercise. Make sure you help out others where you can. Always make sure you understand the code and the exercise; don’t simply be satisfied that you’ve got the right answer.\nAsk a GTA or Lecturer. Our Graduate Teaching Assistants are there to help you. There are no “stupid questions” in statistics, so just ask the GTAs any question about what you’re doing. Likewise, ask the Lecturer.\nAsk on the Discussion Forum. On the PSYC122 moodle page you will find a Discussion Forum. This is a great way to ask a question outside of the lab sessions. It might seem scary to ask a question in the forum, but please don’t be afraid to do this. If you have a question, you can bet 30+ other students also have the same question! So by asking the question on the forum, you help out many more people on the module. A friendly GTA or Lecturer will be along to answer the question as soon as possible (we aim for within 48 hours during the working week).\nAsk on the module Q & A session. Each week we hold a “Q&A” online session where we will try and resolve any general queries and problems. It’s an ideal time to discuss things that students are struggling with or confused about. You can ask on the discussion forum above and then we might be able to pick up the issues and discuss them, but you can also ask in the session itself.\n\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nMargriet Groen\nm.groen at lancaster dot ac dot uk\n\n\nRob Davies\nr.davies1 at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC122"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html",
    "href": "PSYC121/Week4.html",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "",
    "text": "Watch Part 1\n\nWatch Part 2\n\nWatch Lecture Part 3\n```\nWatch Lecture Part 4\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#customisation-of-data-plots",
    "href": "PSYC121/Week4.html#customisation-of-data-plots",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "3.1 - Customisation of data plots",
    "text": "3.1 - Customisation of data plots\nStep 1. Set up a project / folder that you created last week.\nStep 2. Bring the week_4_2024.zip file into R Studio server. Like last week, upload the zip file. Launch the week_4 R script as before, and read in the data file.\n{If you’ve done Step 1 &2 already as a pre-lab preparation, super, pat yourself on the back, skip these steps an move on)}\nStep 3. Once again, we’re gong to be using commands from the tidyverse library (the pipe operator is one example) so we need to ensure that it’s active. Run the command\nlibrary(tidyverse)\nStep 4. Read in the datafiles that will be on the server. There’s already a script line for this, you just need to change the file name (and we’ve done this in previous weeks)\nStep 5. We’ve provided a suggestion of how you can complete the visualisation challenge task from week 3.\nStep 6. Customize you graph work. We’ve provided some suggestions about adding titles and labels for your graph. Edit and play with the script lines to make them useful to you and to understand how they work. Note that the ggplot instructions have a similar structure / grammar to the group_by() instructions that we used: piping a data frame to a (here, plotting) function and piping that to an output or summarisation format.\n\nTry change the text, the colours, and so on of the graphs.\nAdd comments for yourself about what the different commands do. The idea is to learn by trying different things out (changing values, taking out elements of the command, putting other is) and record for yourself.\nIf you are struggling or not sure, try look at help files.",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week4.html#z-scores",
    "href": "PSYC121/Week4.html#z-scores",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "3.2 - z-scores",
    "text": "3.2 - z-scores\n\nHint / Reminder: Sketch a normal (z score) distribution and mark the mean/mode, and mark off the relevant parts of the question so you know what you are trying to achieve and how to interpret any calculations you make.\n\n\nHint/ Guide 2. For questions 6 & 7, typically in psychology we use the 5% level as a cutoff to decide, in broadly described terms, whether something is extreme or unlikely vs. at least somewhat plausible or likely.\n\n\n3.2.1 z-scores 1\nz-score distributions\nQ1. What is the relationship between the sign of a z-score and its position in a distribution?\nQ2. If a distribution has a mean of 100 and a standard deviation of 10, what is the raw score equivalent to a z-score of 1.96?\nQ3. If a distribution has a mean of 157 and a standard deviation of 19, what is the raw score equivalent to a z-score of 1?\n\n\n3.2.2 z-scores 2 Using z-score tables\nQ4. What proportion of scores lie between the mean and a z-score of 0.5?\nQ5. What is the combined proportion of scores lying between z=-1.2 and z=.85?\n\n\n3.2.3 z-scores 3 Applying z-scores to inferential problems\nQ6. A Neuropsychologist has presented a test of face recognition to 200 neurotypical participants and finds that the scores are normally distributed with a mean of 85 and the standard deviation of 12. Two brain-damaged patients are also given the test. The one with right hemisphere brain damage scored 58 and the one with left hemisphere damage scored 67.\n\nWhat is the z score of the right hemisphere patient when compared to the neurotypical group?\nWhat proportion of neurotypical participants score lower that this patient?\nIs this patient likely to belong to the population of neurotypical participants? (justify your answer)\nWhat is the z score of the left hemisphere patient when compared to the neurotypical group?\nWhat proportion of neurotypical participants score lower than this patient?\nIs this patient likely to belong to the population of neurotypical participants? (justify your answer)\n\n\n\n3.2.4 Final z-score challenge\nCome back to this afterwards for some extra practice if you want:\nQ7. Tom Bunion has completed a huge research study and measured the foot size of men and women and found each to be normally distributed. The men have a mean size of 55 with a standard deviation of 5 and the women a mean of 33 and a standard deviation of 5. Joanna Toes has foolishly measured two individuals but forgotten to note their gender. These have foot sizes of 37 and 47. To which gender is each more likely to belong? What evidence is there for this?",
    "crumbs": [
      "Home",
      "PSYC121",
      "4. Customisation of graphs, and z-scores"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html",
    "href": "PSYC121/Week7.html",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "",
    "text": "Watch Part 1\n\nWatch Part 2\n\nWatch Part 3\n\nWatch Part 4\n\nDownload the lecture slides here\n\n\nChapter 12 of Howell\nToday we will look in a bit more detail at people’s estimates of the average UK salary. We will first plot this data using geom_histogram() and also geom_boxplot(). When we do this, we’ll see that there are some unusual values, and we’ll need to do a bit of data wrangling to remove them, using the filter() command. We’ll then turn to the conceptual ideas of the lecture - how can we tell if the mean of our sample is unusual, or whether we would actually expect this mean value under the null hypothesis? Finally, we’ll continue to develop our skills in data visualisation by exploring geom_density() plots.",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html#reading",
    "href": "PSYC121/Week7.html#reading",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "",
    "text": "Chapter 12 of Howell\nToday we will look in a bit more detail at people’s estimates of the average UK salary. We will first plot this data using geom_histogram() and also geom_boxplot(). When we do this, we’ll see that there are some unusual values, and we’ll need to do a bit of data wrangling to remove them, using the filter() command. We’ll then turn to the conceptual ideas of the lecture - how can we tell if the mean of our sample is unusual, or whether we would actually expect this mean value under the null hypothesis? Finally, we’ll continue to develop our skills in data visualisation by exploring geom_density() plots.",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html#task-3---sample-size-size-of-effect-and-the-one-sample-t-test",
    "href": "PSYC121/Week7.html#task-3---sample-size-size-of-effect-and-the-one-sample-t-test",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "Task 3 - Sample size, size of effect, and the one sample t-test",
    "text": "Task 3 - Sample size, size of effect, and the one sample t-test\nIn the lecture this week, Tom used an application to show the process of sampling data. You can access this application at the link below. There are three “parameters” you can change in this:\n\nThe true mean of the effect: Think of this like the bias that was set up in your deck of cards last week. There is some true state out there in the world, and we are going to draw samples from a distribution of data that has a mean that equals this value. If you make this 100, then the true mean is equal to that under the null hypothesis (there is no effect).\nThe standard deviation of the data: This sets how variable the data are in this population. If the data are more variable, then our samples will produce estimations that are less accurate of the true mean value.\nThe sample size: How many observations are drawn in the sample. These are represented by the yellow circles in the plot.\n\nEach time you draw a sample the data points are plotted in yellow and the mean of the sample is marked with the red line. The application also runs a one-sample t-test against the expected mean under the null hypothesis, of 100. The null hypothesis is also represented by the static distribution presented in grey, centred on 100.\nThings to try:\n\nStart with a sample size of 10, and a mean of the effect of 110 (SD = 15). How often do you get a significant result (p &lt; .05) when you draw a new sample?\nNow try changing the mean of the effect to 120. Does this increase or decrease the likelihood of getting significant results? What about changing to 130?\nNow keep the mean effect constant (say 110), but increase the sample size. Try 5, then 10, 15, and so on. Does this increase or decrease the likelihood of getting significant results?\nSet the mean of the effect to 100 and the sample size to 10. Keep drawing new samples, noting each time the p value. You will evenutally get a p value of &lt; .05. What type of error is this?\n\nClick here for the one-sample t-test application",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html#task-4---practising-filtering",
    "href": "PSYC121/Week7.html#task-4---practising-filtering",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "Task 4 - Practising filtering",
    "text": "Task 4 - Practising filtering\n\nFiltering is very useful for selecting certain sub-sets of our data. Here we have given you an example of how we select a sub-set of data based on two conditions from two different columns:\n\n\nname_of_the_data_object %&gt;% \n  filter(home_location_in_UK == \"NW\" & sibling_order == \"oldest\")\n\nWe have given you a few different columns to look at and to use in practicing your filter commands:\n\nsibling_order: what position in age was the respondent within their siblings\nhome_location: UK / Asia / Europe, etc\nhome_location_in_UK: NW, NE, etc (NA is non-UK residents)\nattention_check: respondents were asked “click strongly agree to show you are paying attention” - some people failed this!!!\n\nGain some skills in filtering by trying to complete the following filters. Use the filtered data object that has 120 rows. We’ve put in () the number of rows you should see in the resulting object, after the filter.\n\nJust those people who come from the North East (11 rows)\nThose people who come from South East and are the oldest child within their siblings (10 rows)\nThose people who failed the attention check (16 rows)\nThose people passed the attention check, are from the UK, and are the youngest child (35 rows)\nThose people who are NOT from the North West (hint: you’ll need to use !=) (49 rows)\nThose people who are from the South East or (|) the South West (25 rows)",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html#extra-content",
    "href": "PSYC121/Week7.html#extra-content",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "Extra content!",
    "text": "Extra content!\nIn the data for this week, we used a one-sample t-test to see if the salary estimations were significantly different from the median salary of £34,963. You will have found that they are a significant underestimate. But maybe this is because there were quite a few people who weren’t paying attention. Maybe those people gave low estimations, and this is driving the “underestimation effect”. Use a filter command to remove those participants who failed the attention check, and run another one-sample t-test on this new subset of the data.",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week7.html#week-7-quiz",
    "href": "PSYC121/Week7.html#week-7-quiz",
    "title": "7. Filtering data and testing means (one-sample t-test)",
    "section": "Week 7 Quiz",
    "text": "Week 7 Quiz\nYou can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "7. Filtering data and testing means (one-sample t-test)"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html",
    "href": "PSYC121/Week2.html",
    "title": "2. Descriptive statistics in RStudio",
    "section": "",
    "text": "Due to illness, we’re releasing some previously recorded lecture videos that explain the week 2 content. Unfortuantely I’m full of cold right now - and making a video recording is just creating a muffled and hard to hear version of the ideas :-(\nAlso, just bear in mind - these lectures in PSYC121 are designed to provide an explanation of the conceptual and computational work involved in some core analytic material. The workshops are designed (a) to put these ideas into practice (b) to work with R Studio in practicing with data, as a tool that can help to explore data. So the lectures and lab complement each other, and we will continue to takle this approach across the module.\nWatch Part 1\n\nWatch Part 2\n\nWatch Part 3\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#r-studio-tasks-recap",
    "href": "PSYC121/Week2.html#r-studio-tasks-recap",
    "title": "2. Descriptive statistics in RStudio",
    "section": "3.1 R Studio tasks recap",
    "text": "3.1 R Studio tasks recap\nFor a reminder of how to start RStudio, see week 1’s instructions\n(remember: off campus, you will need to be on the VPN)\n\nA word of advice (from David Howell’s statistics book: One more word of advice I can’t resist adding what is perhaps the best advice I have. If there is something that you don’t understand, just remember that “Google is your friend.” She certainly is mine. (Well, maybe Google is getting a bit pushy, but there are many other search sites.) If you don’t understand what Fisher’s Exact Test is, or you don’t like my explanation, go to Google and type in Fisher’s Exact Test. I just did that and had 260,000 hits. You can’t tell me that there isn’t going to be something useful in there.)",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#bringing-data-scripts-and-files-into-r-studio",
    "href": "PSYC121/Week2.html#bringing-data-scripts-and-files-into-r-studio",
    "title": "2. Descriptive statistics in RStudio",
    "section": "3.2 Bringing data, scripts and files into R Studio",
    "text": "3.2 Bringing data, scripts and files into R Studio\nIn week 1, we had a tiny dataset (relatively speaking) that we entered into R through a script line. That worked for what is was. But it’s going to become painful and tedious when (a) we want to work with larger datasets (b) we have data more complex than a 1-dimensional list of numbers (think about some 2-dimensional data sheets you might have encountered in excel for example)\nR can handle data files, and this week we’re going to explore them. Within R, we can specify ‘data frames’ which can have, essentially, multiple columns of data, and we can link data files to data frames for processing\nTo make things straightforward, each week we’ll provide students with a “zip” file that contains the script to start from (which you can expand and annotate etc, and save on your file space). We’ll also provide a data file or data files for you to use in the zip file. R can then import these files into the RStudio environment. So when you upload the zip file, you can import the data AND you can open up the script",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#main-lab-exercise---descriptive-information-in-r-studio-penelope-the-cow",
    "href": "PSYC121/Week2.html#main-lab-exercise---descriptive-information-in-r-studio-penelope-the-cow",
    "title": "2. Descriptive statistics in RStudio",
    "section": "3.3 Main lab exercise - descriptive information in R Studio (Penelope the cow)",
    "text": "3.3 Main lab exercise - descriptive information in R Studio (Penelope the cow)\nSome years ago, a large group of participants gave an estimate of the weight of Penelope the cow. Just over 17,000 guesses. And the distribution of guesses was something like this: \nWhat we can see from this graph is that:\n\nGuesses formed a roughly normal distribution. There is a bit of a skew with a right-hand tail, but this is inevitable as a weight of less than 0 is physically impossible, but there is no limit of the semantics of a large guess.\nThe mean guess weight (1,287 lbs) is very close to the actual (true) weight of the cow (1,355 lbs). So even though lots of people were inaccurate, a central tendency measure has a pretty good alignment with the true weight. This is known as the Wisdom of Crowds phenomenon, first identified by Galton in 1907 (though he suggested using the median weight). The concept of the wisdom of crowds continues to be used and investigated in psychology today (see for example here and here\n\nLet’s look at (a sample of) the PSYC121 student data collected on guessing the weight of Penelope, and ask whether it resembles the properties of this large dataset.\n\nStep 1. Loading the data\nUsing the instructions and advice given on Moodle, get the week_2_2024.zip file and bring the data and R script into R Studio. The week 2 zip file is here\n\n\nStep 2. Using the R script\nLet’s start working with our data, by opening up (clicking on) the script “Week_2.R” file.\nThe first command is to load a library of functions:\n\nlibrary(tidyverse)\n\nTo run this, simply click anywhere on line 1 of the R script to put the cursor there, and press ctrl+enter (cmd+enter on a mac) or click the button called run. You will see a number of messages appear in the console. Don’t worry about these, or worry too much about what exactly this command is doing. Essentially this is giving us some useful tools for our analysis. We will introduce the features of the tidyverse gradually during this course.\nThe data are on the RStudio server if you have followed all the instructions to this point. Note that when you imported the data into the R environment, a command line was generated at the console\n\ncows &lt;- read_csv(\"~/penelope22.csv\")\n\nWhat this command accomplished was to read the spreadsheet called ‘penelope22’ into an object in R called cows. You could use any object label - it doesn’t have to be ‘cows’- but it’s important to then keep that alternate name consistent in what you do next.\nThe command was also generated\n\n View(cows)\n\nwhich presents the data in a window of RStudio. Note that “NA” means not available or missing data. Does this file structure make some sense to you?\n\n\nStep 3. Finding the mean and median estimates\nUse the data to answer the following questions…\n\nWhat is the mean weight estimates?\nWhat is the standard deviation of the estimates?\nWhat is the median weight of the estimates?\nWhich of these central tendency measures is the more accurate measure of the true cow weight? (make a judgement)\nWhat is the mean weight estimate (and standard deviation) for female respondents and non-female (male / non-binary /prefer not to say) respondents?\n\nYou may be thinking, how do I possibly do any of this?! Well this week most of the commands you need are contained in the R script you have downloaded. Also, remember from last week, we explored the R command:\n\nmean(week_1_lecture_data)\n\nThat gave us the mean of the small dataset week_1_lecture_data. This time, we want to explore the penelope dataset. But also, the lecture_data was just a single list of numbers. The penelope22 object is more like a datasheet. So we need to tell R Studio which column we are interested in. RStudio uses the format data$column. So run the followinbg line in the script\n\nmean(cows$estimate) \n\n\nsd(cows$estimate)\n\nSo from this, can you work out what you would do to get the median value (remember from last week how we got the median value?)? Part of the command is given to you, can you change the text so that it works?\n\n\nStep 4. Calculations from a range of columns\nWe have seen that:\n\nmean(cows$estimate) \n\nwill provide a mean of the column “estimate”. In the third column, named “female_estimate”, we have the estimates of just the female respondents. In the fourth column, named “other_estimate”, we have the estimates of the “other” respondents (males and non-binary and prefer not to say).\nSo can you now figure out how you might get information about the estimate from the female data (only) or the non-female data? Try it, based on what you have just done. Does it work?\nYou will find that the result of the this command produces an “NA” result. This means that the answer is “Not Available”, or in other words, is a “missing value”. This is because some of the values in this column are NA, and the mean of a column with NAs will always lead to the result NA.\nInstead, try change the script so it looks like this:\n\nmean(cows$female_estimate, na.rm = TRUE )\n\nAny different? The na.rm = TRUE instruction tells RStudio that missing data can be ignored in this mean calculation. (in technical language, na.rm is a parameter of the function mean that removes the NAs if set to TRUE)\n\n\nStep 5. Simple graphs\nRStudio can be used to create graphical data plots that can help interpret datasets\nThe first thing we can do is create a histogram distribution of guesses from the sample student data to compare with the previous large sample study (i.e. the 17,000 guesses):\n\nhist(cows$estimate)\n\nOne way to alter or adjust the histogram is to change the width of the bars, the intervals, between each plot section. Try run this line from the script\n\nhist(cows$estimate, breaks = ??)\n\nDoes it work? No? What you need to do is replace the two question marks in the script (or better still, create a new instruction line in which you amend this to have a numerical value representing the number of different plot bars. Try at least 3 different values. Look at and think about how this affects the visual distribution.\nWe can also create a “box and whisker plot”. Here’s a general simple description of a box-and-whisker plot as a graphical representation of data:",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week2.html#extension-and-practice.-apply-your-new-skils-to-a-different-dataset",
    "href": "PSYC121/Week2.html#extension-and-practice.-apply-your-new-skils-to-a-different-dataset",
    "title": "2. Descriptive statistics in RStudio",
    "section": "3.4. Extension and practice. Apply your new skils to a different dataset",
    "text": "3.4. Extension and practice. Apply your new skils to a different dataset\nIn the zip file, we also provide data on the estimates of the percentage of immigrants in the UK. This will allow you to explore this variable, create visualisations of the data and its spread. We’ll be looking at a version of this variable in week 3: but for now, can you apply the analysis of the penelope data to the immigration data (report descriptive statistics)? Write some new script lines to investigate this additional dataset, and annotate those new script lines.",
    "crumbs": [
      "Home",
      "PSYC121",
      "2. Descriptive statistics in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/index.html",
    "href": "PSYC121/index.html",
    "title": "Statistics for Psychologists I",
    "section": "",
    "text": "Welcome\nWelcome to the PSYC121 lab material for 2024!\nIn this module we will provide you with an introduction to data handling, data processing and data visualisation. What that means is that, by the end of the this module (at Christmas time), you will be able to take a set of data, look at some basic statistics (e.g., the mean value), filter and process the data in order to answer basic questions about it, and present the data in an appealing way with different graphs. On top of this, you will be able to apply some of your knowledge of the basic “inferential” statistical tests that we will introduce in the lecture series (e.g., “t-test”).\nIn Week 1 we will introduce you to the software that we use to do all this useful work in statistics: “R” and “RStudio”. This is a coding language, and you will be taught the basics of how to write code in order to do all of the above key steps in data analysis. This tuition will continue in Term 2, and in your statistics modules in Year 2. Coding is challenging, but we know from experience that those students who attend classes, who work through the exercises carefully, and who seek help when they need it, do very well on these modules.\nMost of all, it’s important that you recognise that data analysis (statistics) is a critical aspect of the study of psychology. When we want to understand behaviour, we take measurements of that behaviour, which the majority of the time will result in quantitative (numerical) data. In order to understand the behaviour in a meaningful way, we need to conduct all of the above steps in our data analysis workflow. In summary, we cannot investigate psychological processes without the skills and toolbox of statistics and data analysis techniques.\n\n\nWorking at your own pace and seeking help\nWe have carefully prepared and refined the lab materials in this course over several years, and we feel that the pace of the materials is just about right for our students. Some students will complete them more quickly, and others more slowly - both of these scenarios are absolutely fine. You should work at the pace that suits you best, making sure you understand the materials before you move forward.\nIt is fairly inevitable that you will get stuck on the lab materials in this module at some point. This might be in Week 1, Week 2, or later. When you do, it’s important you reach out for help:\n\nAsk your friends on your table. We’ve designed this teaching space to help collaborative work. You are encouraged to work with other students. Make sure you ask others to explain how they’ve solved an exercise. Make sure you help out others where you can. Always make sure you understand the code and the exercise; don’t simply be satisfied that you’ve got the right answer.\nAsk a GTA or Lecturer. Our Graduate Teaching Assistants are there to help you. There are no “stupid questions” in statistics, so just ask the GTAs any question about what you’re doing. Likewise, ask the Lecturer.\nAsk on the Discussion Forum. On the PSYC121 moodle page you will find a Discussion Forum. This is a great way to ask a question outside of the lab sessions. It might seem scary to ask a question in the forum, but please don’t be afraid to do this. If you have a question, you can bet 30+ other students also have the same question! So by asking the question on the forum, you help out many more people on the module. A friendly GTA or Lecturer will be along to answer the question as soon as possible (we aim for within 48 hours during the working week).\nAsk on the module Q & A session. Each week we hold a “Q&A” online session where we will try and resolve any general queries and problems. It’s an ideal time to discuss things that students are struggling with or confused about, and can share ideas and answers. You can ask on the discussion forum above and then we might be able to pick up the issues and discuss them, but also you can ask in the session itself.\n\nAsking good questions - it’s really important that you give us as much information as you can when you ask your questions (in class and especially on the forum). It’s so much harder to help respond to “I can’t do Exercise 5 in Week 7” (because we don’t know why it is that you can’t do it) than for example “In Exercise 5 of Week 7, I’ve managed to read in the data, put the graph looks quite odd. Here is the code I’m using…”\n\n\nCourse Contacts\nIf you have something that needs to be private, then please feel free to email the academic staff at the email addresses below:\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley (Coordinator)\nt.beesley at lancaster dot ac dot uk\n\n\nJohn Towse\nj.towse at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html",
    "href": "PSYC121/Week8.html",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "Watch Part 1\n\nWatch Part 2\n\nWatch Part 3\n\nWatch Part 4\n\nDownload the lecture slides here\nTo play around with the t distribution simulator that Tom uses in the lecture, go here: (on campus, or VPN required)\n\n\nChapter 13 of Howell\nToday we will take a look at summarising means and standard errors (SEs) from our data. We will look at how we plot these together on the one graph (using ggplot() commands that allow us to share mappings between different geoms. We will explore our data on the famous “Stroop Task” and we will use a related-samples t-test to examine the differences between the means of our different conditions in this task.\n\n\n\nOnline tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\n\nCreate a folder for Week 8 and download the Week_8 csv file file and upload it into this new folder in RStudio Server.\n\n\n\n\n\n\nThe “Stroop Effect” is a classic demonstration of automaticity of behaviour. Participants have to say the colour a word is printed in, which is an easy task for a “compatible” stimulus like GREEN, and a much more difficult task for an “incompatible” stimulus like BLUE. We can’t help but read the text - it has seemingly become an automatic process.\n In this task we will calculate the means and standard errors of the means and then we will then plot them using ggplot(). First though, we’ll need to inspect the data and maybe do a bit of data wrangling by using our filter() command.\n\nCreate a new R Markdown document. If you’re unsure about this step, see the instructions from Week 6 (or 7).\nAs usual, add a code chunk with library(tidyverse) and a read_csv command (see above for the link to the csv). Assign the result to a new data object, and call your data something meaningful (perhaps data_w8 or data_stroop but maybe not bestest_most_fantastic_data_on_the_stroop_test_eva_init)\nView the data with View(data_object_name). You will see that the data are a little different from the data we have worked with previously. We have an pID variable, which gives a unique number for each person, but each person has 3 rows of data. This is because the different conditions of the Stroop task reflect a within-subjects variable (related samples). For data like this it is often useful to have them arranged in what is referred to as “long format”, with multiple rows for each response the participant provides. For the current data that means we have a variable called condition, which is our IV, and one called time which is our DV. We also have a column labelled avg_time, which is the average of the 3 time values for each participant (the data is duplicated, which is both normal and necessary with long format data).\nLet’s look at the distribution of time (our DV) as a function of condition. Add another chunk of code and include the following code:\n\n\n# distribution of times by condition\nyour_data_object %&gt;% \n  ggplot() +\n  geom_density(aes(x = missing_column_name_A, fill = missing_column_name_B), alpha = .8) + # you need to EDIT this for Q4\n  theme_dark()\n\n\nYou’ll need to “map” x to time and fill to condition for our geom_density() plot. You can play around with the alpha parameter (which sets the transparency of the elements of the graph), setting it to a value between 0 and 1. Note that this is done OUTSIDE of the aes() command.\nFrom the density plot, it does seem like we have some outlier values. It’s probably best if we remove data for the whole participant if their average time is unusual. To do that, we’ll look at the data using the avg_time column. Add the following code for a geom_histogram() to plot the distribution of values in the new avg_time column.\n\n\n# distribution of average times\nyour_data_object %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = missing_column_name_A), fill = \"pink\") + # you need to EDIT this line\n  theme_classic()\n\n\nLet’s use the filter command we learned last week to remove these high values. Like last week, we will do this (for now) in a fairly unprincipled manner, by “eyeballing” the data (next week we’ll consider something a bit more “scientific”). Complete the filter command so that it keeps only the responses for people that had an avg_time less than 10 seconds. Remember that you need to think about how you are storing the result of this filter process. Do you want to create a new object, or overwrite the existing object?\n\n\n# filter out the high values\nnew_data_object &lt;- # create a new object (or overwrite)\n  your_data_object %&gt;% # original data object \n  filter(insert_an_expression_here)\n\n\n\n\n\n\n\nCheck your result!\n\n\n\nIf you’ve done this correctly, you should now have a data object that has 327 rows (data for 109 participants, with 3 responses each).\n\n\n\nAdd and edit the following code to plot a histogram of the filtered data.\n\n\n# draw a histogram of the filtered data \nnew_data_object %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = missing_columns_name), \n                 fill = \"pink\", # try some different colours?\n                 colour = \"purple\", # and here?\n                 bins = 3) #  # adjust the bins? \n# you could also add (+) a theme to this plot! \n# for a list of themes, type: ?theme_classic\n\n\nFinally, copy the code for the original geom_density() plot that you drew in step 4. Paste it, and edit the code so that it now plots the filtered set of data (from step 7) for each of the three conditions in the stroop task.\n\n\n\n\nWe have seen in our density plots that the reaction times (DV) look different in the three different Stroop conditions (our IV). But now we need to look at whether there are statistically significant differences between the means of the three conditions.\nTo do this, we will first summarise the mean time taken by each condition in the Stroop task. In Week 3 we learnt how to use group_by() and summarise() to get summary stats (e.g., mean, sd) at each level of the IV. That’s what we want to do now:\n\nCopy the code below into your R Markdown and edit the group_by() line to specify the IV and the summarise() line to calculate the mean() of our DV. If you do this correctly, you’ll get three values - a mean value for each level (condition) of our IV. Do these means reflect what you would expect in the Stroop task? Do they match the central tendency of the distributions you plotted?\n\n\nname_of_data_object %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q1\n  summarise(stroop_mean = mean(name_of_DV_column)) # you need to EDIT this for Q1\n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nYou should have 3 values: 4.63, 5.92, 7.25\nIf all of your values are the same, you’ve analysed the wrong column.\n\n\n\n\nNext we need to test if these differences between our means are real. To do that, we can run a related samples t-test. We use this test because each level of the IV in this experiment came from the same person. First though, we must use a filter() to restrict the data to just two levels of the IV. The IV is the condition column/variable in the data. The related samples t-test looks at the difference between two means (and only two), so the column we use for the t-test needs to have just two levels of the IV (two of the conditions).\nCopy the code below into your R Markdown. Edit the first filter command to select the data for the compatible condition, and the second filter command to get the data for the control condition. HINT: you’ll want to use a “==” here - this allows us to select the data that is the same as (==) a particular value (i.e., the name of that condition).\n\n\n# use filter to select the compatible level of the IV\nstroop_compatible &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# use filter to select the control level of the IV\nstroop_control &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# use filter to select the incompatible level of the IV\nstroop_incompatible &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# run the t-test comparing the means of two of the levels\nt.test(x = stroop_compatible$time, y = stroop_control$time, paired = TRUE)\n\n# add two more t-tests for the other two comparisions\n\n\nRun the t-test on this selection of data to compare the means from these two levels of the IV. Is the result significant? Note the t-value and the p-value.\nWrite out a statement in your R Markdown document to express this result. Here’s a template you can use, where you need to edit the bits in the []:\n\n\n“There [was a / was no] significant difference between the [describe the variables that were compared], t([degrees of freedom here]) = [t value here], p &lt; [p value here].”\n\n\nWith 3 levels (L1,L2,L3) to the IV condition there are 3 possible comparisons we can make (L1 vs. L2; L1 vs. L3; L2 vs. L3). Complete all three tests, by copying and pasting the code chunk twice more, editing each filter expression to select the relevant data. For each pair of conditions/levels, run the t-test. Write out a reporting statement (Q5) for each of your comparisons.\n\n\n\n\n\n\n\nCheck your results!\n\n\n\n\nUsing the filtered data (327 rows) you should get the following t statistics: -9.7648, -7.7069, -14.844\nRemember that it’s the magnitude that’s important, not whether it’s positive or negative. The sign simply depends which way round they are compared in the t calculations (mean_1 - mean_2 or mean_2 - mean_1)\n\n\n\n\n\n\n\nIn Task 2 you calculated the means for each condition in the Stroop task. We’ve seen in lectures that “standard error” provides an estimate of how variable that mean will be across the samples we collect. A very typical way to plot a mean value is to plot it with the standard error of the mean (SEM):\n\n\n\nThe code from Task 2, Question 1 will give the mean. We will now add a second line of this code to give the standard error values:\n\n\nstroop_summary &lt;- \n  name_of_data_object %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q2\n  summarise(stroop_mean = mean(name_of_DV_column),\n            stroop_SE = sd(name_of_DV_column)/sqrt(n())) # you need to EDIT this for Q1\n\n\nAdd this code to your document and the correct column (DV) to both the sd() and the mean() commands. Note that you don’t need to put anything in n(), as this simply calculates how many rows there are.\nView the new summary object you have created. Check that the means and SEs are different for the 3 conditions. If they are the same, you probably summarised the wrong column!\n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nThe SE values should be: 0.137, 0.136, 0.168\n\n\n\n\nWe will now plot these 3 mean values in a figure. Let’s use geom_point() so that our means and SEs look a bit like the figure above. Complete the ggplot command to plot our summarised value called stroop_mean (y), as a function of the IV, condition (x):\n\n\n# let's first plot the means\nname_of_data_object %&gt;%\n  ggplot(aes(x = name_of_IV_column, y = name_of_DV_column)) + # map variables to x and y for Q5\n  geom_point(size = 5)\n\n\nNow we need to add some “error bars” which provide a visual guide as to how much uncertainty we have in our mean value. Edit the code below for the ggplot() command to plot both geom_point() (same as Q5) and geom_errorbar. You will need to calculate a ymin and a ymax value.\n\n\n\n\n\n\n\nPlotting the error bars\n\n\n\nUse the illustration of the error bars above to work out how to combine the mean value and the SE value (hint: you’ll need to either ADD or SUBTRACT for the two statements) to create the right ymin and ymax. You need to put this in the “missing_equation” bit of the code below:\n\n\n\n# let's first plot the means\nname_of_data_object %&gt;%\n  ggplot(aes(x = name_of_IV_column, y = name_of_DV_column)) + # map variables to x and y for Q5\n  geom_point(size = 5) +\n  geom_errorbar(aes(ymin = missing_equation, # edit this for Q5\n                    ymax = missing_equation), # edit this for Q5\n                width = .2) \n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nThe correct result will have 3 points, and an error bar around each mean point. These 3 error bars should all be different sizes (as per the 3 SEs you calculated in steps 2-4)\n\n\n\nEXTRA: These next steps can be completed to practice customising your plot\n\nAdd a labs() layer to the plot to change the axis titles, and the title of the plot.\nChange the theme of the plot (e.g., theme_classic() or theme_dark())\nMap the colour aesthetic to the variable condition. You can do this for geom_point or geom_errorbar or both at once by putting it in the aes() within the ggplot() command.\nTry changing your geom_point() to geom_col.\n\n\n\n\nLet’s try knitting the document. If you’ve done everything right, then the knitting process will work and you’ll get a nice output (in html, or PDF, whichever you choose). If something goes wrong, here’s a few things you can check\n\nDid you keep all your code in the code chunks?\nCheck all your code blocks run.\nAre there any red cross symbols next to your lines of code? These indicate a code error and need to be fixed before it will knit.\n\nWhen you knit the document, you will probably see the code you have written in the output. You can decide whether you want to present the code or not using the options for each code chunk:\n\nClick the cog, then select the type of output you want each code to produce.\nKnitting the document is a great way to see how your work looks as an actual report. Go back and add more description between your code chunks to describe all the steps you have performed in your analysis.\n\n\n\n\nYou can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html#reading",
    "href": "PSYC121/Week8.html#reading",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "Chapter 13 of Howell\nToday we will take a look at summarising means and standard errors (SEs) from our data. We will look at how we plot these together on the one graph (using ggplot() commands that allow us to share mappings between different geoms. We will explore our data on the famous “Stroop Task” and we will use a related-samples t-test to examine the differences between the means of our different conditions in this task.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html#pre-lab-work-online-tutorial",
    "href": "PSYC121/Week8.html#pre-lab-work-online-tutorial",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "Online tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\n\nCreate a folder for Week 8 and download the Week_8 csv file file and upload it into this new folder in RStudio Server.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html#rstudio-tasks",
    "href": "PSYC121/Week8.html#rstudio-tasks",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "The “Stroop Effect” is a classic demonstration of automaticity of behaviour. Participants have to say the colour a word is printed in, which is an easy task for a “compatible” stimulus like GREEN, and a much more difficult task for an “incompatible” stimulus like BLUE. We can’t help but read the text - it has seemingly become an automatic process.\n In this task we will calculate the means and standard errors of the means and then we will then plot them using ggplot(). First though, we’ll need to inspect the data and maybe do a bit of data wrangling by using our filter() command.\n\nCreate a new R Markdown document. If you’re unsure about this step, see the instructions from Week 6 (or 7).\nAs usual, add a code chunk with library(tidyverse) and a read_csv command (see above for the link to the csv). Assign the result to a new data object, and call your data something meaningful (perhaps data_w8 or data_stroop but maybe not bestest_most_fantastic_data_on_the_stroop_test_eva_init)\nView the data with View(data_object_name). You will see that the data are a little different from the data we have worked with previously. We have an pID variable, which gives a unique number for each person, but each person has 3 rows of data. This is because the different conditions of the Stroop task reflect a within-subjects variable (related samples). For data like this it is often useful to have them arranged in what is referred to as “long format”, with multiple rows for each response the participant provides. For the current data that means we have a variable called condition, which is our IV, and one called time which is our DV. We also have a column labelled avg_time, which is the average of the 3 time values for each participant (the data is duplicated, which is both normal and necessary with long format data).\nLet’s look at the distribution of time (our DV) as a function of condition. Add another chunk of code and include the following code:\n\n\n# distribution of times by condition\nyour_data_object %&gt;% \n  ggplot() +\n  geom_density(aes(x = missing_column_name_A, fill = missing_column_name_B), alpha = .8) + # you need to EDIT this for Q4\n  theme_dark()\n\n\nYou’ll need to “map” x to time and fill to condition for our geom_density() plot. You can play around with the alpha parameter (which sets the transparency of the elements of the graph), setting it to a value between 0 and 1. Note that this is done OUTSIDE of the aes() command.\nFrom the density plot, it does seem like we have some outlier values. It’s probably best if we remove data for the whole participant if their average time is unusual. To do that, we’ll look at the data using the avg_time column. Add the following code for a geom_histogram() to plot the distribution of values in the new avg_time column.\n\n\n# distribution of average times\nyour_data_object %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = missing_column_name_A), fill = \"pink\") + # you need to EDIT this line\n  theme_classic()\n\n\nLet’s use the filter command we learned last week to remove these high values. Like last week, we will do this (for now) in a fairly unprincipled manner, by “eyeballing” the data (next week we’ll consider something a bit more “scientific”). Complete the filter command so that it keeps only the responses for people that had an avg_time less than 10 seconds. Remember that you need to think about how you are storing the result of this filter process. Do you want to create a new object, or overwrite the existing object?\n\n\n# filter out the high values\nnew_data_object &lt;- # create a new object (or overwrite)\n  your_data_object %&gt;% # original data object \n  filter(insert_an_expression_here)\n\n\n\n\n\n\n\nCheck your result!\n\n\n\nIf you’ve done this correctly, you should now have a data object that has 327 rows (data for 109 participants, with 3 responses each).\n\n\n\nAdd and edit the following code to plot a histogram of the filtered data.\n\n\n# draw a histogram of the filtered data \nnew_data_object %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = missing_columns_name), \n                 fill = \"pink\", # try some different colours?\n                 colour = \"purple\", # and here?\n                 bins = 3) #  # adjust the bins? \n# you could also add (+) a theme to this plot! \n# for a list of themes, type: ?theme_classic\n\n\nFinally, copy the code for the original geom_density() plot that you drew in step 4. Paste it, and edit the code so that it now plots the filtered set of data (from step 7) for each of the three conditions in the stroop task.\n\n\n\n\nWe have seen in our density plots that the reaction times (DV) look different in the three different Stroop conditions (our IV). But now we need to look at whether there are statistically significant differences between the means of the three conditions.\nTo do this, we will first summarise the mean time taken by each condition in the Stroop task. In Week 3 we learnt how to use group_by() and summarise() to get summary stats (e.g., mean, sd) at each level of the IV. That’s what we want to do now:\n\nCopy the code below into your R Markdown and edit the group_by() line to specify the IV and the summarise() line to calculate the mean() of our DV. If you do this correctly, you’ll get three values - a mean value for each level (condition) of our IV. Do these means reflect what you would expect in the Stroop task? Do they match the central tendency of the distributions you plotted?\n\n\nname_of_data_object %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q1\n  summarise(stroop_mean = mean(name_of_DV_column)) # you need to EDIT this for Q1\n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nYou should have 3 values: 4.63, 5.92, 7.25\nIf all of your values are the same, you’ve analysed the wrong column.\n\n\n\n\nNext we need to test if these differences between our means are real. To do that, we can run a related samples t-test. We use this test because each level of the IV in this experiment came from the same person. First though, we must use a filter() to restrict the data to just two levels of the IV. The IV is the condition column/variable in the data. The related samples t-test looks at the difference between two means (and only two), so the column we use for the t-test needs to have just two levels of the IV (two of the conditions).\nCopy the code below into your R Markdown. Edit the first filter command to select the data for the compatible condition, and the second filter command to get the data for the control condition. HINT: you’ll want to use a “==” here - this allows us to select the data that is the same as (==) a particular value (i.e., the name of that condition).\n\n\n# use filter to select the compatible level of the IV\nstroop_compatible &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# use filter to select the control level of the IV\nstroop_control &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# use filter to select the incompatible level of the IV\nstroop_incompatible &lt;-\n  name_of_data_object %&gt;% # Edit this for the name of YOUR data object\n  filter(*MISSING*)\n\n# run the t-test comparing the means of two of the levels\nt.test(x = stroop_compatible$time, y = stroop_control$time, paired = TRUE)\n\n# add two more t-tests for the other two comparisions\n\n\nRun the t-test on this selection of data to compare the means from these two levels of the IV. Is the result significant? Note the t-value and the p-value.\nWrite out a statement in your R Markdown document to express this result. Here’s a template you can use, where you need to edit the bits in the []:\n\n\n“There [was a / was no] significant difference between the [describe the variables that were compared], t([degrees of freedom here]) = [t value here], p &lt; [p value here].”\n\n\nWith 3 levels (L1,L2,L3) to the IV condition there are 3 possible comparisons we can make (L1 vs. L2; L1 vs. L3; L2 vs. L3). Complete all three tests, by copying and pasting the code chunk twice more, editing each filter expression to select the relevant data. For each pair of conditions/levels, run the t-test. Write out a reporting statement (Q5) for each of your comparisons.\n\n\n\n\n\n\n\nCheck your results!\n\n\n\n\nUsing the filtered data (327 rows) you should get the following t statistics: -9.7648, -7.7069, -14.844\nRemember that it’s the magnitude that’s important, not whether it’s positive or negative. The sign simply depends which way round they are compared in the t calculations (mean_1 - mean_2 or mean_2 - mean_1)\n\n\n\n\n\n\n\nIn Task 2 you calculated the means for each condition in the Stroop task. We’ve seen in lectures that “standard error” provides an estimate of how variable that mean will be across the samples we collect. A very typical way to plot a mean value is to plot it with the standard error of the mean (SEM):\n\n\n\nThe code from Task 2, Question 1 will give the mean. We will now add a second line of this code to give the standard error values:\n\n\nstroop_summary &lt;- \n  name_of_data_object %&gt;%\n  group_by(name_of_IV_column) %&gt;% # you need to EDIT this for Q2\n  summarise(stroop_mean = mean(name_of_DV_column),\n            stroop_SE = sd(name_of_DV_column)/sqrt(n())) # you need to EDIT this for Q1\n\n\nAdd this code to your document and the correct column (DV) to both the sd() and the mean() commands. Note that you don’t need to put anything in n(), as this simply calculates how many rows there are.\nView the new summary object you have created. Check that the means and SEs are different for the 3 conditions. If they are the same, you probably summarised the wrong column!\n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nThe SE values should be: 0.137, 0.136, 0.168\n\n\n\n\nWe will now plot these 3 mean values in a figure. Let’s use geom_point() so that our means and SEs look a bit like the figure above. Complete the ggplot command to plot our summarised value called stroop_mean (y), as a function of the IV, condition (x):\n\n\n# let's first plot the means\nname_of_data_object %&gt;%\n  ggplot(aes(x = name_of_IV_column, y = name_of_DV_column)) + # map variables to x and y for Q5\n  geom_point(size = 5)\n\n\nNow we need to add some “error bars” which provide a visual guide as to how much uncertainty we have in our mean value. Edit the code below for the ggplot() command to plot both geom_point() (same as Q5) and geom_errorbar. You will need to calculate a ymin and a ymax value.\n\n\n\n\n\n\n\nPlotting the error bars\n\n\n\nUse the illustration of the error bars above to work out how to combine the mean value and the SE value (hint: you’ll need to either ADD or SUBTRACT for the two statements) to create the right ymin and ymax. You need to put this in the “missing_equation” bit of the code below:\n\n\n\n# let's first plot the means\nname_of_data_object %&gt;%\n  ggplot(aes(x = name_of_IV_column, y = name_of_DV_column)) + # map variables to x and y for Q5\n  geom_point(size = 5) +\n  geom_errorbar(aes(ymin = missing_equation, # edit this for Q5\n                    ymax = missing_equation), # edit this for Q5\n                width = .2) \n\n\n\n\n\n\n\nCheck your result!\n\n\n\n\nThe correct result will have 3 points, and an error bar around each mean point. These 3 error bars should all be different sizes (as per the 3 SEs you calculated in steps 2-4)\n\n\n\nEXTRA: These next steps can be completed to practice customising your plot\n\nAdd a labs() layer to the plot to change the axis titles, and the title of the plot.\nChange the theme of the plot (e.g., theme_classic() or theme_dark())\nMap the colour aesthetic to the variable condition. You can do this for geom_point or geom_errorbar or both at once by putting it in the aes() within the ggplot() command.\nTry changing your geom_point() to geom_col.\n\n\n\n\nLet’s try knitting the document. If you’ve done everything right, then the knitting process will work and you’ll get a nice output (in html, or PDF, whichever you choose). If something goes wrong, here’s a few things you can check\n\nDid you keep all your code in the code chunks?\nCheck all your code blocks run.\nAre there any red cross symbols next to your lines of code? These indicate a code error and need to be fixed before it will knit.\n\nWhen you knit the document, you will probably see the code you have written in the output. You can decide whether you want to present the code or not using the options for each code chunk:\n\nClick the cog, then select the type of output you want each code to produce.\nKnitting the document is a great way to see how your work looks as an actual report. Go back and add more description between your code chunks to describe all the steps you have performed in your analysis.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week8.html#week-8-quiz",
    "href": "PSYC121/Week8.html#week-8-quiz",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "You can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "8. Related-samples t-tests, plotting means and SE bars"
    ]
  },
  {
    "objectID": "PSYC121/Week9.html",
    "href": "PSYC121/Week9.html",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "Watch Part 1\n\nWatch Part 2\n\nWatch Part 3\n\nWatch Part 4\n\nDownload the lecture slides here\n\n\nChapter 14 of Howell\n\n\n\nOnline tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\nCreate a folder for Week 9, download the Week_9 csv file and upload it into this new folder in RStudio Server.\n\n\n\n\n\nIn this class we will be exploring some data on people’s estimations on aspects of the UK population. We asked people 4 different questions:\n\nOut of every 100 people, about how many do you think are:\n\n\n\nChristian?\nMuslim?\nOver the age of 65?\n\n\nWe also asked a related question about immigration:\n\nWhat percentage of the UK population do you think are immigrants to this country? (i.e. not born in UK)\n\n\nCreate a new R Markdown document for Week 9.\nIn the first chunk add library(tidyverse) and create a new data object by using read_csv() to read “data_wk9.csv”.\nYou can view the data by clicking on it in the environment. Note that in previous weeks we’ve used the view() command in our scripts (which does the same thing), but that can conflict with knitting the .Rmd file.\nTake a look at the summary statistics for all of the columns in our data using summary(your_data_object_name)\n\nTo what extent are people’s estimations of these population parameters related? Let’s look at this by plotting these data as geom_point().\n\nCopy the following code. Edit it to map one of the numeric columns in the data to x and another numeric column to y. You can pick any of the columns you like, but it’s important that you understand what research question you are asking with your choice. For example, you might be asking “Do people who estimate that there are more Christians in the population also think there are more Muslims in the population?”\n\n\ndata_object_name %&gt;% \n  ggplot(aes()) + # map two of the columns to x and y\n  geom_point() # you can change the size or colour of the points if you wish\n\n\nConsider the graph, noting any general pattern/trend in the data. Is there a postive relationship: do people who give high estimations for one variable tend to give high estimations for the other variable? Or is there a negative relationship: do people who give high estimations for one variable tend to give lower estimations for the other variable? Or is there no discernible relationship at all?\nWrite a statement after this code chunk to briefly make a comment about any pattern you see in the data (or absence of a pattern).\nCopy this code into new chunks and explore relationships between the other numeric variables, each time noting the research question you are asking, and discussing with people on your table what kind of relationship you can see in the data. Write some comments in your R Markdown document to describe the patterns you are seeing.\n\n\n\n\n\nYou may have noticed that there are some fairly extreme values in some of these numeric estimations of the population. As we’ve discussed in previous weeks, these outlier values can be problematic when we run our statistical tests, so (like last week) we probably want to control their influence by removing them. As you saw in your online tutorial, we can convert the data to z scores, and then remove z values above and below certain values.\nLet’s create a “z-transform column” called z_imm for the estimates of the percentage of immigrants. Complete the code below by changing the data object names and adding the relevant variable (column) name. Note that you may want to assign the result (&lt;-) to a new data object at this point.\n\n\n# use mutate and scale to create z-scores of immigration estimates\nnew_data_object_name &lt;- \n  data_object_name %&gt;% \n  mutate(z_imm = scale(column_name))\n\n# N.B. the z_imm column has a slightly odd format after this calculation\n# If you want, you can replace the last line here with this:\n# mutate(z_imm = scale(column_name)[,1])\n# ...but this isn't essential for the next steps\n\n\nView the new data object to check this column has been created correctly. Like in the online tutorial, it would be a good idea to calculate some descriptive statistics for this new column to check it conforms to what we know about z-scores (e.g., mean() = 0, give or take some rounding, and sd() = 1).\n\n\n# check the mean and sd of the new column\nmean(data_object_name$column_name)\nsd(data_object_name$column_name)\n\n\nWe know from our earlier lectures on the z distribution that values of greater than 2 (or less than -2) reflect around 5% of the distribution, and values greater than 3 (or less than -3) represent less than 1% of the distribution:\n\n\n\nLet’s consider an outlier any value that has a z of 2.5 (a conventional cutoff). Plot a histogram of the z_imm column in order to inspect whether there are data that are above 2.5 or below -2.5.\n\n\n# histogram of the z_imm column\n\n# eh, what, no code? Come on...you've got this!!! \n# start with the data, pipe, then ggplot\n# if you get really stuck, look back at last week\n\n\nAdd a filter command to remove the values in the z_imm column are greater than 2.5 or less than -2.5. That means you’ll need two separate statements with an & in the middle\n\n\n# add a filter command\ndata_object_filtered &lt;- \n  data_object_name %&gt;% \n  filter(first_expression_here & second_expression_here) \n\n\nYou should have removed 1 row of data to make an object with 102 rows. Make a note of this in your R markdown document.\n\n\n\n\n\nWe have also included a categorical variable in our data this week, which is one you have seen before in our analysis classes: the home location in the UK of the respondent, home_location_in_UK. For this data object we have included only those responses from those people from the “North” (NW and NE) and those from the “South” (SW and SE). Other respondents from elsewhere have been removed from the data. We can therefore look at whether people’s home location determines their population estimations.\n\nFirst we will look at the mean population estimations, split by home location. To do this, copy and edit the code below and complete the group_by() and summarise() commands to give the mean() estimates of the proportion of immigrants in the population by home location. You don’t need to edit the N = n() line - this provides the number of participants at each level of the home_location_in_UK variable.\n\n\n# summary statistics\ndata_object_name %&gt;% \n  group_by(column_name) %&gt;% \n  summarise(mean_imm_est = complete_this_statement,\n            sample_size = n())\n\n\nWhat do the means suggest? Do people in the North and South give different estimations? Write a statement in your .Rmd file to describe the difference.\nIn order to do a t-test (and variance test) we will need to split the data into two conditions. Complete the following filter code to first get only the participants from the North, then all the participants from the South:\n\n\nnorth_condition &lt;- data_object_name %&gt;% filter(home_location_in_UK == *MISSING*)\nsouth_condition &lt;- data_object_name %&gt;% filter(home_location_in_UK == *MISSING*)\n\n\nNow we can test whether the means of these two conditions are different. First, it is worth noting that many more respondents originate from the North than from the South (see the N column in the summary). We have unequal sample sizes, and potentially unequal variances. Copy the code below into your .Rmd. This code runs the var.test() to check if the variances of the two samples are similar (homogeneity of variance). If this test produces a p value less that .05, then the variances in the two samples are unequal. That will have consequences for how we run the t-test() in the next step.\n\n\n# check if variances are unequal (p &lt; .05) - Q4\nvar.test(x = north_condition$pop_est_immigrants, \n         y = south_condition$pop_est_immigrants)\n\n\nNow let’s run the t-test. This week we are comparing data from different samples of participants (those who are from the North and South). We need to tell the t-test that the data are NOT paired (paired = FALSE). The result of the var.test() in the last step will tell you whether the var.equal value should be TRUE or FALSE. Set var.equal = FALSE or var.equal = TRUE depending on whether the variances are equal. When you’re happy with the parameters, run the t-test.\n\n\n# run unrelated samples t-test\nt.test(x = north_condition$pop_est_immigrants, y = south_condition$pop_est_immigrants, \n       paired = missing, # is this a paired or unpaired t-test? \n       var.equal = missing_value) # you'll need to set this to TRUE or FALSE depending on what you found in var.test\n\n\nIf you’ve done this correctly, you should have a t-statistic = –0.04763. Examine the p-value and discuss on your table what this means. Write a line in your .Rmd to formally state the result of the t-test (look at last week for the format). Describe what this result means in your own words.\nIn that t-test we looked at the pop_est_immigrants variable, but we can do this test for all of our population estimates. Copy the code to run another var.test() and t.test(), for the pop_est_age variable. Document the result of the t-test in your .Rmd and describe the finding. What do these tell us about the relationship between home_location and estimates of the population (if anything)? Feel free to come back to this and run the other two tests on the other population estimations.\n\n\n\n\n\nWe saw in last week’s lab tasks that there was a significant effect in our Stroop task data: participants were faster to say the colour names of the compatible list compared to the incompatible list (there were significant differences with the control list too). We will now use these data to calculate an effect size (Cohen’s d) for the t-statistic that we observed in that test.\nImport the new stroop data csv file. We have reduced the data down to just the compatible and incompatible conditions.\nAdd the following code into a chunk and run it (edit the data object name) to calculate the cohens_d() effect size, which is reported as effsize. You can ignore any negative sign, taking note of the absolute value.\n\n\nlibrary(effectsize)\nlibrary(pwr)\n\n# add code here to read in the stroop csv file\n\ncompatible_condition &lt;- \n  your_stroop_data_object %&gt;% # rename\n  filter(condition == \"compatible\")\n\nincompatible_condition &lt;- \n  your_stroop_data_object %&gt;% # rename\n  filter(condition == \"incompatible\")\n\ncohens_d(x = compatible_condition$time,\n         y = incompatible_condition$time,\n         paired = TRUE)\n\n\nWe already know that this large effect size was significant with our large sample of participants. What might we have expected with a much smaller sample size? Copy the code below and use the pwr.t.test() function to add in the effect size that you calculated (Cohen’s d) in step 3. We can set the N to 20. What power would we have achieved with this sample size, to detect this large effect? Discuss with your table, or staff, what this power means.\n\n\npwr.t.test(d = value_from_step_3, n = 20, type = \"paired\") #Q4\n\n\nLet’s say we wanted our next experiment to have an 80% chance of finding an effect at least as large as the one we found. Copy the code you used in step 4, and edit it to run pwr.t.test() to work out the minimum sample size we would need to achieve power of .8, with the effect size you calculated in step 3.\nLet’s say we are looking to run a new experiment in which we give people a stressful task to complete simultaneously. We will ask them to put their hands in a bucket of ice cold water while doing the Stroop task (this is a real “stressor task” people use!). We are unsure of what consequence this will have for our effect size, but we want to estimate the effect size that could be detected in our experiment. We decide to run 40 participants, and want to achieve a power of .90 (90% chance to find an effect at least this large). Copy the pwr-t-test() code and edit it to find the minimum effect size we could hope to detect under these conditions?\n\n\n\n\nScripts: By now you are hopefully getting used to editing and working within the R Markdown script. As you know, to save a script, you simply click the save icon, or press ctrl+S (cmd+s on a mac).\nPlots: To save a graph you have produced, click the “Export” button in the plot window, then “Save as Image”. You can resize the graph and give it an appropriate filename. If you’ve set the working directory correctly, then the new file should appear in the current folder.\nData: The data objects you create (in the Environment) only exist within RStudio, and are temporary (with a script and the csv file, you can always redo the analysis). But what if you want to use the data elsewhere? For example you may want to share the data with your project (PEP?) supervisor. To do this, we need to write the data to a csv file (like those we use to import the data). You can do this with the following command: write_csv(the_data_object, \"the_filename.csv\").\nExporting from RStudio: The above save operations save files to a folder within RStudio Server. At some stage you will need to get these files out of RStudio Server, for example if you need a graph for your report, or you need to share the data or the scripts. Or maybe you want to make the csv file available to other researchers. To get files out of RStudio, simply select the files you want in the Files pane, click “More” and then “Export”. Selecting multiple files will produce a “.zip” file, which will need to be “unzipped” on your computer to access the individual files (instructions for Windows and instructions for Mac)\n\n\n\nYou can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "9. Unrelated-samples t-test and Power"
    ]
  },
  {
    "objectID": "PSYC121/Week9.html#reading",
    "href": "PSYC121/Week9.html#reading",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "Chapter 14 of Howell",
    "crumbs": [
      "Home",
      "PSYC121",
      "9. Unrelated-samples t-test and Power"
    ]
  },
  {
    "objectID": "PSYC121/Week9.html#pre-lab-work-online-tutorial",
    "href": "PSYC121/Week9.html#pre-lab-work-online-tutorial",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "Online tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\nCreate a folder for Week 9, download the Week_9 csv file and upload it into this new folder in RStudio Server.",
    "crumbs": [
      "Home",
      "PSYC121",
      "9. Unrelated-samples t-test and Power"
    ]
  },
  {
    "objectID": "PSYC121/Week9.html#rstudio-tasks",
    "href": "PSYC121/Week9.html#rstudio-tasks",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "In this class we will be exploring some data on people’s estimations on aspects of the UK population. We asked people 4 different questions:\n\nOut of every 100 people, about how many do you think are:\n\n\n\nChristian?\nMuslim?\nOver the age of 65?\n\n\nWe also asked a related question about immigration:\n\nWhat percentage of the UK population do you think are immigrants to this country? (i.e. not born in UK)\n\n\nCreate a new R Markdown document for Week 9.\nIn the first chunk add library(tidyverse) and create a new data object by using read_csv() to read “data_wk9.csv”.\nYou can view the data by clicking on it in the environment. Note that in previous weeks we’ve used the view() command in our scripts (which does the same thing), but that can conflict with knitting the .Rmd file.\nTake a look at the summary statistics for all of the columns in our data using summary(your_data_object_name)\n\nTo what extent are people’s estimations of these population parameters related? Let’s look at this by plotting these data as geom_point().\n\nCopy the following code. Edit it to map one of the numeric columns in the data to x and another numeric column to y. You can pick any of the columns you like, but it’s important that you understand what research question you are asking with your choice. For example, you might be asking “Do people who estimate that there are more Christians in the population also think there are more Muslims in the population?”\n\n\ndata_object_name %&gt;% \n  ggplot(aes()) + # map two of the columns to x and y\n  geom_point() # you can change the size or colour of the points if you wish\n\n\nConsider the graph, noting any general pattern/trend in the data. Is there a postive relationship: do people who give high estimations for one variable tend to give high estimations for the other variable? Or is there a negative relationship: do people who give high estimations for one variable tend to give lower estimations for the other variable? Or is there no discernible relationship at all?\nWrite a statement after this code chunk to briefly make a comment about any pattern you see in the data (or absence of a pattern).\nCopy this code into new chunks and explore relationships between the other numeric variables, each time noting the research question you are asking, and discussing with people on your table what kind of relationship you can see in the data. Write some comments in your R Markdown document to describe the patterns you are seeing.\n\n\n\n\n\nYou may have noticed that there are some fairly extreme values in some of these numeric estimations of the population. As we’ve discussed in previous weeks, these outlier values can be problematic when we run our statistical tests, so (like last week) we probably want to control their influence by removing them. As you saw in your online tutorial, we can convert the data to z scores, and then remove z values above and below certain values.\nLet’s create a “z-transform column” called z_imm for the estimates of the percentage of immigrants. Complete the code below by changing the data object names and adding the relevant variable (column) name. Note that you may want to assign the result (&lt;-) to a new data object at this point.\n\n\n# use mutate and scale to create z-scores of immigration estimates\nnew_data_object_name &lt;- \n  data_object_name %&gt;% \n  mutate(z_imm = scale(column_name))\n\n# N.B. the z_imm column has a slightly odd format after this calculation\n# If you want, you can replace the last line here with this:\n# mutate(z_imm = scale(column_name)[,1])\n# ...but this isn't essential for the next steps\n\n\nView the new data object to check this column has been created correctly. Like in the online tutorial, it would be a good idea to calculate some descriptive statistics for this new column to check it conforms to what we know about z-scores (e.g., mean() = 0, give or take some rounding, and sd() = 1).\n\n\n# check the mean and sd of the new column\nmean(data_object_name$column_name)\nsd(data_object_name$column_name)\n\n\nWe know from our earlier lectures on the z distribution that values of greater than 2 (or less than -2) reflect around 5% of the distribution, and values greater than 3 (or less than -3) represent less than 1% of the distribution:\n\n\n\nLet’s consider an outlier any value that has a z of 2.5 (a conventional cutoff). Plot a histogram of the z_imm column in order to inspect whether there are data that are above 2.5 or below -2.5.\n\n\n# histogram of the z_imm column\n\n# eh, what, no code? Come on...you've got this!!! \n# start with the data, pipe, then ggplot\n# if you get really stuck, look back at last week\n\n\nAdd a filter command to remove the values in the z_imm column are greater than 2.5 or less than -2.5. That means you’ll need two separate statements with an & in the middle\n\n\n# add a filter command\ndata_object_filtered &lt;- \n  data_object_name %&gt;% \n  filter(first_expression_here & second_expression_here) \n\n\nYou should have removed 1 row of data to make an object with 102 rows. Make a note of this in your R markdown document.",
    "crumbs": [
      "Home",
      "PSYC121",
      "9. Unrelated-samples t-test and Power"
    ]
  },
  {
    "objectID": "PSYC121/Week9.html#task-3---unrelated-samples-t-test",
    "href": "PSYC121/Week9.html#task-3---unrelated-samples-t-test",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "We have also included a categorical variable in our data this week, which is one you have seen before in our analysis classes: the home location in the UK of the respondent, home_location_in_UK. For this data object we have included only those responses from those people from the “North” (NW and NE) and those from the “South” (SW and SE). Other respondents from elsewhere have been removed from the data. We can therefore look at whether people’s home location determines their population estimations.\n\nFirst we will look at the mean population estimations, split by home location. To do this, copy and edit the code below and complete the group_by() and summarise() commands to give the mean() estimates of the proportion of immigrants in the population by home location. You don’t need to edit the N = n() line - this provides the number of participants at each level of the home_location_in_UK variable.\n\n\n# summary statistics\ndata_object_name %&gt;% \n  group_by(column_name) %&gt;% \n  summarise(mean_imm_est = complete_this_statement,\n            sample_size = n())\n\n\nWhat do the means suggest? Do people in the North and South give different estimations? Write a statement in your .Rmd file to describe the difference.\nIn order to do a t-test (and variance test) we will need to split the data into two conditions. Complete the following filter code to first get only the participants from the North, then all the participants from the South:\n\n\nnorth_condition &lt;- data_object_name %&gt;% filter(home_location_in_UK == *MISSING*)\nsouth_condition &lt;- data_object_name %&gt;% filter(home_location_in_UK == *MISSING*)\n\n\nNow we can test whether the means of these two conditions are different. First, it is worth noting that many more respondents originate from the North than from the South (see the N column in the summary). We have unequal sample sizes, and potentially unequal variances. Copy the code below into your .Rmd. This code runs the var.test() to check if the variances of the two samples are similar (homogeneity of variance). If this test produces a p value less that .05, then the variances in the two samples are unequal. That will have consequences for how we run the t-test() in the next step.\n\n\n# check if variances are unequal (p &lt; .05) - Q4\nvar.test(x = north_condition$pop_est_immigrants, \n         y = south_condition$pop_est_immigrants)\n\n\nNow let’s run the t-test. This week we are comparing data from different samples of participants (those who are from the North and South). We need to tell the t-test that the data are NOT paired (paired = FALSE). The result of the var.test() in the last step will tell you whether the var.equal value should be TRUE or FALSE. Set var.equal = FALSE or var.equal = TRUE depending on whether the variances are equal. When you’re happy with the parameters, run the t-test.\n\n\n# run unrelated samples t-test\nt.test(x = north_condition$pop_est_immigrants, y = south_condition$pop_est_immigrants, \n       paired = missing, # is this a paired or unpaired t-test? \n       var.equal = missing_value) # you'll need to set this to TRUE or FALSE depending on what you found in var.test\n\n\nIf you’ve done this correctly, you should have a t-statistic = –0.04763. Examine the p-value and discuss on your table what this means. Write a line in your .Rmd to formally state the result of the t-test (look at last week for the format). Describe what this result means in your own words.\nIn that t-test we looked at the pop_est_immigrants variable, but we can do this test for all of our population estimates. Copy the code to run another var.test() and t.test(), for the pop_est_age variable. Document the result of the t-test in your .Rmd and describe the finding. What do these tell us about the relationship between home_location and estimates of the population (if anything)? Feel free to come back to this and run the other two tests on the other population estimations.",
    "crumbs": [
      "Home",
      "PSYC121",
      "9. Unrelated-samples t-test and Power"
    ]
  },
  {
    "objectID": "PSYC121/Week9.html#task-4---power-and-effect-size-d-calculations",
    "href": "PSYC121/Week9.html#task-4---power-and-effect-size-d-calculations",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "We saw in last week’s lab tasks that there was a significant effect in our Stroop task data: participants were faster to say the colour names of the compatible list compared to the incompatible list (there were significant differences with the control list too). We will now use these data to calculate an effect size (Cohen’s d) for the t-statistic that we observed in that test.\nImport the new stroop data csv file. We have reduced the data down to just the compatible and incompatible conditions.\nAdd the following code into a chunk and run it (edit the data object name) to calculate the cohens_d() effect size, which is reported as effsize. You can ignore any negative sign, taking note of the absolute value.\n\n\nlibrary(effectsize)\nlibrary(pwr)\n\n# add code here to read in the stroop csv file\n\ncompatible_condition &lt;- \n  your_stroop_data_object %&gt;% # rename\n  filter(condition == \"compatible\")\n\nincompatible_condition &lt;- \n  your_stroop_data_object %&gt;% # rename\n  filter(condition == \"incompatible\")\n\ncohens_d(x = compatible_condition$time,\n         y = incompatible_condition$time,\n         paired = TRUE)\n\n\nWe already know that this large effect size was significant with our large sample of participants. What might we have expected with a much smaller sample size? Copy the code below and use the pwr.t.test() function to add in the effect size that you calculated (Cohen’s d) in step 3. We can set the N to 20. What power would we have achieved with this sample size, to detect this large effect? Discuss with your table, or staff, what this power means.\n\n\npwr.t.test(d = value_from_step_3, n = 20, type = \"paired\") #Q4\n\n\nLet’s say we wanted our next experiment to have an 80% chance of finding an effect at least as large as the one we found. Copy the code you used in step 4, and edit it to run pwr.t.test() to work out the minimum sample size we would need to achieve power of .8, with the effect size you calculated in step 3.\nLet’s say we are looking to run a new experiment in which we give people a stressful task to complete simultaneously. We will ask them to put their hands in a bucket of ice cold water while doing the Stroop task (this is a real “stressor task” people use!). We are unsure of what consequence this will have for our effect size, but we want to estimate the effect size that could be detected in our experiment. We decide to run 40 participants, and want to achieve a power of .90 (90% chance to find an effect at least this large). Copy the pwr-t-test() code and edit it to find the minimum effect size we could hope to detect under these conditions?",
    "crumbs": [
      "Home",
      "PSYC121",
      "9. Unrelated-samples t-test and Power"
    ]
  },
  {
    "objectID": "PSYC121/Week9.html#a-note-on-saving-your-work",
    "href": "PSYC121/Week9.html#a-note-on-saving-your-work",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "Scripts: By now you are hopefully getting used to editing and working within the R Markdown script. As you know, to save a script, you simply click the save icon, or press ctrl+S (cmd+s on a mac).\nPlots: To save a graph you have produced, click the “Export” button in the plot window, then “Save as Image”. You can resize the graph and give it an appropriate filename. If you’ve set the working directory correctly, then the new file should appear in the current folder.\nData: The data objects you create (in the Environment) only exist within RStudio, and are temporary (with a script and the csv file, you can always redo the analysis). But what if you want to use the data elsewhere? For example you may want to share the data with your project (PEP?) supervisor. To do this, we need to write the data to a csv file (like those we use to import the data). You can do this with the following command: write_csv(the_data_object, \"the_filename.csv\").\nExporting from RStudio: The above save operations save files to a folder within RStudio Server. At some stage you will need to get these files out of RStudio Server, for example if you need a graph for your report, or you need to share the data or the scripts. Or maybe you want to make the csv file available to other researchers. To get files out of RStudio, simply select the files you want in the Files pane, click “More” and then “Export”. Selecting multiple files will produce a “.zip” file, which will need to be “unzipped” on your computer to access the individual files (instructions for Windows and instructions for Mac)",
    "crumbs": [
      "Home",
      "PSYC121",
      "9. Unrelated-samples t-test and Power"
    ]
  },
  {
    "objectID": "PSYC121/Week9.html#week-9-quiz",
    "href": "PSYC121/Week9.html#week-9-quiz",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "You can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "9. Unrelated-samples t-test and Power"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html",
    "href": "PSYC121/Week1.html",
    "title": "1. Introduction to PSYC121",
    "section": "",
    "text": "Watch the introduction: Lecture Part 1\n\nWatch Lecture Part 2\n\nWatch Lecture Part 3\n\nWatch Lecture Part 4\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.1---check-in-with-the-university-attendance-register",
    "href": "PSYC121/Week1.html#task-3.1---check-in-with-the-university-attendance-register",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.1 - check-in with the University attendance register",
    "text": "Task 3.1 - check-in with the University attendance register\nWhen you arrive, make sure you have checked-in to your Analysis session in the Levy lab. All students are required by the University to confirm attendance at taught session\nStaff will remind you of this in your class.",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.2---getting-dicy",
    "href": "PSYC121/Week1.html#task-3.2---getting-dicy",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.2 - Getting dicy",
    "text": "Task 3.2 - Getting dicy\n\n\n\n\n\nHere’s a simple task for you to complete as a group around each of the workstations;\nYou will be given a pair of dice\n\nWorking in pairs, one person rolls both dice.\nAdd up the total on each of them and have someone record that total (if you don’t have some spare paper or a pen, use your computer)\nRepeat those steps 20 times.\nThen swap over your roles (the person rolling the dice, the person recording the outcome)\nOnce everyone at the workstation has had a turn at this, each person should attempt to work out (a) the mean and (b) the median of their dice roll total.\nCheck each others working, and discuss any differences or problems you have.\n\nAre all your answers the same? Why / why not? If not, are they very different or very similar?",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.3---using-rstudio",
    "href": "PSYC121/Week1.html#task-3.3---using-rstudio",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.3 - Using RStudio",
    "text": "Task 3.3 - Using RStudio\n\nIntroducing R Studio\nR and RStudio is the software that we will be using to explore and learn about analysis in your Psychology degree. It’s a computational engine: a very snazzy calculator that you should see as your friend and ally in the journey to understand and appreciate psychology. It sits alongside what we teach about the concepts and interpretation of statistical analysis.\nR is the core software, RStudio is the interface for interacting with it. Put another way, R is the engine, RStudio is the cockpit.\nLike even a simplest calculator, it just does what you ask (at least when you ask nicely!) but it requires the user to know what they want from it and to understand what it is telling you. A calculator can’t help a kid get the right answer to a multiplication problem if they don’t know the difference between multiplication and division and addition etc. And whilst a calculator is brilliant at doing the number crunching (and as a bonus, R Studio can help with turning the numbers into beautiful graphs and images too), even a calculator requires a thoughtful person to take the answers and make sensible interpretations from them.\nTherefore, we need to learn both about the concepts of statistical analysis on the one hand, and the processing of statistical information -through R- on the other. The lectures will provide the starting point and the direction for statistical concepts, whilst these analysis labs provide the more practical experiences in how to use R, and how to make R your ally. Over the next year, in these labs we will increasingly be using RStudio to focus on the latter, processing side, which will allow you to focus your energies on the conceptual side and its relevance for appreciating psychology.\n\n\nGetting started with RStudio\nFor Lancaster University Psychology Students in 2024, we will be learning about R Studio through a simple but powerful web server architecture. That is, through the power of the internet, you can access and use R Studio by logging into a free account that we have provided and we will maintain for your use.\n\nHere’s a little secret: There are several different ways to access RStudio. For example, you can download a copy of the software onto your computer, or use a Virtual Machine set up to run a copy. There’s nothing to stop you having your local copy, but please note - we can’t support your own version through lab classes. We’re using the web server to make sure everyone has the same, controlled experience.\n\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n \nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\n\n\nWhat does RStudio look like?\nWhen RStudio starts, it will look something like this: \nRStudio has three panels or windows: there are tabs for Console (taking up the left hand side), Environment (and History top right) , Current file (bottom right). You will also see a 4th window for a script or set of commands you develop, also (on the left hand side).\n\n\nLet’s get started!\nThe first thing we want to do in RStudio is to create a folder for this week so that we can put the relevant material there and keep it tidy.\nFrom the lower-right panel of RStudio, click the files tab.\nSelect the “new folder” option and create a new folder (eg “week 1”)\nClick on that folder to open it\nNext, we’ve prepared some instructions for RStudio to use - this is called a “script”. So we need to get this script into the server for you to explore and play with\n\nDownload the “zip” file by clicking this link\nFind the location of the file on your computer and check it is saved as a “.zip” file\nReturn to RStudio\nClick “Upload”\nClick choose file and find the file on your computer.\nSelect the file and click “Open”. Click “OK”\n\nYou should now see the files extracted in the directory.\nYou should now have the script available in RStudio.\nUse “Save…As” to create a new version of the script. By doing this, you’ll be able to have a “before” and “after” version of the script and can go back over the changes\nIn the script, select or highlight the first line of text, which is this one:\n\nRun your first ever R instruction!\n\n5 + 5\n\nand “run” this line. That tells RStudio to carry out the instruction.\nYou should see that in the console tab, RStudio calculates the answer to this incredibly hard maths challenge! (amazing huh? OK, maybe not *that* amazing…).\n\n\nModify your first ever R instruction!\nUse your imagination – add a new line to the script and ask a different simple arithmetic question of your own choosing! What happens?\n\n\nCalculate descriptive stats in R for the first time!\nIn this week’s analysis lecture, we looked at measures of central tendency and how to calculate them. So let’s get R to do these calculations also!\nFirst, we tell R about the data used in the lecture. We’ve already created the instruction that will do exactly this and it is in the script, so run this line from the script\n\nweek_1_lecture_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nThis creates an “object” called week_1_lecture_data. We can then perform calculations on this object. For example, we can find the mean by running the following command (use the script to do this)\n\nmean(week_1_lecture_data)\n\nCheck the answer is the same we found in the lecture (it should be 6!).\nNext, let’s ask for the median by running this line from the script:\n\nmedian(week_1_lecture_data)\n\nThis also should be the answer from the lecture (7)\nR doesn’t have a single corresponding command for the mode, but we can use the block of code in the script for this that starts and ends with the “getmode” text (there are 6 lines of text)\nThis is just a bit of clever jiggery-pokery that gets the mode. What does R say the mode is?\n\n\nYour challenge\nHow can you get RStudio to verify / check the dice calculations that you attempted earlier? Think about how you might solve this problem, on the basis of what we have covered so far.\nWe will discuss this in class and attempt to get RStudio to check your answers. In doing so, annotate the script (add notes for you - not RStudio) using the “#” command",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#before-you-finish",
    "href": "PSYC121/Week1.html#before-you-finish",
    "title": "1. Introduction to PSYC121",
    "section": "Before you finish",
    "text": "Before you finish\n\nMake sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\nEnd your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\nThere is a red “power” button near the top right of the R studio window (do ask for help if you can’t find it). It’s a good habit to get into to turn the session off\n\n\nExtra content for outside the lab class\n\nIn the Howell text book on statistics, there’s some R code on descriptive statistics. It is included in the script for you to look at and play with.\nin your own time and think about the following:\n\nIn R, “&lt;-” is the assignment operator as in the command we used:\n\nPSYC121_week_1_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nWe create the variable label on the left (Analysis_week1_data) and we give it those numbers on the right. The nameAnalysis_week1_data is largely arbitrary: try use a variable of your own naming (your own name?) instead - and then use that alternative name for the other commands.\n\nThroughout this year, we’ll use the convention of the “underscore” to separate words in labels (it_makes_them_easier_to_read than ifyoudidn’thaveanyspaces)",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.4-review-the-learnr-sample-practice-questions",
    "href": "PSYC121/Week1.html#task-3.4-review-the-learnr-sample-practice-questions",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.4 – Review the learnr sample / practice questions",
    "text": "Task 3.4 – Review the learnr sample / practice questions\nAfter every block of teaching in part-1 analysis (specifically, we mean in week 5, week 10, week 15 and week 20) there will be a class test. This will assess your knowledge and your understanding of the material that has been covered.\nThe class test will comprise a set of Multiple Choice Questions (and the set of questions will be different for each student, as the test will involve random selection from a larger pool) under timed conditions.\nIn order to help you get (a) a broad or basic feel for the sort of questions you might get in the class test (b) self-review your progress through the term, we will provide MCQs each week for you to attempt.\nSo these are for your benefit… you can take the questions when you choose to, and the learnr quiz will provide feedback on the answers your provide. Just bear in mind:\n\nWe place a set of questions at the end of the learnr pages so that you can attempt these at the end of each week, after you’ve completed lab activities, follow-up work, weekly Q&As etc. But it’s up to you when you answer the questions\nThese are meant as indicative questions. There’s no point in learning/ memorising these questions (they won’t be on the quiz!) and our advice is to reflect on how the teaching and content links to the sorts of questions that get posed.",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#task-3.5-data-collection-exercise",
    "href": "PSYC121/Week1.html#task-3.5-data-collection-exercise",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3.5 – Data collection exercise",
    "text": "Task 3.5 – Data collection exercise\nIn order to learn about psychology and data analysis techniques, we need data! Rather than rely too much on artificial data (certainly it is sometimes useful to say “Here are a bunch of numbers and this is what we can do with them” – think about the R Studio example for this week’s lab) for the most part, we prefer to draw on datasets that are a bit more engaging and meaningful that you have a stake in yourself! By using a common data set, that we can return to over the year, we can also build up familiarity and confidence in the data and remove a potential obstacle to thinking about the more important analysis part.\nSo a key task will be for everyone to have a go at taking our online survey, and contribute to a dataset that can be used throughout the year.\nThe survey runs by following this link",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week1.html#post---lab-recap-the-slides-we-used",
    "href": "PSYC121/Week1.html#post---lab-recap-the-slides-we-used",
    "title": "1. Introduction to PSYC121",
    "section": "Post - lab recap: The slides we used",
    "text": "Post - lab recap: The slides we used\nWant to see again the introduction slides that we used in the Levy lab? They are available here",
    "crumbs": [
      "Home",
      "PSYC121",
      "1. Introduction to PSYC121"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html",
    "href": "PSYC121/Week3.html",
    "title": "3. DVs and IVs in RStudio",
    "section": "",
    "text": "Watch Part 1\n\nWatch Part 2\n\nWatch Lecture Part 3\n\nWatch Lecture Part 4\n\nDownload the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html#task-1---more-with-the-penelope22-data",
    "href": "PSYC121/Week3.html#task-1---more-with-the-penelope22-data",
    "title": "3. DVs and IVs in RStudio",
    "section": "3.1 Task 1 - More with the penelope22 data",
    "text": "3.1 Task 1 - More with the penelope22 data\nStep 1. Create a project and a folder, and set the working directory. This is covered in the learnr tutorial so head over there if you need reminding.\nStep 2. Bring the week3_2024.zip file into R Studio server. Like last week, upload the zip file, and launch the R script. You can get the file here\nStep 3. This week, we again want to explore commands from the tidyverse library (toolkit) which can help us do more powerful things more elegantly. So let’s get R to work again with the tidyverse library by running the code line\nlibrary(tidyverse)\nStep 4. Explore help() commands. R can give you more information about how it works.\nStep 5. Read or load the penelope data into R. That is what line 18 of the code is designed to do\n\ndata_object_name &lt;- read_csv(\"fill in\") # use your own dataobject_name and specify the file you want to work with\n\nbut note that you will need to edit this line -and ensure you are in the correct working directory- for this to be successful. Then have a look at it using the View() command in the script\nThis time, let’s ask for the estimate data arranged by identity:\n\naggregate(x = *MISSING*$estimate, by = list(*MISSING*$identity), FUN = mean)\n\nFirst, let’s try this (you will need to use your dataframe/variable name in place of MISSING). What do you get? Does this match what we did last week when we calculated the mean for the female and for the other (i.e., non-female) group?\nSecond, let’s look at what is happening here:\naggregate This is a command to call for descriptive statistics\nx= This defines what column we are analyzing\nby=list Now we tell R how to group the estimate data, and which column does that\nFUN=mean Specifies which descriptive function is being asked for Can you explore whether you can call on alternate measures?)\n\n3.1.2 group_by()\nThere’s another way that also allows us to group scores by a (nominal) variable. This is explored in the learnr tutorial, which should help you create the command the get weight estimates broken down by gender identity. You need to define the data frame for the estimates data, and the gender IV and the estimates DV\n\n*MISSING* %&gt;% group_by(*MISSING*) %&gt;% summarise(mean_estimate = mean(*MISSING*))\n\nFirst, try this command and see what you get. If you run this command as entered, it won’t work. So now use your experience at skills from the above and the learnr tutorial to work out what is required.\nNote\n%&gt;% This is called a “pipe operator”, basically take the output from the left and feed it into the requests on the right. Summarise Provide summary statistics information for the specified variable as specified (whether mean, median etc)\n\n\n3.1.3 The assignment operator\nAs well as learning about the pipe operator, we want to remind you /draw attention explicitly to another important element of the R command line syntax: the assignment operator. Uing a command such as\ncows &lt;- read_csv(\"penelope22.csv\")\nlooks for the csv datafile called ‘penelope22’, and assign it to an object or variable called ‘cows’\nWe could create any object name we wanted (within limits of names already known to RStudio). It isn’t just for reading in data files, we can perform a whole range of functions and assign them to an object.\n\nTask 2 - New salary data\nUsing aggregate and summarise may not seem like much progress, because they are just replicating what we had already done with mean() is week 2. However (a) this emphasizes that there are often several ways to get at the same thing in R (b) now we know about grouping, about working with 2-dimensional data, we can start to do more efficient and informative things.\nNow, let’s turn to the guesses made about median salary in the UK. We can get the data, from you as a group of PSYC121 students, from the file wages2024.csv (you will need to adapt the code we used above for the weight estimation file so that it will load in the wages data, and in what follows the assumption is your new variable name is called wages)\nLet’s take a peek at the dataset with\nglimpse(wages)\nGlimpse pretty much does what you might think from the meaning of the word – it just gives us a data sample (handy because this is a much bigger dataset) and shows that we have 3 columns; uk_region (where someone lives, note ‘other’ probably equals Northern Ireland, Europe, China, etc), family_position (age relationship with siblings), and salary (estimate).\nBy the way, the govt statistics say the actual median income in 2023 was approx. £34,963 see this link\n\nWriting into your script, use the working “aggregate” commands from task 1 with the penelope weight data to find out the salary guesses as a function of where someone lives? That is, can you adapt that code for this problem? First, make sure you read in the data file into an R object.\nCan you use the aggregate command to find out salary guess as a function of family relationships? (if you are the youngest child maybe you have older siblings earning money that changes your evaluation?)\nCan you get a breakdown of guess as a function of BOTH UK region AND family relationship together?\nCan you use the group by() command to display salary guesses as a function of where someone lives? Check this gives you the same answer.",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html#task-3.2---new-phone-use-data",
    "href": "PSYC121/Week3.html#task-3.2---new-phone-use-data",
    "title": "3. DVs and IVs in RStudio",
    "section": "Task 3.2 - New phone use data",
    "text": "Task 3.2 - New phone use data\n\nDataset 3: Use the dataset of phone screen time usage, screentime2024.csv to further explore and consolidate the group_by() command (ie we’ll drop the aggregate command for this task to focus on group_by()). Use copy and paste to adapt the existing script lines from the above two tasks so that this time you read in and calculate screen time usage as a function of the type of phone. In other words, add line (and comments) to the scripts for this new task.\nUse RStudio to figure out the overall mean screen time usage estimate and the standard deviation. Calculate by hand what usage estimate would have a z score of z=-1.5?",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week3.html#task-3.3---final-challenge-visualisation",
    "href": "PSYC121/Week3.html#task-3.3---final-challenge-visualisation",
    "title": "3. DVs and IVs in RStudio",
    "section": "Task 3.3 - Final challenge: visualisation",
    "text": "Task 3.3 - Final challenge: visualisation\nCan you find a way to visualise the screentime usage data that you have been working with above? The script provides two ways to consider doing this - boxplots (which we have looked at in script commands already) and ggplot, which we have spent less time with but is an extremely powerful engine for creating plots. We’ve provided the start of the code in each case, leaving you to work out the specifics. Remember to annotate your creations!",
    "crumbs": [
      "Home",
      "PSYC121",
      "3. DVs and IVs in RStudio"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html",
    "href": "PSYC121/Week6.html",
    "title": "6. Sampling, probability and binomial tests",
    "section": "",
    "text": "Watch Part 1\n\n\nWatch Part 2\n\n\nWatch Part 3\n\n\nWatch Part 4\n\n\nYou can download the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#week-6-lecture",
    "href": "PSYC121/Week6.html#week-6-lecture",
    "title": "6. Sampling, probability and binomial tests",
    "section": "",
    "text": "Watch Part 1\n\n\nWatch Part 2\n\n\nWatch Part 3\n\n\nWatch Part 4\n\n\nYou can download the lecture slides here",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#reading",
    "href": "PSYC121/Week6.html#reading",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Reading",
    "text": "Reading\nChapter 8 of Howell",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#pre-lab-work",
    "href": "PSYC121/Week6.html#pre-lab-work",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Pre-lab work",
    "text": "Pre-lab work\n\nEnsure you have watched the above lecture content for Week 6.\nComplete the short learnr tutorial which will introduce you to running the binomial tests in R. You can find it here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#working-with-r-markdown-documents-.rmd",
    "href": "PSYC121/Week6.html#working-with-r-markdown-documents-.rmd",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Working with “R Markdown” documents (.Rmd)",
    "text": "Working with “R Markdown” documents (.Rmd)\nToday we will take a look at a different type of document file called “R Markdown” (the filetype is .Rmd). It’s like a .R script, only a bit more fancy! Today we’ll introduce a couple of basic features of these files and use this document for our tasks below.\n\nMake sure you have a Week 6 folder\nOnce in that folder in the files pane, click “more” and “set as working directory”\nThen click the new file button and select “R Markdown”\n\n\n\nYou’ll be asked to name this file. Leave the other options as they are and click OK.\nWhen the new R Markdown file appears, try “knitting” it (the icon at the top with the ball of wool). You should get a nice output of the default R Markdown document.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#adding-code-chunks",
    "href": "PSYC121/Week6.html#adding-code-chunks",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Adding Code chunks",
    "text": "Adding Code chunks\nR Markdown documents allow you to freely type in text in the main body of the file. You are no longer restricted to putting comments after the “#” sign. Instead, when you want to put in R code you will need to create a “code chunk” within the document. These are places where you put your R code you want to run.\nThe easiest way to add a code chunk is to use the dedicated button in the editor pane (the shortcut is ctrl+alt+i):\n\nSelect “R” as that is the type of code you want to write. With your template document, try running the very first code chunk (click the green arrow on the right). This will run the code and display the output (in the document and in the console).\n\n\n\n\n\n\n.Rmd and .R differences\n\n\n\nIn a way, R Markdown files function in the opposite way to .R files:\n\n.Rmd: you write normal text in the main part (like you’d do in a word processor), but you create a special “code chunk” for your code.\n.R: you write code in the main body, and use the “#” to write normal text as a comment",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#source-mode-vs.-visual-mode",
    "href": "PSYC121/Week6.html#source-mode-vs.-visual-mode",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Source mode vs. Visual mode",
    "text": "Source mode vs. Visual mode\nOne of the nice things about working with R Markdown documents is that you can switch from the “coding” view (called “Source”) to something that is much more like a word processor (called “Visual”). You’ll see these two options in the top left corner of the document. Try flicking between them. Visual mode allows you to edit the document like a word-processor, making it easy to make formatting changes, add titles, insert images and tables, etc.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#creating-an-r-markdown-for-your-lab-work",
    "href": "PSYC121/Week6.html#creating-an-r-markdown-for-your-lab-work",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Creating an R Markdown for your lab work",
    "text": "Creating an R Markdown for your lab work\n\nDelete all of the script except the first title (## R Markdown)\nChange this title to something more relevant: (e.g., ## Task 1 - Card Sampling task)\nKnit your document to check it is still working!",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#card-sampling-task",
    "href": "PSYC121/Week6.html#card-sampling-task",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Card sampling task",
    "text": "Card sampling task\n\nIn the first task this week we will look at the sampling of events and we will apply the basic statistical test of the binomial test: binom.test()\nEach table has a set of cards. These will be 13 red cards and 13 black cards - please check your set to ensure you have the right number of each colour (it doesn’t matter what suit the cards are).\nWe are going to play a game in which one person chooses a set of these cards and biases the deck towards either red or black. The other members of the table have to try and work out which way the set is biased. To do this, they will draw samples from the deck.\n\n\n\n\n\n\nThe Experiment\n\n\n\nThink of this as an experiment: there is something real out there in the world in our “population” (the cards). As an experimenter we are trying to estimate what is true about the world, and in order to do this we need to take samples. When you can only see the backs of the cards, that data is unobserved. But as we draw samples, we start to understand how the “world” is - whether it is biased towards red or black.\nSo each time you draw a card, you are observing one data point from the population, and based on the data you collect (your samples) you are going to draw an inference about what is true about the population.\n\n\n\nSet up and instructions:\n\nOne person on each table should act as the “world” (the person who biases the cards). Congratulations, you are God! This person will determine what is true about the state of things in the world. This means they control what is contained in the deck of cards.\nFor each experiment, this person secretly looks at the cards, and removes some cards to use in the experiment. For example, from the set of 26 cards, they might choose to remove 4 black cards. The deck is now biased towards red (13 red; 9 black).\nIt’s important that no one sees what the cards are (the ones you’ve kept, or the ones removed). Shuffle the cards so they are ready to be sampled.\nThe remaining people (1 or more) will act as the experimenters. Your job is to draw samples and work out whether you think the deck is biased or not towards either red or black.\nNow copy the following text and code to your R Markdown document to record your work:\n\n### Experiment 1\n\nNumber of samples:\nTotal red cards found:\nTotal black cards found:\nConclusion: \nThe true bias was: \n\n\n```{r}\n\nbinom.test(x, n) # x is the number or red or black; n is the sample size\n\n```\n\n\nRunning each experiment\nDo these steps for each experiment:\n\nThe “World” removes some cards from the full deck (the number and colour of the cards removed is up to them). They shuffle the chosen cards ready to start the “experiment”.\nThe “Experimenters” pre-register their sample size. That is, they state how many cards they are going to draw.\nDraw samples one at a time (each experimenter can take one card, to speed things up).\n\n\n\n\n\n\n\nImportant!\n\n\n\nMake sure you replace all the cards each time you draw samples. The world/dealer should also give the pack a quick shuffle.\n\n\n\nDo step 3 until you have collect the sample size you chose\nOnce you have all the samples, the experimenters should draw a conclusion based initially on their own “gut feeling” about the data. Do you think the deck was biased towards red, black, or was it unbiased?\nChange the binom.test() code to provide a statistical result. Note the “p value”. Was this result unusual? How likely were the data given the null hypothesis?\nThe “world” can then reveal the hidden cards. Was the deck actually biased or not? How does this sit with a) your initial conclusions, and b) the result of the binomial test?\nComplete your record log in the .Rmd file. Feel free to Write a short statement about what you found in this experiment.\n\nRepeat all of the above steps (1-8) for a new experiment, making sure that you try different parameters for the experiment. So vary a) how many cards are removed from the deck, b) the combination of cards removed from the deck, and c) the pre-registered sample size. Feel free to swap the roles around.\nOnce you’ve conducted a few experiments, discuss on your table the results you found. It might be useful to think about the following things:\n\nwere there times when your intuitions were different to the statistical result? For example, you were sure there was a bias, but in fact the statistics told you this was not that unusual (p was &gt; .05)?\nwere there times when the deck was actually biased, but you failed to prove this with your experiment (you failed to see p &lt; .05)? Do you remember what this type of error is called?\nwere there times when the deck was not biased, but the test result suggested it was (p &lt; .05)? Do you remember what type of error this is called?",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#risky-and-safe-decisions",
    "href": "PSYC121/Week6.html#risky-and-safe-decisions",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Risky and safe decisions",
    "text": "Risky and safe decisions\nFor the second exercise today we will look at data from the survey on “risky and safe decisions”. You may remember that you were asked the following question:\n\n\n\n\n\n\nGain\n\n\n\nImagine you are on a game-show and you win £300. The presenter gives you have a choice: receive an additional £100 for sure, or take a gamble offering a 50% chance to gain £200 and a 50% chance to gain nothing. Which would you choose?\n\nreceive £100 for sure\ntake the gamble of a 50% chance to gain £200 and a 50% chance to gain nothing\n\n\n\nWe then asked you a similar question:\n\n\n\n\n\n\nLoss\n\n\n\nImagine you are on a different game-show and you win £500. Here you are given a choice between either losing £100 from your winnings for sure, or taking a gamble offering a 50% chance to lose nothing and a 50% chance to lose £200. Which would you choose?\n\nlose £100 for sure\ntake the gamble of a 50% chance to lose nothing and a 50% chance to lose £200\n\n\n\nWe’ve called these “gain” and “loss”, because in the first scenario you’re being asked about a chance to gain money, while in the second, it’s about a chance to lose money. Note that the “expected utility” of the choices is equivalent:\n\n\n\n\n\n\n“The expected utility of an act is a weighted average of the utilities of each of its possible outcomes, where the utility of an outcome measures the extent to which that outcome is preferred, or preferable, to the alternatives. The utility of each outcome is weighted according to the probability that the act will lead to that outcome.” see here\n\n\n\nThat’s to say, on average, you’ll end up with +£100 for the two cases in the gain scenario, or -£100 for the two cases in the loss scenario.\nBut what do people actually pick? Well people tend to be risk-averse, choosing the safe option overall. But interestingly, the safe option is picked far less when the scenario is presented as a loss. People seem to want to take the risk of potentially not losing anything (but maybe losing more).\nLet’s look to see if you showed the same pattern!\n\nCreate a new section of your markdown, giving it a suitable header\nCreate a new code chunk by clicking the “Code” menu, then “Insert Chunk”\nYou can download the data from this link, then upload it into the server.\nAdd a read_csv() command to read the data into the environment.\n\n\n\n\n\n\n\nReading CSVs\n\n\n\n\nRemember to set the working directory so R knows where the file is\nRemember to assign (&lt;-) to a new data object and give this a sensible name\n\n\n\n\nView the data, and see that the columns represent the gain and loss scenarios. The values represent the choices people made.\nUse the count() function to count the number of “safe” and “risky” choices that were made for our sample\n\n\ncount(my_data_object_name, gain)\n\n\nWe can now tell whether, in our sample, people tended to play it safe or take the risky choice. Did the sample have a meaningful bias towards one type of decision? If they didn’t we’d expect it to be a 50/50 split between safe and risky (people might make their choice at random). Use the binom.test() to look at whether the result would be expected by chance, noting the p value that is found.\nWrite a sentence or two after your code to explain what this result means.\nRepeat for the data from the loss column. Was the p value &lt; .05 here? Again, write a sentence or two to explain what this means.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week6.html#week-6-quiz",
    "href": "PSYC121/Week6.html#week-6-quiz",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Week 6 Quiz",
    "text": "Week 6 Quiz\nYou can access a set of quiz questions related to this week here.",
    "crumbs": [
      "Home",
      "PSYC121",
      "6. Sampling, probability and binomial tests"
    ]
  },
  {
    "objectID": "PSYC121/Week5.html",
    "href": "PSYC121/Week5.html",
    "title": "5. Class test",
    "section": "",
    "text": "Formulae for weeks 1-4\nHere you can download a list of the formulae used in the first half of this module.\nThere are no other materials for this week!\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC121",
      "5. Class test"
    ]
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html",
    "title": "PSYC121: Week 9 Lab",
    "section": "",
    "text": "For this week’s online tutorial we have data on how much students use social media platforms and people’s mean reading times from the stroop task. We will use these data to look at how we create new variables (columns) using mutate(). We will also expand what we have learnt about filtering data based on conditions, using the filter() function. Let’s take a look at the data we have using the summary(), to get useful statistics. You can also use head() to look at the first few rows of data\n\n\nsummary(data)\nhead(data)"
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html#exploring-the-data",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html#exploring-the-data",
    "title": "PSYC121: Week 9 Lab",
    "section": "",
    "text": "For this week’s online tutorial we have data on how much students use social media platforms and people’s mean reading times from the stroop task. We will use these data to look at how we create new variables (columns) using mutate(). We will also expand what we have learnt about filtering data based on conditions, using the filter() function. Let’s take a look at the data we have using the summary(), to get useful statistics. You can also use head() to look at the first few rows of data\n\n\nsummary(data)\nhead(data)"
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html#mutating-new-variables",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html#mutating-new-variables",
    "title": "PSYC121: Week 9 Lab",
    "section": "“Mutating” new variables",
    "text": "“Mutating” new variables\nIn this data set, all of the variables are numerical. Sometimes we may want to recode a variable to turn it from a continuous numerical variable, to a categorical/nominal variable. For example, maybe we want to compare students who are high users of facebook, to those who are low users of facebook. We could do that in the following way using the mutate() function:\n\n\ndata %&gt;% \n  mutate(facebook_use = facebook_days &gt;= 4)\n\n\n\n\n\nWhen you run this code you’ll see that mutate() has created a new column on the end, which tells us whether the person uses facebook for at least 4 days a week. Note that the new column is called “facebook_use” - this works in exactly the same way as the summarise() commands you have been practising, where you tell it the new variable name you want to create. The values “TRUE” and “FALSE” are not especially informative here - we probably want to be a bit clearer in the names we give to these levels of the new variable. To do that we can modify this code a little to use an if_else(), which checks if the “conditional statement” (facebook_days &gt;= 4) is TRUE or FALSE, and specifies the values to use in each case:\n\n\ndata %&gt;% \n  mutate(facebook_use = if_else(facebook_days &gt;= 4,true = \"high\",false = \"low\"))\n\n\n\n\n\n\nJust like with summarise(), we can have multiple new columns created within the one mutate() command. Each of these needs to be separated by a comma, and it’s good practice to put each one on a new line. Try copying and pasting the if_else command, and then modifying it to make two new variables to code for “high” and “low” instagram and twitter use:\n\n\ndata &lt;- \n  data %&gt;% \n  mutate(facebook_use = if_else(facebook_days &gt;= 4,true = \"high\",false = \"low\"),\n         instagram_use = ,\n         twitter_use = )\n\ndata\n# because we are assigning (&lt;-) the changes to update \"data\", \n# we write it again here to display the results\n\n\n\n\n\ndata &lt;- \n  data %&gt;% \n  mutate(facebook_use = if_else(facebook_days &gt;= 4,true = \"high\",false = \"low\"),\n         instagram_use = if_else(instagram_days &gt;= 4,true = \"high\",false = \"low\"),\n         twitter_use = if_else(twitter_days &gt;= 4,true = \"high\",false = \"low\"))\n\ndata\n# because we are assigning (&lt;-) the changes to update \"data\", \n# we write it again here to display the results\n\n\n\n\n\nLet’s now look at our avg_stroop variable, and first plot the data in a box and whisker plot:\n\n\ndata %&gt;% \n  ggplot() +\n  geom_boxplot(aes(y = avg_stroop))\n\n\n\n\n\n\nWe can see here that we have a number of points that lie outside the “whiskers” on the plot. One thing we can do to identify potential outliers is to create a new variable that transforms the data to a z distribution. This is easy to do in R using the scale() function:\n\n\ndata &lt;- \n  data %&gt;% \n  mutate(z_stroop = scale(avg_stroop))\n\ndata\n# because we are assigning (&lt;-) the changes to update \"data\", \n# we write it again here to display the results\n\n\n\n\n\n\nYou may remember that a distribution of z scores has a mean of 0 and a standard deviation of 1. Let’s check that here:\n\n\nmean()\nsd()\n\n\n\n\n\nmean(data_set$variable) # example\nsd(data_set$variable) # example\n\n\n\n\nmean(data$z_stroop)\nsd(data$z_stroop)\n\n\nYou’ll notice that the value for the mean is not quite 0 (but it is very very small!). This is probably due to a rounding of values in the scale() function. To get a value of 0 we could encolse the statement in the round() function, e.g. `round(mean(),digits = 2).\n\n\n\nA good way to get a sense of the range of values in our z-scores is to use arrange() to sort them in order. We can then use head() and tail() to see the values at each end of the distribution (the first ‘n’ rows, and the last ‘n’ rows)\n\n\ndata &lt;- \n  data %&gt;% \n  arrange(desc(z_stroop)) # arrange in descending order\n\nhead(data, n = 10)\ntail(data, n = 10)\n\n\n\nYou can see that we’ve got some quite extreme values here. First, we’ve got some very slow participants, showing average reading times of over 10 seconds. Secondly, we’ve got some extremely fast participants. The fastest participant of all is particularly unusual - perhaps they put in incorrect values into the survey?"
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html#filtering-data",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html#filtering-data",
    "title": "PSYC121: Week 9 Lab",
    "section": "Filtering data",
    "text": "Filtering data\nAs you can see, we quite often want to filter our data to select or remove some of the rows we are working with. To do this, we can use the filter() command.\nTo use filter(), we simply specify the data first, and then we need to use an expression to state how we want the data to be filtered. For example:\n\n\ndata %&gt;% \n  filter(z_stroop &lt;= 2) \n# find all those people who have z_stroop values lower than positive 2\n\n\n\n\nCommon expressions\nThe following table gives some examples of very common expressions used in filtering data:\n\n\n\n\n\nOperator\nMeaning\nExample\n\n\n\n\n==\nis the same as\nfilter(dataQ, age==27)\n\n\n&lt;\nis less than\nfilter(dataQ, age&lt;25)\n\n\n&gt;\nis greater than\nfilter(dataQ, age&gt;30)\n\n\n!=\nis not equal to\nfilter(dataQ, gender != 'female')\n\n\n&\nand\nfilter(dataQ, age&lt;30 & gender == 'female')\n\n\n|\nor\nfilter(dataQ, gender == 'male' | gender == 'non-binary')\n\n\n\n\n\n\n\nIt’s particularly important to note the difference between “==” and “=” in R. “=” is used as an assignment operator - you’ve used it several times already inside functions (e.g., na.rm = TRUE, mu = 29600). You can think of “=” as meaning “set this to”. In contrast the double equals operator, “==”, asks a question: “is this thing the same as this other thing?” In the above example, z_stroop &lt;= 2, it looks for all rows in the data where z_stroop is the same as, or less than, the value of 2. In programming terms, the expression returns a boolean value, which reports whether the statement is TRUE or FALSE (and when used in the filter, it finds all rows where it is TRUE). You can see this in the results of the following “conditional expressions”:\n\n\n2 == 3\n\"blah\" == \"blah\"\n\"blah\" == \"BLAH\"\n\"John\" == \"rock star\"\nmean(c(3,4,5,6)) == 4.5\nTRUE == FALSE # this is getting meta...\n\n\n\n\n\nPractice filtering\nPractice writing your own filter commands in the box below. Try to filter the data to match the following queries:\n\nData for those people who are “high” facebook users (hint: facebook_use == )\nData for those people who use instagram for 7 days, and have a z_stroop score of less than -1 (hint: use &)\nData for those people who are “high” users of at least one social media platform (hint: this needs two “or”: | )\n\n[Each hint here gives the solution to each query]\n\n\ndata %&gt;% \n  filter()\n\n\n\n\n\n# Query 1 solution\ndata %&gt;% \n  filter(facebook_use == \"high\")\n\n\n\n\n# Query 2 solution\ndata %&gt;% \n  filter(instagram_days == 7 & z_stroop &lt; -1)\n\n\n\n\n# Query 3 solution\ndata %&gt;% \n  filter(facebook_use == \"high\" | instagram_use == \"high\" | twitter_use == \"high\")"
  },
  {
    "objectID": "PSYC121/data/Week_9/Week_9_learnr.html#section-6",
    "href": "PSYC121/data/Week_9/Week_9_learnr.html#section-6",
    "title": "PSYC121: Week 9 Lab",
    "section": "",
    "text": "End of tutorial\nThis is now the end of the online tutorial on filter() and mutate(). Please return to the tasks in the lab worksheet."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to Year 1 statistics!\n\nThis year you will begin your statistics and data coding with R journey with us. You will complete two statistics modules:\n\nPSYC121: Statistics for Psychologists 1, with John Towse and Tom Beesley\nPSYC122: Statistics for Psychologists 2, with Margriet Groen and Rob Davies\n\nClick on the button below to access your module: Term 1: PSYC121, Term 2: PSYC122.\n\n\n  \n    \n\n    \n      PSYC121\n    \n    \n      PSYC122"
  },
  {
    "objectID": "PSYC122/Week11.html",
    "href": "PSYC122/Week11.html",
    "title": "1. Week 11 - Correlation",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week.\nToday we will take a look at correlation as a measure of association between two numerical variables. We will create scatterplots to visualise correlations, we will run a correlation analysis and we will practise interpreting and reporting the results.",
    "crumbs": [
      "Home",
      "PSYC122",
      "1. Week 11 - Correlation"
    ]
  },
  {
    "objectID": "PSYC122/Week11.html#sec-wk11-lectures",
    "href": "PSYC122/Week11.html#sec-wk11-lectures",
    "title": "1. Week 11 - Correlation",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material comes in two parts.\n\nTheory(~30 minutes) Watch this part before you complete the reading and the pre-lab activities. The video has captions, in case you find that helpful. You can download the slides and a transcript from the links below the video. Finally, if you open the video in ‘eStream’ by clicking on the green ‘e’ in the bottom right, you can navigate to separate sections of the video by clicking on the ‘chapters’ pane. This might be particularly useful if you quickly want to revisit a particular section.\n\n\nSlides Transcript\n\nHow to(~13 minutes) Watch this part after you’ve completed the pre-lab activities and before you attend the lab session.\n\n\nSlides Transcript",
    "crumbs": [
      "Home",
      "PSYC122",
      "1. Week 11 - Correlation"
    ]
  },
  {
    "objectID": "PSYC122/Week11.html#sec-wk11-reading",
    "href": "PSYC122/Week11.html#sec-wk11-reading",
    "title": "1. Week 11 - Correlation",
    "section": "Reading",
    "text": "Reading\nThe reading that accompanies the lectures this week and next week is from chapter 9 of the core text by Howell (2017).\nRougly, this week we’ll cover the material in sections 9.1 to 9.4, as well as sections 9.8 to 9.11 and section 9.15. Next week, we’ll cover the material in sections 9.5 to 9.7, and sections 9.12 to 9.13. Even if a section is not mentioned here, all of chapter 9 is relevant.",
    "crumbs": [
      "Home",
      "PSYC122",
      "1. Week 11 - Correlation"
    ]
  },
  {
    "objectID": "PSYC122/Week11.html#sec-wk11-prelab-activities",
    "href": "PSYC122/Week11.html#sec-wk11-prelab-activities",
    "title": "1. Week 11 - Correlation",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures on correlation and read the textbook sections you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Visualing correlations\nHave a look at this visualisation of correlations by Kristoffer Magnusson.\nAfter having read the relevant sections of Howell (2017) Chapter 9, use this visualisation page to visually replicate the scatterplots in Figures 9.1 and 9.2 - use a sample of 100. After that, visually replicate the scatterplots in Figure 9.3.\nEach time you change the correlation, pay attention to the shared variance (the overlap between the two variables) and see how this changes with the changing level of relationship between the two variables. The greater the shared variance, the stronger the relationship.\nAlso, try setting the correlation to r = .5 and then moving a single dot to see how one data point, a potential outlier, can change the stated correlation value between two variables.\n\n\nPre-lab activity 2: Guess the correlation\nNow that you are well versed in interpreting scatterplots (scattergrams) have a go at this online app on guessing the correlation.\nThis is a very basic app that allows you to see how good you are at recognising different correlation strengths from the scatterplots. We would recommend you click the “Track Performance” tab so you can keep an overview of your overall bias to underestimate or overestimate a correlation.\nIs this all just a bit of fun? Well, yes, because stats is actually fun, and no, because it serves a purpose of helping you determine if the correlations you see in your own data are real, and to help you see if correlations in published research match with what you are being told. As you will have seen from the above examples, one data point can lead to a misleading relationship and even what might be considered a medium to strong relationship may actually have only limited relevance in the real world. One only needs to mention Anscombe’s Quartet to be reminded of the importance of visualising your data, which leads us to the final pre-lab activity for this week.\n\n\nPre-lab activity 3: Anscombe’s quartet\nAnscombe (1973) showed that four sets of bivariate data (X, Y) that have the exact same means, medians, and relationships can look very different when plotted. You can read more about this here.\nAll in this is a clear example of why you should visualise your data and not to rely on just the numbers.\n\n\nPre-lab activity 4: Getting ready for the lab class\n\nRemind yourself of the basics of how to work with RStudio and get your files ready\nYou might want to re-visit the materials that John and Tom provided in PSYC121:\n\nBasics of working with RStudio\nCreate a folder for Week 11.\nDownload the 122_week11_forStudents.zip file and upload it into the new folder in RStudio Server you created at the previous step.\n\n\n\n\n\n\n\nIf you have difficulty uploading files to the server\n\n\n\nIf you get error messages when attempting to upload a file or a folder with files to the server, you can try the following steps:\n\nClose the R Studio server, close your browser and start afresh.\nOpen the R Studio server in a different browser.\nFollow a work around where you use code to directly download the file to the server. The code to do that will be available at the start of the lab activity where you need that particular file. The code to download the file you need to complete the quiz is below.\n\n\n\n\ndownload.file(\"https://github.com/lu-psy-r/statistics_for_psychologists/blob/main/PSYC122/data/week11/122_week11_forStudents/exams.csv?raw=true\", destfile = \"exams.csv\")",
    "crumbs": [
      "Home",
      "PSYC122",
      "1. Week 11 - Correlation"
    ]
  },
  {
    "objectID": "PSYC122/Week11.html#sec-wk11-labactivities",
    "href": "PSYC122/Week11.html#sec-wk11-labactivities",
    "title": "1. Week 11 - Correlation",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nconstructing and interpreting scatterplots\nrunning correlation analysis and interpret the results\nreporting the results in APA format\nconstructing a correlation matrix in APA format\nwhen and why to apply correlation analysis to answers questions in psychological science\n\n\nLab activity 1: Interpreting correlation\n\nQuestion 1\nBelow are scatterplots that show the relationship between ‘how much you know about correlation and how attractive you appear to members of the opposite (&/or same) sex’. Choose the type of correlation (strength and direction) displayed in each graph using one of the following:\n\nPerfect positive correlation\nPerfect negative correlation\nStrong positive correlation\nStrong negative correlation\nModerate positive correlation\nModerate negative correlation\nNull correlation\n\nFigure A \nFigure B \nFigure C \nFigure D \n\n\n\n\n\n\nNote\n\n\n\nFor the following questions, explain your chosen answer based on the statistic given, not on why you think the correlation may or may note make ‘logical’ sense.\n\n\n\n\nQuestion 2\nSuppose it was observed that there is a correlation of r = -.81 between a driver’s age and the cost of car insurance. This correlation would mean that, in general, older people pay more for car insurance.\nTRUE or FALSE? Explain why.\n\n\nQuestion 3\nSuppose that there is a correlation of r = .87 between the length of time a person is in prison and the amount of aggression the person displays on a psychological inventory administered at release. This means that spending a longer amount of time in prison causes people to become more aggressive.\nTRUE or FALSE? Explain why.\n\n\nQuestion 4\nA significant correlation was found between having great hair and performance in correlation labs. The correlation coefficient was .7. How much variance in correlation lab performance can the ‘greatness’ of your hair explain?\n\n51%\n70%\n49%\n30%\nWho cares I’ve got great hair.\n\nWhat was the reason for your answer?\nWhat is this ‘new coefficient’ called?\n\n\n\nLab activity 2: Visualising, calculating and reporting correlations\n\n\n\n\n\n\nWatch the ‘How to’ video\n\n\n\nIf you haven’t done so already, this is a good time to watch the ‘How to’ video (here Section 1) on ‘How to conduct a correlation analysis using R’.\n\n\nGoing back to the data discussed in the ‘How to’ video (see Section 1), you’ll remember it contains data from 25 8-year-old children on:\n\na standardised test of reading ability (Abil)\nintelligence (IQ)\nthe number of minutes per week spent reading in the home (Home)\nand the number of minutes per week spent watching TV (TV)\n\nIn the video on ‘How to conduct correlation analysis using R’ we looked at the correlation between reading ability and intelligence. Now, let’s look at the correlation between number of minutes per week spent reading in the home and watching TV.\n\n\n\n\n\n\nSet your working directory\n\n\n\nThe folder you were asked to download under ‘Pre-lab activity 4: Getting ready for the lab class’ contains the datafile (“MillerHadenData.csv”). Make sure you have set your working directory to this folder by right-clicking on it and selecting ‘Set as working directory’.\n\n\n\n\n\n\n\n\nDo this if you couldn’t upload files to the server\n\n\n\nIf you experienced difficulties with uploading a folder or a file to the server, you can use the code below to directly download the file you need in this lab activity to the server (instead of first downloading it to you computer and then uploading it to the server). Remember that you can copy the code to your clipboard by clicking on the ‘clipboard’ in the top right corner.\n\n\n\ndownload.file(\"https://github.com/lu-psy-r/statistics_for_psychologists/blob/main/PSYC122/data/week11/MillerHadenData.csv?raw=true\", destfile = \"MillerHadenData.csv\")\n\n\n\n\n\n\n\nNew R Markdown script\n\n\n\nBefore we begin, make sure you have started a new R Markdown script. If you need a reminder of how to do that, please revisit week 6 of PSYC121 (here).\n\n\n\nStep 1. Add the code to load the broom and the tidyverse libraries in a new code chunk. If you are unsure, you can look at the ‘Hint’ below for a clue by expanding it. After that, if you are still unsure, you can view the code by expanding the ‘Code’ section below.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the library()function. Remember to put it inside a ‘code chunk’ in your R Markdown script.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nlibrary(broom)\nlibrary(tidyverse)\n\n\n\n\nStep 2. Read in the data. You should now see a dataframe with 25 observations and 5 variables in the ‘Environment’. Click on it to view it.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the read_csv()function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nmh &lt;- read_csv(\"MillerHadenData.csv\")\n\n\n\n\nStep 3. Construct a scatterplot of the relationship between ‘Home’ and ‘TV’. Also add a line of best fit and make sure you use clear labels for your axes.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nUse the ggplot()function in combination with geom_point() and geom_smooth(). Make sure you use clear labels for your axes by using labs().\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nBelow is a template of the code, make sure to add the data frame and the relevant variable names. Also add clear labels for your axes.\nggplot(DATA, aes(x = , y = )) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nggplot(mh, aes(x = Home, y = TV)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  labs(x = \"Time spend reading at home\", y = \"Time spend watching TV at home\")\n\n\n\nQuestion 3a: What can you tell from the scatterplot about the direction of the relationship? Write a few sentences in your R Markdown script to describe the relationship.\n\nStep 4. Conduct the correlation analysis.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nUse the cor.test()function in combination with tidy().\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nBelow is a template of the code, make sure to add the data frame and the relevant variables names.\nresults &lt;- cor.test(DATA$X, \n                    DATA$Y, \n                    method = \"pearson\", \n                    alternative = \"two.sided\") %&gt;% \n  tidy()\nresults\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nresults &lt;- cor.test(mh$Home, \n                    mh$TV, \n                    method = \"pearson\", \n                    alternative = \"two.sided\") %&gt;% \n  tidy()\nresults\n\n\n\nOnce you’ve run this code chunk, the output should appear in your R Markdown script and you can answer the questions below using that output. You can also pull out the different pieces of information by using the pull() function and round the values using the round() function, like this:\nr &lt;- results %&gt;%\n  pull(estimate) %&gt;%\n  round(2)\nQuestion 4a: What is the correlation coefficient (Pearson’s r)?\nQuestion 4b: What is the p value?\nQuestion 4c: Is the correlation significant at the p &lt; .05 level?\nQuestion 4d: What are the degrees of freedom you need to report?\n\nStep 5. Calculate how much variance in ‘time spent reading’ can be accounted for by ‘time spent watching TV’?\n\nAs discussed in the theory lecture, this is referred to as the ‘coefficient of determination’ or ‘R-squared’. To calculate it, you square the value for Pearson’s r. To calculate how much variance in one variable is accounted for by the other variable, you multiply it by 100 and round it to 0 decimals.\n\nStep 6. Write a few sentences in which you report this result, following APA guidelines.\n\n\n\nLab activity 3: Hazardous alcohol use and impulsivity\nResearchers were interested in the relationship between hazardous alcohol use and impulsivity (making unplanned, rapid decisions without thinking or ‘acting on a whim’). To investigate the relationship, 20 participants completed both the alcohol use disorder identification test (AUDIT; Saunders, Aasland, Babor, de la Fuente, & Grant, 1993) and Barratt’s Impulsiveness Scale (BIS-11) (Patton, Stanford, & Barratt, 1995).\n\n\n\n\n\n\nBefore we begin\n\n\n\nAssuming you are using the same R Markdown script as for the previous lab activity, you should already have code to load the broom and the tidyverse libraries. If this is a new session, you just need to re-run that code chunk to ensure they are loaded. At this point, it is a good idea to clear your environment to avoid any confusion between data frames or values. You can do this by clicking on the broom icon on the top right of the Environment pane. The data file (“alcoholUse_Impulsivity.csv”) is in the folder you were asked to download under ‘Pre-lab activity 4: Getting ready for the lab class’. As long as that folder is set as your working directory, you are good to go.\n\n\n\n\n\n\n\n\nDo this if you couldn’t upload files to the server\n\n\n\nIf you experienced difficulties with uploading a folder or a file to the server, you can use the code below to directly download the file you need in this lab activity to the server (instead of first downloading it to you computer and then uploading it to the server). Remember that you can copy the code to your clipboard by clicking on the ‘clipboard’ in the top right corner.\n\n\n\ndownload.file(\"https://github.com/lu-psy-r/statistics_for_psychologists/blob/main/PSYC122/data/week11/122_week11_forStudents/alcoholUse_Impulsivity.csv?raw=true\", destfile = \"alcoholUse_Impulsivity.csv\")\n\n\nStep 1. Read in the data. You should now see an object containing the data in the ‘Environment’.\n\nQuestion 1a: How many variables does it have?\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the read_csv()function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\ndata &lt;- read_csv(\"alcoholUse_Impulsivity.csv\")\n\n\n\n\nStep 2. Plot the relationship between hazard alcohol use and impulsivity using a scatterplot and a line of best fit\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nUse the ggplot()function in combination with geom_point() and geom_smooth(). Make sure you use clear labels for your axes by using labs().\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nUse this code template and add the name of the data frame and the variables. Also add informative labels for your axes.\nggplot(, aes(x = , y = )) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below\nggplot(data, aes(x = hau, y = imp)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  labs(x = \"Hazardous Alcohol Use\", y = \"Impulsivity\")\n\n\n\nQuestion 2a: What can you tell from the scatterplot about the direction of the relationship?\n\nStep 3. Conduct the correlation analysis.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nUse the cor.test()function in combination with tidy().\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nBelow is a template of the code, make sure to add the data frame and the relevant variables names.\nresults &lt;- cor.test(DATA$X, \n                    DATA$Y, \n                    method = \"pearson\", \n                    alternative = \"two.sided\") %&gt;% \n  tidy()\nresults\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nresults &lt;- cor.test(data$hau, \n                    data$imp, \n                    method = \"pearson\", \n                    alternative = \"two.sided\") %&gt;% \n  tidy()\n\nresults\n\n\n\nQuestion 3a: What is the correlation coefficient (Pearson’s r)?\nQuestion 3b: What is the p value?\nQuestion 3c: Is the correlation significant at the p &lt; .05 level?\nQuestion 3d: What are the degrees of freedom you need to report?\nQuestion 3e: How much variance in ‘impulsivity’ can be accounted for by ‘hazardous alcohol use’?\nQuestion 3f: Give three logically possible directions of causality, indicating for each direction whether it is a plausible explanation in light of the variables involved (and why). No, this is not a trick question —-I know that correlation does not infer causation, but think critically! New studies/ideas are constructed by thinking what the previous study doesn’t tell us about what could be happening with the variables of interest.\n\nStep 4. Write a few sentences in which you report this result, following APA guidelines.\n\nJob completed — Well done!",
    "crumbs": [
      "Home",
      "PSYC122",
      "1. Week 11 - Correlation"
    ]
  },
  {
    "objectID": "PSYC122/Week11.html#answers",
    "href": "PSYC122/Week11.html#answers",
    "title": "1. Week 11 - Correlation",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at code written by someone else…\n\n\nLab activity 1: Interpreting correlation\n\nBelow are scatterplots that show the relationship between ‘how much you know about correlation and how attractive you appear to members of the opposite (&/or same) sex’. Choose the type of correlation (strength and direction) displayed in each graph.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nstrong positive correlation\nnull correlation\nmoderate positive correlation\nperfect negative correlation\n\n\n\n\n\nSuppose it was observed that there is a correlation of r = -.81 between a driver’s age and the cost of car insurance. This correlation would mean that, in general, older people pay more for car insurance.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFALSE Explanation: The correlation coefficient is negative and therefore infers a negative correlation. As such, older people pay less for car insurance: as age increases, car insurance costs decrease.\n\n\n\n\nSuppose that there is a correlation of r = .87 between the length of time a person is in prison and the amount of aggression the person displays on a psychological inventory administered at release. This means that spending a longer amount of time in prison causes people to become more aggressive.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFALSE Explanation: This is a bit of trick question as it has the sneaky ‘cause’ word in. The correlation coefficient is a positive number, suggesting a positive relationship between length of time in prison and aggression. However, causation cannot be inferred from correlation and therefore we cannot know whether time spent in prison CAUSES aggression, and rather we suggest a relationship between the two: as length of time in prison increases, aggression increases.\n\n\n\n\nA significant correlation was found between having great hair and performance in correlation labs. The correlation coefficient was .7. How much variance in correlation lab performance can the ‘greatness’ of your hair explain?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nc 49% The ‘coefficient of determination’ or ‘R-squared’ tells us the proportion or variance in one variable that can be predicted if we know the other variable. We can determine this by squaring the r. Therefore, .72 = .49, R2 = .49.\n\n\n\n\n\nLab activity 2: Constructing scatterplots and calculating correlations\nYou can download the RMd-script that contains the code to complete lab activities 2 and 3 here: 122_wk11_labActivities2_3.Rmd.\nQuestion 3a: What can you tell from the scatterplot about the direction of the relationship?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere is a negative association between ‘Home’ and ‘TV’. This means that the longer a child spends watching TV, the shorter they will read at home.\n\n\n\nQuestion 4a: What is the correlation coefficient (Pearson’s r)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nr = -.65\n\n\n\nQuestion 4b: What is the p value?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\np &lt; .001\n\n\n\nQuestion 4c: Is the correlation significant at the p &lt; .05 level?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes, because the p-value is smaller than .005\n\n\n\nQuestion 4d: What are the degrees of freedom you need to report?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n23\n\n\n\nStep 5. How much variance in ‘time spent reading’ can be accounted for by ‘time spent watching TV’?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n42%\n\n\n\nStep 6. Write a few sentences in which you report this result, following APA guidelines.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSomething along the lines of: A Pearson’s correlation coefficient was used to assess the relationship between time spent reading at home and time spent watching TV at home. There was a significant negative correlation, r(23) = -.65, p &lt; .001. As time spent watching TV increased, time spent reading at home decreased.\n\n\n\n\n\nLab activity 3: Hazardous alcohol use and impulsivity\nQuestion 1a: How many variables does it have?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n3\n\n\n\nQuestion 2a: What can you tell from the scatterplot about the direction of the relationship?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere is a positive association between ‘hazardous alcohol use’ and ‘impulsivity’. This means that as a participant’s score on ‘hazardous alcohol use’ goes up, their score on ‘impulsivity’ also goes up.\n\n\n\nQuestion 3a: What is the correlation coefficient (Pearson’s r)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nr = .54\n\n\n\nQuestion 3b: What is the p value?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\np = .014\n\n\n\nQuestion 3c: Is the correlation significant at the p &lt; .05 level?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes\n\n\n\nQuestion 3d: What are the degrees of freedom you need to report?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n18\n\n\n\nQuestion 3e: How much variance in ‘impulsivity’ can be accounted for by ‘hazardous alcohol use’?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n29%\n\n\n\nQuestion 3f:. Give three logically possible directions of causality, indicating for each direction whether it is a plausible explanation in light of the variables involved (and why). No, this is not a trick question —-I know that correlation does not infer causation, but think critically! New studies/ideas are constructed by thinking what the previous study doesn’t tell us about what could be happening with the variables of interest.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nJust really looking for reasoning here.\nExamples:\n\nBeing more impulsive may make people consume more alcohol.\nConsuming more alcohol may make people more impulsive.\nAn outgoing personality might influence both your level of impulsivity and you are more likely to be socialising in the pub and consuming alcohol. So the same ‘third factor’ may influence both our variables of interest.\n\n\n\n\nStep 4. Write a few sentences in which you report this result, following APA guidelines.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSomething along the lines of: A Pearson’s correlation coefficient was used to assess the relationship between alcohol use and impulsivity. There was a significant positive correlation, r(18) = .54, p &lt; .014. People who reported to consume more alcohol, scored higher on the impulsivity scale.",
    "crumbs": [
      "Home",
      "PSYC122",
      "1. Week 11 - Correlation"
    ]
  },
  {
    "objectID": "PSYC122/Week11.html#online-qa",
    "href": "PSYC122/Week11.html#online-qa",
    "title": "1. Week 11 - Correlation",
    "section": "Online Q&A",
    "text": "Online Q&A\nBelow is the recording of this week’s online Q&A.\n\nTranscript",
    "crumbs": [
      "Home",
      "PSYC122",
      "1. Week 11 - Correlation"
    ]
  },
  {
    "objectID": "PSYC122/data/week14/122_wk14_labAct2.html",
    "href": "PSYC122/data/week14/122_wk14_labAct2.html",
    "title": "122_wk13_labActivity2",
    "section": "",
    "text": "Aim: What we want to do is to run a chi-square analysis to determine whether those in the RTA condition were more likely to remember to return the paper-clips than those in the control condition."
  },
  {
    "objectID": "PSYC122/data/week14/122_wk14_labAct2.html#lab-activity-2---reminders-through-association",
    "href": "PSYC122/data/week14/122_wk14_labAct2.html#lab-activity-2---reminders-through-association",
    "title": "122_wk13_labActivity2",
    "section": "",
    "text": "Aim: What we want to do is to run a chi-square analysis to determine whether those in the RTA condition were more likely to remember to return the paper-clips than those in the control condition."
  },
  {
    "objectID": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html",
    "href": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html",
    "title": "2023-24-PSYC122-w19-how-to",
    "section": "",
    "text": "In Week 19, we aim to further develop skills in working with the linear model.\nWe do this to learn how to answer research questions like:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\nIn this class, what is new is our focus on critically evaluating – comparing, reflecting on – the evidence from more than one relevant study.\n\nThis work simulates the kind of critical evaluation of evidence that psychologists must do in professional research.\n\n\n\nI will format dataset names like this:\n\nstudy-one-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}.\n\n\n\nIn this how-to guide, we use data from two 2020 studies of the response of adults from a UK national sample to written health information:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nThe reason we are going to work with two datasets is that we will be comparing the results of analyses of the data to assess whether the results are robust.\nHere, our assessment of robustness focuses on whether similar results are found in two different studies.\nCheck out the PSYC122 Week 19 lecture for a discussion of how an assessment of robustness is important to psychological science."
  },
  {
    "objectID": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#naming-things",
    "href": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#naming-things",
    "title": "2023-24-PSYC122-w19-how-to",
    "section": "",
    "text": "I will format dataset names like this:\n\nstudy-one-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}."
  },
  {
    "objectID": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#the-data-we-will-be-using",
    "href": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#the-data-we-will-be-using",
    "title": "2023-24-PSYC122-w19-how-to",
    "section": "",
    "text": "In this how-to guide, we use data from two 2020 studies of the response of adults from a UK national sample to written health information:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nThe reason we are going to work with two datasets is that we will be comparing the results of analyses of the data to assess whether the results are robust.\nHere, our assessment of robustness focuses on whether similar results are found in two different studies.\nCheck out the PSYC122 Week 19 lecture for a discussion of how an assessment of robustness is important to psychological science."
  },
  {
    "objectID": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-1-set-up",
    "href": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-1-set-up",
    "title": "2023-24-PSYC122-w19-how-to",
    "section": "Step 1: Set-up",
    "text": "Step 1: Set-up\nTo begin, we set up our environment in R.\n\nTask 1 – Run code to empty the R environment\n\nrm(list=ls())                            \n\n\n\nTask 2 – Run code to load relevant libraries\n\nlibrary(\"ggeffects\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-2-load-the-data",
    "href": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-2-load-the-data",
    "title": "2023-24-PSYC122-w19-how-to",
    "section": "Step 2: Load the data",
    "text": "Step 2: Load the data\n\nTask 3 – Read in the data files we will be using\nThe data files are called:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nUse the read_csv() function to read the data files into R:\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")  \n\nRows: 169 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstudy.two.gen &lt;- read_csv(\"study-two-general-participants.csv\")  \n\nRows: 172 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data files in, give the data objects you create distinct name e.g. study.one.gen versus study.two.gen.\n\n\nTask 4 – Inspect the data file\nUse the summary() or head() functions to take a look at both datasets.\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\nsummary(study.two.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:172         Min.   :0.4107   Min.   :3.786   Length:172        \n Class :character   1st Qu.:0.6786   1st Qu.:6.411   Class :character  \n Mode  :character   Median :0.7679   Median :7.321   Mode  :character  \n                    Mean   :0.7596   Mean   :7.101                     \n                    3rd Qu.:0.8393   3rd Qu.:7.946                     \n                    Max.   :0.9821   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :29.00  \n 1st Qu.:25.00   1st Qu.:32.75   1st Qu.: 7.750   1st Qu.:47.00  \n Median :32.50   Median :36.00   Median : 9.000   Median :51.00  \n Mean   :35.37   Mean   :35.13   Mean   : 9.064   Mean   :51.24  \n 3rd Qu.:44.00   3rd Qu.:39.00   3rd Qu.:11.000   3rd Qu.:56.25  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:172         Length:172         Length:172        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :14.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.88                                                           \n 3rd Qu.:16.00                                                           \n Max.   :20.00                                                           \n\n\nNotice that study.two.gen was designed to be a replication of study.one.gen.\n\nWe use the same online survey methods to collect data in both studies.\nWe present different health information texts in the different studies and recorded responses from different groups of adults in the UK."
  },
  {
    "objectID": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-3-compare-the-data-from-the-different-studies",
    "href": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-3-compare-the-data-from-the-different-studies",
    "title": "2023-24-PSYC122-w19-how-to",
    "section": "Step 3: Compare the data from the different studies",
    "text": "Step 3: Compare the data from the different studies\n\nRevise: practice to strengthen skills\n\n\nTask 5 – Compare the data distributions from the two studies\n\nQ.1. What is the mean of the mean.acc and SHIPLEY variables in the two studies?\nA.1. The means are:\nstudy one – mean.acc – mean = 0.8163\nstudy one – mean.self – mean = 6.906\nstudy two – mean.acc – mean = 0.7596\nstudy two – mean.self – mean = 7.101\nQ.2. Draw histograms of both mean.acc and mean.self for both studies.\nA.2. You can write the code as you have been shown to do e.g. in 2023-24-PSYC122-w17-how-to.Rmd:\n\n\nggplot(data = study.one.gen, aes(x = mean.acc)) + \n  geom_histogram(binwidth = .1) +\n  theme_bw() +\n  labs(x = \"Mean accuracy (mean.acc)\", y = \"frequency count\") +\n  xlim(0, 1)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = mean.self)) + \n  geom_histogram(binwidth = 1) +\n  theme_bw() +\n  labs(x = \"Mean self-rated accuracy (mean.self)\", y = \"frequency count\") +\n  xlim(0,10)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = mean.acc)) + \n  geom_histogram(binwidth = .1) +\n  theme_bw() +\n  labs(x = \"Mean accuracy (mean.acc)\", y = \"frequency count\") +\n  xlim(0, 1)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = mean.self)) + \n  geom_histogram(binwidth = 1) +\n  theme_bw() +\n  labs(x = \"Mean self-rated accuracy (mean.self)\", y = \"frequency count\") +\n  xlim(0,10)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\nIntroduce: make some new moves\n\n\nTask 6 – Create grids of plots to make the comparison easier to do\n\n\nhint: Task 6 – What we are going to do is to create two histograms and then present them side by side to allow easy comparison of variable distributions\nWe need to make two changes to the coding approach you have been using until now.\nBefore we explain anything, let’s look at an example: run these line of code and check the result.\n\nMake sure you identify what is different about the plotting code, shown following, compared to what you have done before: there is a surprise in what is going to happen.\n\nFirst, create plot objects, give them names, but do not show them:\n\nplot.one &lt;- ggplot(data = study.one.gen, aes(x = mean.acc)) + \n  geom_histogram(binwidth = .1) +\n  theme_bw() +\n  labs(x = \"Mean accuracy (mean.acc)\", y = \"frequency count\", title = \"Study One\") +\n  xlim(0, 1)\n\nplot.two &lt;- ggplot(data = study.two.gen, aes(x = mean.acc)) + \n  geom_histogram(binwidth = .1) +\n  theme_bw() +\n  labs(x = \"Mean accuracy (mean.acc)\", y = \"frequency count\", title = \"Study Two\") +\n  xlim(0, 1)\n\nSecond, show the plots, side-by-side:\n\nplot.one + plot.two\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nThis is what you are doing: check out the process, step-by-step. (And notice that you repeat the process for each of two (or more) plots.)\n\nggplot(...) tell R you want to make a plot using the ggplot() function;\nplot.one &lt;- tell R you want to give the plot a name; the name appears in the environment;\nggplot(data = study.one.gen ...) tell R you want to make a plot with the study.two data;\nggplot(..., aes(x = mean.acc)) tell R that you want to make a plot with the variable mean.acc;\n\n\nhere, specify the aesthetic mapping, x = mean.acc\n\n\ngeom_histogram() tell R you want to plot values of mean.acc as a histogram;\nbinwidth = .1 adjust the binwidth to show enough detail but not too much in the distribution;\ntheme_bw() tell R what theme you want, adjusting the plot appearance;\nlabs(x = \"Mean accuracy (mean.acc)\", y = \"frequency count\", title = \"Study One\") fix the x-axis and y-axis labels;\n\n\nhere, add a title for the plot, so you can tell the two plots apart;\n\n\nxlim(0, 1) adjust the x-axis limits to show the full range of possible score values on this variable.\n\nDo this process twice, once for each dataset, creating two plots so that you can compare the distribution of mean.acc scores between the studies.\nFinally, having created the two plots, produce them for viewing:\n\nplot.one + plot.two having constructed – and named – both plots, you enter their names, separated by a +, to show them in a grid of two plots.\n\nNotice: until you get to step 10, nothing will appear.\nThis will be surprising but it is perfectly normal when we increase the level of complexity of the plots we build.\n\nYou first build the plots.\nYou are creating plot objects and you give these objects names.\nThe objects will appear in the Environment with the names you give them.\nYou then produce the plots for viewing, by using their names.\n\nUntil you complete the last step, you will not see any changes until you use the object names to produce them for viewing.\nThis is how you construct complex arrays of plots.\n\n\nTask 7 – Try this out for yourself, focusing now on the distribution of mean.self scores in the two studies\nFirst, create plot objects but do not show them.\n\nGive each plot a name. You will use the names next.\n\n\nplot.one &lt;- ggplot(data = study.one.gen, aes(x = mean.self)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Self-rated accuracy (mean.self)\", y = \"frequency count\", title = \"Study One\") +\n  xlim(0, 10)\n\nplot.two &lt;- ggplot(data = study.two.gen, aes(x = mean.self)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Self-rated accuracy (mean.self)\", y = \"frequency count\", title = \"Study Two\") +\n  xlim(0, 10)\n\nSecond produce the plots for viewing, side-by-side, by naming them.\n\nplot.one + plot.two\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\nQ.3. Now use the plots to do some data analysis work: how do the mean.self distributions compare, when you compare the mean.self of study.one.gen versus `mean.self of study.two.gen?\nA.3. When you compare the plots side-by-side you can see that the mean.self distributions are similar in the two studies: most people have high mean.self scores. This means that they rated the accuracy of their understanding at a high level, on average.\nQ.4. Is the visual impression you get from comparing the distributions consistent with the statistics you see in the summary?\nA.4. Yes: If you go back to the summary of mean.self, comparing the two studies datasets, then you can see that the median and mean are similar (around 7) in both study.one.gen and study.two.gen datasets."
  },
  {
    "objectID": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-4-now-use-scatterplots-and-correlation-to-examine-associations-between-variables",
    "href": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-4-now-use-scatterplots-and-correlation-to-examine-associations-between-variables",
    "title": "2023-24-PSYC122-w19-how-to",
    "section": "Step 4: Now use scatterplots and correlation to examine associations between variables",
    "text": "Step 4: Now use scatterplots and correlation to examine associations between variables\n\nRevise: practice to strengthen skills\n\n\nTask 8 – Draw scatterplots to compare the potential association between mean.acc and mean.self in both study.one.gen and study.two.gen datasets\n\n\nhint: Task 8 – The plotting steps are explained in some detail in 2023-24-PSYC122-w17-how-to.Rmd\n\nggplot(data = study.one.gen, aes(x = mean.self, y = mean.acc)) +\n  geom_point(alpha = 0.75, size = 3, colour = \"darkgrey\") +\n  geom_smooth(method = \"lm\", size = 1.5, colour = \"green\") +\n  theme_bw() +\n  labs(x = \"mean self-rated accuracy\", y = \"mean accuracy\") +\n  xlim(0, 10) + ylim(0, 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = mean.self, y = mean.acc)) +\n  geom_point(alpha = 0.75, size = 3, colour = \"darkgrey\") +\n  geom_smooth(method = \"lm\", size = 1.5, colour = \"green\") +\n  theme_bw() +\n  labs(x = \"mean self-rated accuracy\", y = \"mean accuracy\") +\n  xlim(0, 10) + ylim(0, 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nTask 9 – Create a grid of plots to make the comparison easier to do\n\n\nhint: Task 9 – We follow the same steps as we used in tasks 6 and 7 to create the plots\nWe again:\n\nFirst construct the plot objects and give them names;\nThen create and show a grid of named plots.\n\nThough this time we are producing a grid of scatterplots.\nFirst, create plot objects – give them names but do not show them:\n\nplot.one &lt;- ggplot(data = study.one.gen, aes(x = mean.self, y = mean.acc)) +\n  geom_point(alpha = 0.75, size = 3, colour = \"darkgrey\") +\n  geom_smooth(method = \"lm\", size = 1.5, colour = \"green\") +\n  theme_bw() +\n  labs(x = \"mean self-rated accuracy\", y = \"mean accuracy\", title = \"Study One\") +\n  xlim(0, 10) + ylim(0, 1)\n\nplot.two &lt;- ggplot(data = study.two.gen, aes(x = mean.self, y = mean.acc)) +\n  geom_point(alpha = 0.75, size = 3, colour = \"darkgrey\") +\n  geom_smooth(method = \"lm\", size = 1.5, colour = \"green\") +\n  theme_bw() +\n  labs(x = \"mean self-rated accuracy\", y = \"mean accuracy\", title = \"Study Two\") +\n  xlim(0, 10) + ylim(0, 1)\n\n\nNotice that in the plotting code we ask R to give each plot a title using labs().\n\nSecond name the plots, to show them side-by-side in the plot window:\n\nplot.one + plot.two\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow use the plots to make comparison judgments.\n\nQ.5. How does the association, shown in the plots, between mean.self and mean.acc compare when you look at the study.one.gen versus the study.two.gen plot?\nhint: Q.5. When comparing evidence about associations in different studies, we are mostly going to focus on the slope – the angle – of the prediction lines, and the ways in which points do or do not cluster about the prediction lines.\nA.5. If you examine the study.one.gen versus the study.two.gen plots then you can see that in both plots higher mean.self scores appear to be associated with higher mean.acc scores. But the trend maybe is a bit stronger – the line is steeper – in study.two.gen compared to study.two.gen.\n\nWe are now in a position to answer one of our research questions:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nIf people can accurately evaluate whether they correctly understand written health information then mean.self (a score representing their evaluation) should be associated with mean.acc (a score representing their accuracy of understanding) for each person.\n\n\nRevise: practice to strengthen skills\n\n\nTask 10 – Can you estimate the association between mean.acc and mean.self in both datasets?\n\n\nhint: Task 10 – Use cor.test() as you have been shown how to do e.g. in 2023-24-PSYC122-w16-how-to.Rmd\nDo the correlation for both datasets.\nFirst, look at the correlation between mean.acc and mean.self in study.one.gen:\n\ncor.test(study.one.gen$mean.acc, study.one.gen$mean.self, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.one.gen$mean.acc and study.one.gen$mean.self\nt = 7.1936, df = 167, p-value = 2.026e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3619961 0.5937425\nsample estimates:\n      cor \n0.4863771 \n\n\n\nQ.6. What is r, the correlation coefficient?\nA.6. r = 0.4863771\nQ.7. Is the correlation significant?\nA.7. r is significant\nQ.8. What are the values for t and p for the significance test for the correlation?\nA.8. t = 7.1936, p-value = 2.026e-11\n\nSecond, look at the correlation between mean.acc and mean.self in study.two.gen:\n\ncor.test(study.two.gen$mean.acc, study.two.gen$mean.self, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$mean.acc and study.two.gen$mean.self\nt = 8.4991, df = 170, p-value = 9.356e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4317217 0.6431596\nsample estimates:\n      cor \n0.5460792 \n\n\n\nQ.9. What is r, the correlation coefficient?\nA.9. r = 0.5460792\nQ.10. Is the correlation significant?\nA.10. r is significant\nQ.11. What are the values for t and p for the significance test for the correlation?\nA.11. t = 8.4991, p = 9.356e-15\n\nNow we can answer the research question:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\n\nQ.12. What do the correlation estimates tell you is the answer to the research question?\nA.12.\n\n\nThe correlations are positive and significant, indicating that higher mean.self (evaluations) are associated with higher mean.acc (understanding), suggesting that people can judge their accuracy of understanding.\n\n\nQ.13. Can you compare the estimates, given the two datasets, to evaluate if the result in study.one.gen is replicated in study.two.gen?\nhint: Q.13. We can judge if the result in a study is replicated in another study by examining if – here – the correlation coefficient is significant in both studies and if the coefficient has the same size and sign in both studies.\nA.13. If you compare the correlation estimates from both study.one.gen and study.two.gen you can see:\nfirst, the correlation is significant in both studies;\nsecond, the correlation is positive in both studies,\nthird, the correlation is similar in magnitude, about \\(r = .5\\) in both studies.\n\nThis suggests that the association we see in study.one.gen is replicated in study.two.gen."
  },
  {
    "objectID": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-5-use-a-linear-model-to-to-answer-the-research-questions-multiple-predictors",
    "href": "PSYC122/data/week19/2023-24-PSYC122-w19-how-to.html#step-5-use-a-linear-model-to-to-answer-the-research-questions-multiple-predictors",
    "title": "2023-24-PSYC122-w19-how-to",
    "section": "Step 5: Use a linear model to to answer the research questions – multiple predictors",
    "text": "Step 5: Use a linear model to to answer the research questions – multiple predictors\n\nRevise: practice to strengthen skills\n\n\nTask 11 – Examine the relation between outcome mean accuracy (mean.acc) and multiple predictors\nWe specify linear models including as predictors the variables:\n\nhealth literacy (HLVA);\nvocabulary (SHIPLEY);\nreading strategy (FACTOR3).\n\n\n\nhint: Task 11 – Use lm(), as you have done before, see e.g. 2023-24-PSYC122-w18-how-to.R\n\n\nTask 11 – Examine the predictors of mean accuracy (mean.acc), first, for the study.one.gen data\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40322 -0.05349  0.01152  0.07128  0.18434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.302030   0.091257   3.310  0.00115 ** \nHLVA        0.017732   0.003923   4.521 1.17e-05 ***\nSHIPLEY     0.005363   0.002336   2.296  0.02291 *  \nFACTOR3     0.003355   0.001264   2.654  0.00872 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1033 on 165 degrees of freedom\nMultiple R-squared:  0.2474,    Adjusted R-squared:  0.2337 \nF-statistic: 18.08 on 3 and 165 DF,  p-value: 3.423e-10\n\n\nUsing the model estimates, we can answer the research question:\n\nWhat person attributes predict success in understanding?\n\nInspect the model summary, then answer the following questions:\n\nQ.14. What is the estimate for the coefficient of the effect of the predictor SHIPLEY in this model?\nA.14. 0.005363\nQ.15. Is the effect significant?\nA.15. It is significant, p &lt; .05\nQ.16. What are the values for t and p for the significance test for the coefficient?\nA.16. t = 2.296, p = 0.02291\nQ.17. Now consider the estimates for all the variables, what do you conclude is the answer to the research question – given the study.one.gen data:\n\n\nWhat person attributes predict success in understanding?\n\n\nhint: Q.17. Can you report the model and the model fit statistics using the language you have been shown in the week 18 lecture?\nA.17.\n\n\nWe fitted a linear model with mean comprehension accuracy as the outcome and health literacy (HLVA), reading strategy (FACTOR3), and vocabulary (SHIPLEY) as predictors. The model is significant overall, with F(3, 165) = 18.08, p &lt; .001, and explains 23% of variance (adjusted R2 = 0.23). Mean accuracy was predicted to be higher given higher scores in health literacy (HLVA estimate = .018, t = 4.52, p &lt; .001), vocabulary knowledge (SHIPLEY estimate = .005, t = 2.96, p &lt; .001), and reading strategy (FACTOR3 estimate = .003, t = 2.65, p = .009).\n\n\n\nTask 12 – Examine the predictors of mean accuracy (mean.acc), now, for the study.two.gen data\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.two.gen)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.242746 -0.074188  0.003173  0.075361  0.211357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.146896   0.076325   1.925  0.05597 .  \nHLVA        0.017598   0.003589   4.904  2.2e-06 ***\nSHIPLEY     0.008397   0.001853   4.533  1.1e-05 ***\nFACTOR3     0.003087   0.001154   2.675  0.00822 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.097 on 168 degrees of freedom\nMultiple R-squared:  0.3636,    Adjusted R-squared:  0.3522 \nF-statistic: 31.99 on 3 and 168 DF,  p-value: &lt; 2.2e-16\n\n\nUsing the model estimates, we can answer the research question:\n\nWhat person attributes predict success in understanding?\n\nInspect the model summary, then answer the following questions:\n\nQ.18. What is the estimate for the coefficient of the effect of the predictor, SHIPLEY, in this model?\nA.18. 0.008397\nQ.19. Is the effect significant?\nA.19. It is significant, p &lt; .05\nQ.20. What are the values for t and p for the significance test for the coefficient?\nA.20. t = 4.533, p = 1.1e-05\nQ.21. Now consider the estimates for all the variables, what do you conclude is the answer to the research question – given the study.two.gen data:\n\n\nWhat person attributes predict success in understanding?\n\n\nhint: Q.21. Can you report the model and the model fit statistics using the language you have been shown in the week 18 lecture?\nA.21.\n\n\nWe fitted a linear model with mean comprehension accuracy as the outcome and health literacy (HLVA), reading strategy (FACTOR3), and vocabulary (SHIPLEY) as predictors. The model is significant overall, with F(3, 168) = 31.99, p &lt; .001, and explains 35% of variance (adjusted R2 = 0.35). Mean accuracy was predicted to be higher given higher scores in health literacy (HLVA estimate = .018, t = 4.90, p &lt; .001), vocabulary knowledge (SHIPLEY estimate = .008, t = 4.53, p &lt; .001), and reading strategy (FACTOR3 estimate = .003, t = 2.68, p = .008).\n\n\nQ.22. Are the findings from study.one.gen replicated in study.two.gen?\nhint: Q.22. We can judge if the results in an earlier study are replicated in another study by examining if – here – the linear model estimates are significant in both studies and if the coefficient estimates have the same size and sign in both studies.\nA.22. If you compare the linear model coefficient estimates from both study.one.gen and study.two.gen you can see:\nfirst, the HLVA, SHIPLEY and FACTOR3 estimates are significant in both study.one.gen and study.two.gen;\nsecond, the estimates have the same sign – positive – in both studies.\n\nThis suggests that the results we see in study.one.gen are replicated in study.two.gen.\n\nQ.23. Are there any important differences between the results of the two studies?\nhint: Q.23. You can look at the estimates but you can also use the model prediction plotting code you used before, see example code in 2022-23-PSYC122-w18-how-to.R.\nhint: Q.23. – Let’s focus on comparing the study.one.gen and study.two.gen estimates for the effect of vocabulary knowledge in both models: we can plot model predictions for comparison:\n\nFirst: fit the models – using different names for the different models:\n\nmodel.one &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.one.gen)\nsummary(model.one)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40322 -0.05349  0.01152  0.07128  0.18434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.302030   0.091257   3.310  0.00115 ** \nHLVA        0.017732   0.003923   4.521 1.17e-05 ***\nSHIPLEY     0.005363   0.002336   2.296  0.02291 *  \nFACTOR3     0.003355   0.001264   2.654  0.00872 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1033 on 165 degrees of freedom\nMultiple R-squared:  0.2474,    Adjusted R-squared:  0.2337 \nF-statistic: 18.08 on 3 and 165 DF,  p-value: 3.423e-10\n\nmodel.two &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.two.gen)\nsummary(model.two)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.two.gen)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.242746 -0.074188  0.003173  0.075361  0.211357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.146896   0.076325   1.925  0.05597 .  \nHLVA        0.017598   0.003589   4.904  2.2e-06 ***\nSHIPLEY     0.008397   0.001853   4.533  1.1e-05 ***\nFACTOR3     0.003087   0.001154   2.675  0.00822 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.097 on 168 degrees of freedom\nMultiple R-squared:  0.3636,    Adjusted R-squared:  0.3522 \nF-statistic: 31.99 on 3 and 168 DF,  p-value: &lt; 2.2e-16\n\n\nSecond, create prediction plots for the SHIPLEY effect for each model:\n\ndat.one &lt;- ggpredict(model.one, \"SHIPLEY\")\nplot.one &lt;- plot(dat.one) + labs(title = \"Study One\")\ndat.two &lt;- ggpredict(model.two, \"SHIPLEY\")\nplot.two &lt;- plot(dat.two) + labs(title = \"Study Two\")"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "In Week 17, we aim to further develop skills in visualizing and testing the associations between variables in psychological data.\nWe do this to learn how to answer research questions like:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\nThese kinds of research questions can be answered using methods like the linear model.\nWhen we do these analyses, we need to think about how we report the results:\n\nwe usually need to report information about the kind of model we specify;\nand we will need to report the nature of the association estimated in our model;\nwe usually need to decide, is the association significant?\ndoes the association reflect a positive or negative relationship between outcome and predictor?\nand is the association we see in our sample data relatively strong or weak?\n\nWe will consolidate and extend learning on data visualization:\n\nfocusing on how we edit ggplot() code to produce professional looking plots.\n\n\n\nI will format dataset names like this:\n\nstudy-one-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}.\n\n\n\nIn this activity, we use data from a 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-one-general-participants.csv\n\n\n\n\nTo begin, we set up our environment in R.\n\n\n\nrm(list=ls())                            \n\n\n\n\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nThe data file is called:\n\nstudy-one-general-participants.csv\n\nUse the read_csv() function to read the data file into R:\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")  \n\nRows: 169 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data file in, give the data object you create a distinct name e.g. study.one.gen.\n\n\n\nUse the summary() or head() functions to take a look.\n\nhead(study.one.gen)\n\n# A tibble: 6 × 12\n  participant_ID mean.acc mean.self study     AGE SHIPLEY  HLVA FACTOR3 QRITOTAL\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 studyone.1         0.49      7.96 studyo…    34      33     7      53       11\n2 studyone.10        0.85      7.28 studyo…    25      33     7      60       11\n3 studyone.100       0.82      7.36 studyo…    43      40     8      46       12\n4 studyone.101       0.94      7.88 studyo…    46      33    11      51       15\n5 studyone.102       0.58      6.96 studyo…    18      32     3      51       12\n6 studyone.103       0.84      7.88 studyo…    19      37    13      45       19\n# ℹ 3 more variables: GENDER &lt;chr&gt;, EDUCATION &lt;chr&gt;, ETHNICITY &lt;chr&gt;\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\nNotice that:\n\nhead() will give you the top few rows of any dataset you have read into R.\nsummary() will give you either descriptive statistics for variable columns classified as numeric or will tell you that columns in the dataset are not numeric.\n\nPay attention to what you see, for the numeric variables, in the information about minimum (Min.) and maximum (Max.) values. You will use this informatio later when you engage in data visualization.\n\n\n\n\n\n\n\n\n\n\n\n\nUse ggplot() with geom_histogram().\nWhen we create a plot, we take things step-by-step.\nHere’s an example you have seen before: run the lines of code and see the result in the Plots window in R-Studio.\n\nggplot(data = study.one.gen, aes(x = mean.acc)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThese are the steps, set out one at a time:\n\nggplot(...) you tell R you want to make a plot using the ggplot() function\nggplot(data = study.one.gen ...) you tell R you want to make a plot with the study.two.gen data\nggplot(..., aes(x = mean.acc)) you tell R that you want to make a plot with the variable mean.acc – here, you specify the aesthetic mapping, x = mean.acc\nggplot(...) + geom_histogram() you tell R you want to plot values of mean.acc as a histogram\n\nNotice that the code works the same whether we have the different bits of code on the same line or in a series of lines.\n\n\n\n\n\n\nWe are going to revise editing:\n\nThe appearance of the bars using binwidth;\nThe colour of the background using theme_bw();\nThe appearance of the labels using labs().\n\nWe proceed to revise:\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"mean accuracy\", y = \"frequency count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40) +\n  geom_vline(xintercept = mean(study.one.gen$SHIPLEY), colour = \"red\", size = 1.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\nIt is often useful to define the limits on the x-axis and on the y-axis:\n\nBecause we want to show the audience where the sample values are distributed compared to where they could be distributed, given the measure.\nThis makes sense e.g. if you want to see the relative age distribution of a sample compared to the population.\nBecause we want plots, shown side by side to be directly comparable.\nAnd because we want to give the audience a more accurate picture of the data.\n\nYou can see ggplot reference information on setting limits here:\nhttps://ggplot2.tidyverse.org/reference/lims.html\nNotice that we used the geom_vline() in:\ngeom_vline(xintercept = mean(study.one.gen$SHIPLEY), colour = \"red\", size = 1.5)\n\nto plot a vertical line at the location on the x-axis we define.\n\nWe do this in the steps:\n\ngeom_vline(...) draw a vertical line;\n...xintercept... draw the line so it hits the x-axis (intercepts the x-axis);\n...xintercept = mean(study.one.gen$SHIPLEY... defined by the mean of the variable mean(study.one.gen$SHIPLEY);\n...colour = \"red\", size = 1.5... make the line red and make the line one and a half times thicker than the default line thickness.\n\nSee ggplot() reference information for this new geom is here:\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun a chunk of code to make the plot.\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis plot shows:\n\nthe possible association between x-axis variable SHIPLEY and y-axis variable mean.acc.\n\nThe plot code moves through the following steps:\n\nggplot(...) make a plot;\nggplot(data = study.one.gen, ...) with the study.one.gen dataset;\nggplot(...aes(x = SHIPLEY, y = mean.acc)) using two aesthetic mappings\n\n\nx = SHIPLEY map SHIPLEY values to x-axis (horizontal, left to right) positions;\ny = mean.acc map mean.acc values to y-axis (vertical, bottom to top) positions;\n\n\ngeom_point() show the mappings as points.\n\n\n\n\n\n\n\n\nWe are going to revise making edits to:\n\nthe appearance of the points using alpha, size, shape, and colour;\nthe colour of the background using theme_bw();\nthe appearance of the labels using labs().\n\nWe are then going to try out some new moves:\n\nSet the x-axis and y-axis limits to the potential minimum-maximum ranges of the variables we plot.\n\nHere, “potential” references the fact that e.g. SHIPLEY scores can start at 0 (a person gets 0 responses correct) and has a maximum of 40 (a person could get all responses correct, for up to 40 items).\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\") \n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\")   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\")   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\")   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)\n\n\n\n\n\n\n\n\nNotice that I set the x-axis limits to the minimum (0) and maximum (40) possible values for the SHIPLEY variable. - I set the y-axis limits to the minimum (0) and maximum (1) possible values for the mean accuracy variable mean.acc.\n\n\n\nIt is generally a good idea to show the minimum value (the origin) for each variable. Not doing this i.e. showing a more narrow slice of the sample range is an easy way to exaggerate the strength of associations or to imply incorrectly the breadth in variation.\nYou can change the transparency (alpha), size, colour and shape of important parts of a plot.\n\nHere, we are changing the appearance of the points.\nBut you can also change the transparency (alpha), size, colour and shape of reference lines added to a plot.\n\nThe ggplot geom_point()reference information is here:\nhttps://ggplot2.tidyverse.org/reference/geom_point.html\n\nwhere you can see some examples of the edits we have done.\n\nSome useful information about shape options is here:\nhttp://www.cookbook-r.com/Graphs/Shapes_and_line_types/\nSome useful information about colourons is here:\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\n\n\n\n\n\n\n\n\n\nOne of our research questions is:\n\nWhat person attributes predict success in understanding?\n\n\n\n\n\n\nFit the model and get a summary of the model statistics.\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\nNotice that we do the linear model in the steps:\n\nmodel &lt;- lm(...) fit the model using lm(...), give the model a name – here, we call it “model”;\n...lm(mean.acc ~ SHIPLEY...) tell R you want a model of the outcome mean.acc predicted (~) by the predictor SHIPLEY;\n...data = study.one.gen) tell R that the variables you name in the formula live in the study.one.gen dataset;\nsummary(model) ask R for a summary of the model you called “model”.\n\nNotice: R has a general formula syntax: outcome ~ predictor or y ~ x\n\nand uses the same format across a number of different functions;\neach time, the left of the tilde symbol ~ is some output or outcome;\nand the right of the tilde ~ is some input or predictor or set of predictors.\n\n\n\n\nIf you look at the model summary you can answer the following questions.\n\nQ.5. What is the estimate for the coefficient of the effect of the predictor, SHIPLEY?\nA.5. 0.01050\nQ.6. Is the effect significant?\nA.6. It is significant, p &lt; .05\nQ.7. What are the values for t and p for the significance test for the coefficient?\nA.7. t = 4.585, p = 8.85e-06\nQ.8. What do you conclude is the answer to the research question, given the linear model results?\nA.8. The model slope estimate suggests that as SHIPLEY scores increase so mean.acc scores increase also.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe line will show the model predictions, given the model intercept and effect coefficient estimates.\nFirst fit a model and get a summary: model the relationship between mean.acc and SHIPLEY\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\nQ.9. What is the coefficient estimate for the intercept?\nA.9. 0.44914\nQ.10. What is the coefficient estimate for the slope of SHIPLEY?\nA.10. 0.01050\n\nSecond, use the geom_abline() function to draw the line:\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\")   +\n  geom_abline(intercept = 0.44914, slope = 0.01050, colour = \"red\", size = 1.5) +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)\n\n\n\n\n\n\n\n\nYou can see that what we do is:\n\nadd the geom_abline(...) function;\nand into that function code, we add information about the intercept and the slope which we take from the model summary.\n\nYou can see reference information here:\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html\nNote that we can get the prediction line drawn for us automatically, as:\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  # geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = `square`)   +\n  geom_smooth(method = \"lm\", colour = \"purple\", alpha = .2, size = 2.5, se = FALSE) +\n  geom_abline(intercept = 0.44914, slope = 0.01050, colour = \"red\", size = 1) +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)  \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nHere, I turned the points off by commenting them out, adding # to the start of the line.\nAnd I added geom_smooth(method = \"lm\", ...) to draw a prediction line.\nYou can compare the red prediction line I drew using the model estimates\nwith the purple line I used geom_smooth() to draw automatically to see that they are identical\n\nThis shows you something of what geom_smooth() does. It is very useful:\nhttps://ggplot2.tidyverse.org/reference/geom_smooth.html"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#naming-things",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#naming-things",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "I will format dataset names like this:\n\nstudy-one-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}."
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#the-data-we-will-be-using",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#the-data-we-will-be-using",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "In this activity, we use data from a 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-one-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-1-set-up",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-1-set-up",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "To begin, we set up our environment in R.\n\n\n\nrm(list=ls())                            \n\n\n\n\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-2-load-the-data",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-2-load-the-data",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "The data file is called:\n\nstudy-one-general-participants.csv\n\nUse the read_csv() function to read the data file into R:\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")  \n\nRows: 169 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data file in, give the data object you create a distinct name e.g. study.one.gen.\n\n\n\nUse the summary() or head() functions to take a look.\n\nhead(study.one.gen)\n\n# A tibble: 6 × 12\n  participant_ID mean.acc mean.self study     AGE SHIPLEY  HLVA FACTOR3 QRITOTAL\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 studyone.1         0.49      7.96 studyo…    34      33     7      53       11\n2 studyone.10        0.85      7.28 studyo…    25      33     7      60       11\n3 studyone.100       0.82      7.36 studyo…    43      40     8      46       12\n4 studyone.101       0.94      7.88 studyo…    46      33    11      51       15\n5 studyone.102       0.58      6.96 studyo…    18      32     3      51       12\n6 studyone.103       0.84      7.88 studyo…    19      37    13      45       19\n# ℹ 3 more variables: GENDER &lt;chr&gt;, EDUCATION &lt;chr&gt;, ETHNICITY &lt;chr&gt;\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\nNotice that:\n\nhead() will give you the top few rows of any dataset you have read into R.\nsummary() will give you either descriptive statistics for variable columns classified as numeric or will tell you that columns in the dataset are not numeric.\n\nPay attention to what you see, for the numeric variables, in the information about minimum (Min.) and maximum (Max.) values. You will use this informatio later when you engage in data visualization."
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-3-use-histograms-to-examine-the-distributions-of-variables",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-3-use-histograms-to-examine-the-distributions-of-variables",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "Use ggplot() with geom_histogram().\nWhen we create a plot, we take things step-by-step.\nHere’s an example you have seen before: run the lines of code and see the result in the Plots window in R-Studio.\n\nggplot(data = study.one.gen, aes(x = mean.acc)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThese are the steps, set out one at a time:\n\nggplot(...) you tell R you want to make a plot using the ggplot() function\nggplot(data = study.one.gen ...) you tell R you want to make a plot with the study.two.gen data\nggplot(..., aes(x = mean.acc)) you tell R that you want to make a plot with the variable mean.acc – here, you specify the aesthetic mapping, x = mean.acc\nggplot(...) + geom_histogram() you tell R you want to plot values of mean.acc as a histogram\n\nNotice that the code works the same whether we have the different bits of code on the same line or in a series of lines.\n\n\n\n\n\n\nWe are going to revise editing:\n\nThe appearance of the bars using binwidth;\nThe colour of the background using theme_bw();\nThe appearance of the labels using labs().\n\nWe proceed to revise:\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"mean accuracy\", y = \"frequency count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40) +\n  geom_vline(xintercept = mean(study.one.gen$SHIPLEY), colour = \"red\", size = 1.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\nIt is often useful to define the limits on the x-axis and on the y-axis:\n\nBecause we want to show the audience where the sample values are distributed compared to where they could be distributed, given the measure.\nThis makes sense e.g. if you want to see the relative age distribution of a sample compared to the population.\nBecause we want plots, shown side by side to be directly comparable.\nAnd because we want to give the audience a more accurate picture of the data.\n\nYou can see ggplot reference information on setting limits here:\nhttps://ggplot2.tidyverse.org/reference/lims.html\nNotice that we used the geom_vline() in:\ngeom_vline(xintercept = mean(study.one.gen$SHIPLEY), colour = \"red\", size = 1.5)\n\nto plot a vertical line at the location on the x-axis we define.\n\nWe do this in the steps:\n\ngeom_vline(...) draw a vertical line;\n...xintercept... draw the line so it hits the x-axis (intercepts the x-axis);\n...xintercept = mean(study.one.gen$SHIPLEY... defined by the mean of the variable mean(study.one.gen$SHIPLEY);\n...colour = \"red\", size = 1.5... make the line red and make the line one and a half times thicker than the default line thickness.\n\nSee ggplot() reference information for this new geom is here:\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-4-now-draw-scatterplots-to-examine-associations-between-variables",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-4-now-draw-scatterplots-to-examine-associations-between-variables",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "Run a chunk of code to make the plot.\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis plot shows:\n\nthe possible association between x-axis variable SHIPLEY and y-axis variable mean.acc.\n\nThe plot code moves through the following steps:\n\nggplot(...) make a plot;\nggplot(data = study.one.gen, ...) with the study.one.gen dataset;\nggplot(...aes(x = SHIPLEY, y = mean.acc)) using two aesthetic mappings\n\n\nx = SHIPLEY map SHIPLEY values to x-axis (horizontal, left to right) positions;\ny = mean.acc map mean.acc values to y-axis (vertical, bottom to top) positions;\n\n\ngeom_point() show the mappings as points.\n\n\n\n\n\n\n\n\nWe are going to revise making edits to:\n\nthe appearance of the points using alpha, size, shape, and colour;\nthe colour of the background using theme_bw();\nthe appearance of the labels using labs().\n\nWe are then going to try out some new moves:\n\nSet the x-axis and y-axis limits to the potential minimum-maximum ranges of the variables we plot.\n\nHere, “potential” references the fact that e.g. SHIPLEY scores can start at 0 (a person gets 0 responses correct) and has a maximum of 40 (a person could get all responses correct, for up to 40 items).\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\") \n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\")   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\")   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\")   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)\n\n\n\n\n\n\n\n\nNotice that I set the x-axis limits to the minimum (0) and maximum (40) possible values for the SHIPLEY variable. - I set the y-axis limits to the minimum (0) and maximum (1) possible values for the mean accuracy variable mean.acc.\n\n\n\nIt is generally a good idea to show the minimum value (the origin) for each variable. Not doing this i.e. showing a more narrow slice of the sample range is an easy way to exaggerate the strength of associations or to imply incorrectly the breadth in variation.\nYou can change the transparency (alpha), size, colour and shape of important parts of a plot.\n\nHere, we are changing the appearance of the points.\nBut you can also change the transparency (alpha), size, colour and shape of reference lines added to a plot.\n\nThe ggplot geom_point()reference information is here:\nhttps://ggplot2.tidyverse.org/reference/geom_point.html\n\nwhere you can see some examples of the edits we have done.\n\nSome useful information about shape options is here:\nhttp://www.cookbook-r.com/Graphs/Shapes_and_line_types/\nSome useful information about colourons is here:\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/"
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-5-use-a-linear-model-to-to-answer-the-research-questions",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-5-use-a-linear-model-to-to-answer-the-research-questions",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "One of our research questions is:\n\nWhat person attributes predict success in understanding?\n\n\n\n\n\n\nFit the model and get a summary of the model statistics.\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\nNotice that we do the linear model in the steps:\n\nmodel &lt;- lm(...) fit the model using lm(...), give the model a name – here, we call it “model”;\n...lm(mean.acc ~ SHIPLEY...) tell R you want a model of the outcome mean.acc predicted (~) by the predictor SHIPLEY;\n...data = study.one.gen) tell R that the variables you name in the formula live in the study.one.gen dataset;\nsummary(model) ask R for a summary of the model you called “model”.\n\nNotice: R has a general formula syntax: outcome ~ predictor or y ~ x\n\nand uses the same format across a number of different functions;\neach time, the left of the tilde symbol ~ is some output or outcome;\nand the right of the tilde ~ is some input or predictor or set of predictors.\n\n\n\n\nIf you look at the model summary you can answer the following questions.\n\nQ.5. What is the estimate for the coefficient of the effect of the predictor, SHIPLEY?\nA.5. 0.01050\nQ.6. Is the effect significant?\nA.6. It is significant, p &lt; .05\nQ.7. What are the values for t and p for the significance test for the coefficient?\nA.7. t = 4.585, p = 8.85e-06\nQ.8. What do you conclude is the answer to the research question, given the linear model results?\nA.8. The model slope estimate suggests that as SHIPLEY scores increase so mean.acc scores increase also."
  },
  {
    "objectID": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-6-use-a-linear-model-to-generate-predictions",
    "href": "PSYC122/data/week17/2023-24-PSYC122-w17-how-to.html#step-6-use-a-linear-model-to-generate-predictions",
    "title": "2023-24-PSYC122-w17-how-to",
    "section": "",
    "text": "The line will show the model predictions, given the model intercept and effect coefficient estimates.\nFirst fit a model and get a summary: model the relationship between mean.acc and SHIPLEY\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\nQ.9. What is the coefficient estimate for the intercept?\nA.9. 0.44914\nQ.10. What is the coefficient estimate for the slope of SHIPLEY?\nA.10. 0.01050\n\nSecond, use the geom_abline() function to draw the line:\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = \"square\")   +\n  geom_abline(intercept = 0.44914, slope = 0.01050, colour = \"red\", size = 1.5) +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)\n\n\n\n\n\n\n\n\nYou can see that what we do is:\n\nadd the geom_abline(...) function;\nand into that function code, we add information about the intercept and the slope which we take from the model summary.\n\nYou can see reference information here:\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html\nNote that we can get the prediction line drawn for us automatically, as:\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  # geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = `square`)   +\n  geom_smooth(method = \"lm\", colour = \"purple\", alpha = .2, size = 2.5, se = FALSE) +\n  geom_abline(intercept = 0.44914, slope = 0.01050, colour = \"red\", size = 1) +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)  \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nHere, I turned the points off by commenting them out, adding # to the start of the line.\nAnd I added geom_smooth(method = \"lm\", ...) to draw a prediction line.\nYou can compare the red prediction line I drew using the model estimates\nwith the purple line I used geom_smooth() to draw automatically to see that they are identical\n\nThis shows you something of what geom_smooth() does. It is very useful:\nhttps://ggplot2.tidyverse.org/reference/geom_smooth.html"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "In Week 16, we aim to develop skills in visualizing and testing the associations between variables in psychological data.\nWe do this to learn how to answer research questions like:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\nThese kinds of research questions can be answered using methods like correlation and linear models.\nWe will consolidate and extend learning on data visualization:\n\nUse histograms to examine the distributions of variables;\nUse scatterplots to examine the relationships we may observe or predict.\n\n\n\nThis guide is designed to help you work out:\n\nhow to write or edit R code to do things like visualization or analysis;\nhow to identify key information for interpretation and reporting.\n\n\n\n\nI will format dataset names like this:\n\nstudy-one-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}.\n\n\n\nI will split .Rmd scripts by steps, tasks and questions:\n\ndifferent steps for different phases of the analysis workflow;\ndifferent tasks for different things you need to do;\ndifferent questions to examine different ideas or coding challenges\n\n\n\n\nIn this how-to guide, we use data from a 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-one-general-participants.csv\n\nNotice that this dataset is different from the data you use to complete the Week 16 lab activity tasks or questions.\n\n\n\nTo begin, we set up our environment in R.\n\n\n\nrm(list=ls())                            \n\n\n\n\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nThe data file is called:\n\nstudy-one-general-participants.csv\n\nUse the read_csv() function to read the data file into R:\n\nstudy.one &lt;- read_csv(\"study-one-general-participants.csv\")  \n\nRows: 169 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data file in, give the data object you create a distinct name e.g. study.one.\n\n\n\nUse the summary() or head() functions to take a look\n\nhead(study.one)\n\n# A tibble: 6 × 12\n  participant_ID mean.acc mean.self study     AGE SHIPLEY  HLVA FACTOR3 QRITOTAL\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 studyone.1         0.49      7.96 studyo…    34      33     7      53       11\n2 studyone.10        0.85      7.28 studyo…    25      33     7      60       11\n3 studyone.100       0.82      7.36 studyo…    43      40     8      46       12\n4 studyone.101       0.94      7.88 studyo…    46      33    11      51       15\n5 studyone.102       0.58      6.96 studyo…    18      32     3      51       12\n6 studyone.103       0.84      7.88 studyo…    19      37    13      45       19\n# ℹ 3 more variables: GENDER &lt;chr&gt;, EDUCATION &lt;chr&gt;, ETHNICITY &lt;chr&gt;\n\nsummary(study.one)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\nNote that head() will give you the top few rows of any dataset you have read into R.\n\nhead(...) is a function, and you put the name of the dataset inside the brackets to view it.\nsummary() will give you either descriptive statistics for variable columns classified as numeric or will tell you that columns in the dataset are not numeric\nsummary() is also a function and, again, you put the name of the dataset inside the brackets to view it\n\nUse the information in the summary view to answer the following questions.\n\nQ.1. What is the mean of mean.acc?\nA.1. 0.8163\nQ.2. What class is the variable study?\nA.2. character\nQ.3. – Does the summary indicate if any variable has missing values (NA)?\nQ.3. – No\n\n\n\n\n\n\n\nstudy.one$study &lt;- as.factor(study.one$study)\n\n\n\n\n\nQ.4. After you have done this, what information does summary() give you about the variable study?\n\n\nsummary(study.one)\n\n participant_ID        mean.acc        mean.self          study    \n Length:169         Min.   :0.3600   Min.   :3.440   studyone:169  \n Class :character   1st Qu.:0.7600   1st Qu.:6.080                 \n Mode  :character   Median :0.8400   Median :7.080                 \n                    Mean   :0.8163   Mean   :6.906                 \n                    3rd Qu.:0.9000   3rd Qu.:7.920                 \n                    Max.   :0.9900   Max.   :9.000                 \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\n\nA.4. We can see the number 169 beside the word studyone: this tells us that there are 169 observations, in the column, each one is a value: the word or character string studyone.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse ggplot() with geom_histogram().\nThe first time we do this, we take things step-by-step.\nHere’s an example: run the line of code and see the result in the Plots window in R-Studio.\n\nggplot(data = study.one, aes(x = mean.acc)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThese are the steps, set out one at a time:\n\nggplot(...) you tell R you want to make a plot using the ggplot() function\nggplot(data = study.one ...) you tell R you want to make a plot with the study.one data\nggplot(..., aes(x = mean.acc)) you tell R that you want to make a plot with the variable mean.acc\nhere, you specify the aesthetic mapping, x = mean.acc\nggplot(...) + geom_histogram() you tell R you want to plot values of mean.acc as a histogram\n\n\n\n\n\nQ.5. Did you get a message in the Console window in R-Studio: what does it say?\nA.5. The message says:\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nQ.6. What does binwidth refer to?\nhint: Q.6. You need to do a search online (e.g., using Google) with the keywords: “ggplot reference geom_histogram”\nA.6. If your search gets you to this page:\n\nhttps://ggplot2.tidyverse.org/reference/geom_histogram.html\n\nThen you will first see a technical definition “The width of the bins. Can be specified as a numeric value …”\nYou will second see examples of the use of the term e.g.\n\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.01)\n\nIf you run the example code, it will produce a plot. Do it.\n\nIt is a very good idea to get get some experience doing searches online for information about working with data analysis tasks.\n\nYou will find that there is a lot of useful information, written in different ways for people with different levels of expertise\n\nHere, we are interested in working with binwidth in geom_histogram().\n\nThe example gives us a hint at what we can do: we can change the appearance of the bars in the histogram.\nThe bars in the histogram represent how often we observe, in a dataset, a grouping of similar values in a variable.\nLarger (wider) bins count more different values as similar.\nSo increasing binwidth will give you fewer wider bars in a histogram. Check it out.\n\n\n\n\n\n\n\nNote that binwidth needs to take into account the scale of the x variable:\n\nIt won’t work for mean.acc if you make binwidth = 100.\nThe number will need to be something between 0 and 1 (the min and max for mean.acc).\nTry a few different numbers, like this:\n\n\nggplot(data = study.one, aes(x = mean.acc)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = mean.acc)) + geom_histogram(binwidth = 0.1)\n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = mean.acc)) + geom_histogram(binwidth = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we break the plot code into steps, it will make it easier to read, and it will make it easier to add edits e.g.\n\nggplot(data = study.one, aes(x = mean.acc)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIf you run the selection of both lines, you get the code to work.\nHere, we are going to edit:\n\nThe appearance of the bars using binwidth;\nThe colour of the background using theme_bw();\nThe appearance of the labels using labs().\n\nTake a look at how the code changes and how the plot changes with each edit to the code.\n\nThe appearance of the bars using binwidth\n\n\nggplot(data = study.one, aes(x = mean.acc)) + \n  geom_histogram(binwidth = 0.2)\n\n\n\n\n\n\n\n\n\nThe colour of the background using theme_bw()\n\n\nggplot(data = study.one, aes(x = mean.acc)) + \n  geom_histogram(binwidth = 0.2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe appearance of the labels using labs()\n\n\nggplot(data = study.one, aes(x = mean.acc)) + \n  geom_histogram(binwidth = 0.2) +\n  theme_bw() +\n  labs(x = \"mean accuracy\", y = \"frequency count\")\n\n\n\n\n\n\n\n\nNotice how each line – except the last one – ends in a +\n\nWhat we are doing is telling R we want this + this + this …\nEach line then adds an extra step.\n\n\n\n\n\nYou can break this code by not adding a + at the end of each bit (except the last line) Try it.\nggplot(data = study.one, aes(x = mean.acc))\n  geom_histogram(binwidth = 0.2) +\n  theme_bw() +\n  labs(x = \"mean accuracy\", y = \"frequency\")\n\nQ.7. You will get an error message in the console window, orange and grey, if you omit a + like in the example: what is the error message?\nA.7. The message will read:\n\nError: Cannot add ggproto objects together. Did you forget to add this object to a ggplot object?\n\nRun `rlang::last_error()` to see where the error occurred.  \n\nQ.8. Copy and then paste the error message into Google: click on one of the results: do you see any useful information?\nA.8. An explanation of the problem is shown e.g. \n\nhttps://community.rstudio.com/t/could-anybody-help-me-cannot-add-ggproto-objects-together/11271\nR will give you hints – often in red – in error messages if something has gone wrong. This can be a bit frightening but usually you can fix a problem by:\n\nUsing trial and error: try changing things to see what happens;\nCopying the error message into an online search: usually, that will get you to a blog or discussion that is helpful.\n\nThe R community has many millions of people in it. The huge value of the community means that for every problem you encounter, someone else has already met that problem, solved it, and posted a blog online about how to fix it\n\n\n\n\n\n\n\nYou have seen these code moves before, in previous classes: we are strengthening skills by practising coding in different contexts\n\n\n\n\n\nggplot(data = study.one, aes(x = mean.self, y = mean.acc)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nThis plot shows the possible association between x-axis variable mean.self and y-axis variable mean.acc\n\nThe plot code moves through the following steps:\n\nggplot(...) make a plot\nggplot(data = study.one, ...) with the study.one dataset\nggplot(...aes(x = mean.self, y = mean.acc)) using two aesthetic mappings\n\n\nx = mean.self map mean.self values to x-axis (horizontal, left to right) positions\ny = mean.acc- map mean.acc values to y-axis (vertical, bottom to top) positions\n\n\ngeom_point() show the mappings as points.\n\n\n\n\n\n\n\n\nggplot(data = study.one, aes(y = mean.self, x = mean.acc)) +\n    geom_point()  \n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = AGE, y = mean.self)) +\n  geom_point()  \n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = SHIPLEY, y = mean.self)) +\n  geom_point()  \n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point()  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe appearance of the points using alpha and size;\nThe colour of the background using theme_bw();\nThe appearance of the labels using labs().\nThe appearance of the points using alpha and size\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2) \n\n\n\n\n\n\n\n\n\nThe colour of the background using theme_bw()\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2)  +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe appearance of the labels using labs()\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2)  +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean self rated accuracy\")\n\n\n\n\n\n\n\n\nThe arguments alpha and size can change the appearance of most geometric objects (geoms) in ggplot:\n\nIn the code example, here, we vary the alpha number to change how opaque or transparent the points are\nAnd we vary the size number to vary the size of the points.\n\n\n\n\n\n\n\n\nQ.9. – Can you find the ggplot reference page?\nhint: Q.9 – Do a search with the keywords “ggplot reference geom_point”\nA.9. – It is here:\n\nhttps://ggplot2.tidyverse.org/reference/geom_point.html\n\nQ.10. – Can you change the colour of the points to a colour you like?\nhint: Q.10. – Useful information on colour can be found here:\n\nhttps://r-graphics.org/recipe-colors-setting\nSee also:\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\nA.10. – Here is how you do it:\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"hotpink\")  +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean self rated accuracy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrevision: make sure you are confident about doing these things\n\nOne of our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe can answer this question by examining whether mean self-rated accuracy of understanding correlates with mean accuracy of understanding. The logic is that if we can accurately rate our own understanding (from bad to good) then that rating should be associated – should be correlated with how accurately we can actually respond to questions that test that understanding.\n\n\n\n\n\ncor.test(study.one$mean.acc, study.one$mean.self, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.one$mean.acc and study.one$mean.self\nt = 7.1936, df = 167, p-value = 2.026e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3619961 0.5937425\nsample estimates:\n      cor \n0.4863771 \n\n\n\nQ.11. – What is r, the correlation coefficient?\nA.11. – r = 0.4863771\nQ.12. – Is the correlation significant?\nA.12. – r is significant\nQ.13. – What are the values for t and p for the significance test for the correlation?\nA.13. – t = 7.1936, p = 2.026e-11\nQ.14. – What do you conclude, given the correlation results?\nhint: Q.6 – Review the scatterplot you drew earlier to examine the shape of the association between these variables\nA.14. – mean.acc and mean.self are positively correlated suggesting that as mean.acc scores increase so also do mean.self scores"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#this-how-to-guide",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#this-how-to-guide",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "This guide is designed to help you work out:\n\nhow to write or edit R code to do things like visualization or analysis;\nhow to identify key information for interpretation and reporting."
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#naming-things",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#naming-things",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "I will format dataset names like this:\n\nstudy-one-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}."
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#we-will-take-things-step-by-step",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#we-will-take-things-step-by-step",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "I will split .Rmd scripts by steps, tasks and questions:\n\ndifferent steps for different phases of the analysis workflow;\ndifferent tasks for different things you need to do;\ndifferent questions to examine different ideas or coding challenges"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#the-data-we-will-be-using",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#the-data-we-will-be-using",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "In this how-to guide, we use data from a 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-one-general-participants.csv\n\nNotice that this dataset is different from the data you use to complete the Week 16 lab activity tasks or questions."
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#step-1-set-up",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#step-1-set-up",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "To begin, we set up our environment in R.\n\n\n\nrm(list=ls())                            \n\n\n\n\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#step-2-load-the-data",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#step-2-load-the-data",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "The data file is called:\n\nstudy-one-general-participants.csv\n\nUse the read_csv() function to read the data file into R:\n\nstudy.one &lt;- read_csv(\"study-one-general-participants.csv\")  \n\nRows: 169 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data file in, give the data object you create a distinct name e.g. study.one.\n\n\n\nUse the summary() or head() functions to take a look\n\nhead(study.one)\n\n# A tibble: 6 × 12\n  participant_ID mean.acc mean.self study     AGE SHIPLEY  HLVA FACTOR3 QRITOTAL\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 studyone.1         0.49      7.96 studyo…    34      33     7      53       11\n2 studyone.10        0.85      7.28 studyo…    25      33     7      60       11\n3 studyone.100       0.82      7.36 studyo…    43      40     8      46       12\n4 studyone.101       0.94      7.88 studyo…    46      33    11      51       15\n5 studyone.102       0.58      6.96 studyo…    18      32     3      51       12\n6 studyone.103       0.84      7.88 studyo…    19      37    13      45       19\n# ℹ 3 more variables: GENDER &lt;chr&gt;, EDUCATION &lt;chr&gt;, ETHNICITY &lt;chr&gt;\n\nsummary(study.one)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\nNote that head() will give you the top few rows of any dataset you have read into R.\n\nhead(...) is a function, and you put the name of the dataset inside the brackets to view it.\nsummary() will give you either descriptive statistics for variable columns classified as numeric or will tell you that columns in the dataset are not numeric\nsummary() is also a function and, again, you put the name of the dataset inside the brackets to view it\n\nUse the information in the summary view to answer the following questions.\n\nQ.1. What is the mean of mean.acc?\nA.1. 0.8163\nQ.2. What class is the variable study?\nA.2. character\nQ.3. – Does the summary indicate if any variable has missing values (NA)?\nQ.3. – No\n\n\n\n\n\n\n\nstudy.one$study &lt;- as.factor(study.one$study)\n\n\n\n\n\nQ.4. After you have done this, what information does summary() give you about the variable study?\n\n\nsummary(study.one)\n\n participant_ID        mean.acc        mean.self          study    \n Length:169         Min.   :0.3600   Min.   :3.440   studyone:169  \n Class :character   1st Qu.:0.7600   1st Qu.:6.080                 \n Mode  :character   Median :0.8400   Median :7.080                 \n                    Mean   :0.8163   Mean   :6.906                 \n                    3rd Qu.:0.9000   3rd Qu.:7.920                 \n                    Max.   :0.9900   Max.   :9.000                 \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\n\nA.4. We can see the number 169 beside the word studyone: this tells us that there are 169 observations, in the column, each one is a value: the word or character string studyone."
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#revise-consolidate-what-you-know",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#revise-consolidate-what-you-know",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "Use ggplot() with geom_histogram().\nThe first time we do this, we take things step-by-step.\nHere’s an example: run the line of code and see the result in the Plots window in R-Studio.\n\nggplot(data = study.one, aes(x = mean.acc)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThese are the steps, set out one at a time:\n\nggplot(...) you tell R you want to make a plot using the ggplot() function\nggplot(data = study.one ...) you tell R you want to make a plot with the study.one data\nggplot(..., aes(x = mean.acc)) you tell R that you want to make a plot with the variable mean.acc\nhere, you specify the aesthetic mapping, x = mean.acc\nggplot(...) + geom_histogram() you tell R you want to plot values of mean.acc as a histogram\n\n\n\n\n\nQ.5. Did you get a message in the Console window in R-Studio: what does it say?\nA.5. The message says:\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nQ.6. What does binwidth refer to?\nhint: Q.6. You need to do a search online (e.g., using Google) with the keywords: “ggplot reference geom_histogram”\nA.6. If your search gets you to this page:\n\nhttps://ggplot2.tidyverse.org/reference/geom_histogram.html\n\nThen you will first see a technical definition “The width of the bins. Can be specified as a numeric value …”\nYou will second see examples of the use of the term e.g.\n\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.01)\n\nIf you run the example code, it will produce a plot. Do it.\n\nIt is a very good idea to get get some experience doing searches online for information about working with data analysis tasks.\n\nYou will find that there is a lot of useful information, written in different ways for people with different levels of expertise\n\nHere, we are interested in working with binwidth in geom_histogram().\n\nThe example gives us a hint at what we can do: we can change the appearance of the bars in the histogram.\nThe bars in the histogram represent how often we observe, in a dataset, a grouping of similar values in a variable.\nLarger (wider) bins count more different values as similar.\nSo increasing binwidth will give you fewer wider bars in a histogram. Check it out.\n\n\n\n\n\n\n\nNote that binwidth needs to take into account the scale of the x variable:\n\nIt won’t work for mean.acc if you make binwidth = 100.\nThe number will need to be something between 0 and 1 (the min and max for mean.acc).\nTry a few different numbers, like this:\n\n\nggplot(data = study.one, aes(x = mean.acc)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = mean.acc)) + geom_histogram(binwidth = 0.1)\n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = mean.acc)) + geom_histogram(binwidth = 0.2)"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#extend-make-some-new-moves",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#extend-make-some-new-moves",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "If we break the plot code into steps, it will make it easier to read, and it will make it easier to add edits e.g.\n\nggplot(data = study.one, aes(x = mean.acc)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIf you run the selection of both lines, you get the code to work.\nHere, we are going to edit:\n\nThe appearance of the bars using binwidth;\nThe colour of the background using theme_bw();\nThe appearance of the labels using labs().\n\nTake a look at how the code changes and how the plot changes with each edit to the code.\n\nThe appearance of the bars using binwidth\n\n\nggplot(data = study.one, aes(x = mean.acc)) + \n  geom_histogram(binwidth = 0.2)\n\n\n\n\n\n\n\n\n\nThe colour of the background using theme_bw()\n\n\nggplot(data = study.one, aes(x = mean.acc)) + \n  geom_histogram(binwidth = 0.2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe appearance of the labels using labs()\n\n\nggplot(data = study.one, aes(x = mean.acc)) + \n  geom_histogram(binwidth = 0.2) +\n  theme_bw() +\n  labs(x = \"mean accuracy\", y = \"frequency count\")\n\n\n\n\n\n\n\n\nNotice how each line – except the last one – ends in a +\n\nWhat we are doing is telling R we want this + this + this …\nEach line then adds an extra step.\n\n\n\n\n\nYou can break this code by not adding a + at the end of each bit (except the last line) Try it.\nggplot(data = study.one, aes(x = mean.acc))\n  geom_histogram(binwidth = 0.2) +\n  theme_bw() +\n  labs(x = \"mean accuracy\", y = \"frequency\")\n\nQ.7. You will get an error message in the console window, orange and grey, if you omit a + like in the example: what is the error message?\nA.7. The message will read:\n\nError: Cannot add ggproto objects together. Did you forget to add this object to a ggplot object?\n\nRun `rlang::last_error()` to see where the error occurred.  \n\nQ.8. Copy and then paste the error message into Google: click on one of the results: do you see any useful information?\nA.8. An explanation of the problem is shown e.g. \n\nhttps://community.rstudio.com/t/could-anybody-help-me-cannot-add-ggproto-objects-together/11271\nR will give you hints – often in red – in error messages if something has gone wrong. This can be a bit frightening but usually you can fix a problem by:\n\nUsing trial and error: try changing things to see what happens;\nCopying the error message into an online search: usually, that will get you to a blog or discussion that is helpful.\n\nThe R community has many millions of people in it. The huge value of the community means that for every problem you encounter, someone else has already met that problem, solved it, and posted a blog online about how to fix it"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#revision-make-sure-you-are-confident-about-doing-these-things",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#revision-make-sure-you-are-confident-about-doing-these-things",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "You have seen these code moves before, in previous classes: we are strengthening skills by practising coding in different contexts\n\n\n\n\n\nggplot(data = study.one, aes(x = mean.self, y = mean.acc)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nThis plot shows the possible association between x-axis variable mean.self and y-axis variable mean.acc\n\nThe plot code moves through the following steps:\n\nggplot(...) make a plot\nggplot(data = study.one, ...) with the study.one dataset\nggplot(...aes(x = mean.self, y = mean.acc)) using two aesthetic mappings\n\n\nx = mean.self map mean.self values to x-axis (horizontal, left to right) positions\ny = mean.acc- map mean.acc values to y-axis (vertical, bottom to top) positions\n\n\ngeom_point() show the mappings as points.\n\n\n\n\n\n\n\n\nggplot(data = study.one, aes(y = mean.self, x = mean.acc)) +\n    geom_point()  \n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = AGE, y = mean.self)) +\n  geom_point()  \n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = SHIPLEY, y = mean.self)) +\n  geom_point()  \n\n\n\n\n\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point()  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe appearance of the points using alpha and size;\nThe colour of the background using theme_bw();\nThe appearance of the labels using labs().\nThe appearance of the points using alpha and size\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2) \n\n\n\n\n\n\n\n\n\nThe colour of the background using theme_bw()\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2)  +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe appearance of the labels using labs()\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2)  +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean self rated accuracy\")\n\n\n\n\n\n\n\n\nThe arguments alpha and size can change the appearance of most geometric objects (geoms) in ggplot:\n\nIn the code example, here, we vary the alpha number to change how opaque or transparent the points are\nAnd we vary the size number to vary the size of the points.\n\n\n\n\n\n\n\n\nQ.9. – Can you find the ggplot reference page?\nhint: Q.9 – Do a search with the keywords “ggplot reference geom_point”\nA.9. – It is here:\n\nhttps://ggplot2.tidyverse.org/reference/geom_point.html\n\nQ.10. – Can you change the colour of the points to a colour you like?\nhint: Q.10. – Useful information on colour can be found here:\n\nhttps://r-graphics.org/recipe-colors-setting\nSee also:\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\nA.10. – Here is how you do it:\n\n\nggplot(data = study.one, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"hotpink\")  +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean self rated accuracy\")"
  },
  {
    "objectID": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#step-6-use-correlation-to-to-answer-the-research-questions",
    "href": "PSYC122/data/week16/2023-24-PSYC122-w16-how-to.html#step-6-use-correlation-to-to-answer-the-research-questions",
    "title": "2023-24-PSYC122-w16-how-to",
    "section": "",
    "text": "revision: make sure you are confident about doing these things\n\nOne of our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe can answer this question by examining whether mean self-rated accuracy of understanding correlates with mean accuracy of understanding. The logic is that if we can accurately rate our own understanding (from bad to good) then that rating should be associated – should be correlated with how accurately we can actually respond to questions that test that understanding.\n\n\n\n\n\ncor.test(study.one$mean.acc, study.one$mean.self, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.one$mean.acc and study.one$mean.self\nt = 7.1936, df = 167, p-value = 2.026e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3619961 0.5937425\nsample estimates:\n      cor \n0.4863771 \n\n\n\nQ.11. – What is r, the correlation coefficient?\nA.11. – r = 0.4863771\nQ.12. – Is the correlation significant?\nA.12. – r is significant\nQ.13. – What are the values for t and p for the significance test for the correlation?\nA.13. – t = 7.1936, p = 2.026e-11\nQ.14. – What do you conclude, given the correlation results?\nhint: Q.6 – Review the scatterplot you drew earlier to examine the shape of the association between these variables\nA.14. – mean.acc and mean.self are positively correlated suggesting that as mean.acc scores increase so also do mean.self scores"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "",
    "text": "In Week 18, we aim to further develop skills in working with the linear model.\nWe do this to learn how to answer research questions like:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\nThese kinds of research questions can be answered using methods like the linear model.\nWhen we do these analyses, we need to think about how we report the results:\n\nwe usually need to report information about the kind of model we specify;\nand we will need to report the nature of the association estimated in our model.\n\nWe usually need to decide:\n\nis the association significant?\ndoes the association reflect a positive or negative relationship between outcome and predictor?\nand is the association we see in our sample data relatively strong or weak?\n\nWe will consolidate and extend learning on data visualization:\n\nfocusing on how we edit ggplot() code to produce professional looking plots.\n\n\n\nI will format dataset names like this:\n\nstudy-one-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}.\n\n\n\nIn this how-to guide, we use data from a 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-one-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#naming-things",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#naming-things",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "",
    "text": "I will format dataset names like this:\n\nstudy-one-general-participants.csv\n\nI will also format variable (data column) names like this: variable\nI will also format value or other data object (e.g. cell value) names like this: studyone\nI will format functions and library names like this: e.g. function ggplot() or e.g. library {tidyverse}."
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#the-data-we-will-be-using",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#the-data-we-will-be-using",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "",
    "text": "In this how-to guide, we use data from a 2020 study of the response of adults from a UK national sample to written health information:\n\nstudy-one-general-participants.csv"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-1-set-up",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-1-set-up",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "Step 1: Set-up",
    "text": "Step 1: Set-up\nTo begin, we set up our environment in R.\n\nTask 1 – Run code to empty the R environment\n\nrm(list=ls())                            \n\n\n\nTask 2 – Run code to load relevant libraries\n\nlibrary(\"ggeffects\")\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-2-load-the-data",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-2-load-the-data",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "Step 2: Load the data",
    "text": "Step 2: Load the data\n\nTask 3 – Read in the data file we will be using\nThe data file is called:\n\nstudy-one-general-participants.csv\n\nUse the read_csv() function to read the data file into R:\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")\n\nRows: 169 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you read the data file in, give the data object you create a distinct name e.g. study.one.gen.\n\n\nTask 4 – Inspect the data file\nUse the summary() or head() functions to take a look.\n\nhead(study.one.gen)\n\n# A tibble: 6 × 12\n  participant_ID mean.acc mean.self study     AGE SHIPLEY  HLVA FACTOR3 QRITOTAL\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 studyone.1         0.49      7.96 studyo…    34      33     7      53       11\n2 studyone.10        0.85      7.28 studyo…    25      33     7      60       11\n3 studyone.100       0.82      7.36 studyo…    43      40     8      46       12\n4 studyone.101       0.94      7.88 studyo…    46      33    11      51       15\n5 studyone.102       0.58      6.96 studyo…    18      32     3      51       12\n6 studyone.103       0.84      7.88 studyo…    19      37    13      45       19\n# ℹ 3 more variables: GENDER &lt;chr&gt;, EDUCATION &lt;chr&gt;, ETHNICITY &lt;chr&gt;\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\nEven though you have done this before, you will want to do it again, here, and pay particular attention to:\n\nsummary information about the numeric variables;\nsummary information about variables of class: character."
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-3-use-a-linear-model-to-to-answer-the-research-questions-one-predictor",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-3-use-a-linear-model-to-to-answer-the-research-questions-one-predictor",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "Step 3: Use a linear model to to answer the research questions – one predictor",
    "text": "Step 3: Use a linear model to to answer the research questions – one predictor\n\nRevise: Practice to strengthen skills\nWe start by revising how to use lm() with one predictor.\nOne of our research questions is:\n\nWhat person attributes predict success in understanding?\n\n\n\nTask 5 – Examine the relation between outcome mean accuracy (mean.acc) and health literacy (HLVA)\n\n\nHint: Task 5\nWe can use lm() to estimate whether variation in health literacy (HLVA) predicts outcome mean accuracy (mean.acc) of understanding.\n\nmodel &lt;- lm(mean.acc ~ HLVA, data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40848 -0.05304  0.01880  0.07608  0.19968 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.61399    0.03387  18.128  &lt; 2e-16 ***\nHLVA         0.02272    0.00369   6.158 5.31e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1068 on 167 degrees of freedom\nMultiple R-squared:  0.1851,    Adjusted R-squared:  0.1802 \nF-statistic: 37.92 on 1 and 167 DF,  p-value: 5.307e-09\n\n\nIn R analysis code, we write method(outcome ~ predictor) so:\nlm(mean.acc ~ HLVA, data = study.one.gen)\n\ngets us an analysis of whether or how HLVA predicts variation in outcome mean.acc.\n\nIf you look at the model summary you can answer the following questions.\n\nQuestions: Task 5\n\nQ.1. What is the estimate for the coefficient of the effect of the predictor, HLVA?\nA.1. 0.02272\nQ.2. Is the effect significant?\nA.2. It is significant, p &lt; .05\nQ.3. What are the values for t and p for the significance test for the coefficient?\nA.3. t = 6.158, p = 5.31e-09\nQ.4. What do you conclude is the answer to the research question, given the linear model results?\nA.4. The model slope estimate suggests that as HLVA scores increase so also do mean.acc scores\nQ.5. What is the F-statistic for the regression? Report F, DF and the p-value.\nA.5. F-statistic: 37.92 on 1 and 167 DF, p-value: 5.307e-09\nQ.6. Is the regression significant?\nA.6. Yes: the regression is significant.\nQ.7. What is the Adjusted R-squared?\nA.7. Adjusted R-squared: 0.1802\nQ.8. Explain in words what this R-squared value indicates?\nA.8. The R-squared suggests that 18% of outcome variance can be explained by the model"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-4-use-a-linear-model-to-to-answer-the-research-questions-multiple-predictors",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-4-use-a-linear-model-to-to-answer-the-research-questions-multiple-predictors",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "Step 4: Use a linear model to to answer the research questions – multiple predictors",
    "text": "Step 4: Use a linear model to to answer the research questions – multiple predictors\n\nIntroduce: make some new moves\n\n\nTask 6 – Examine the relation between outcome mean accuracy (mean.acc) and multiple predictors\nHere, the predictors will include:\n\nhealth literacy (HLVA);\nvocabulary (SHIPLEY);\nreading strategy (FACTOR3).\n\n\nHint: Task 6\nWe use lm(), as before, but now specify each variable listed here by variable name\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.one.gen)\n\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40322 -0.05349  0.01152  0.07128  0.18434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.302030   0.091257   3.310  0.00115 ** \nHLVA        0.017732   0.003923   4.521 1.17e-05 ***\nSHIPLEY     0.005363   0.002336   2.296  0.02291 *  \nFACTOR3     0.003355   0.001264   2.654  0.00872 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1033 on 165 degrees of freedom\nMultiple R-squared:  0.2474,    Adjusted R-squared:  0.2337 \nF-statistic: 18.08 on 3 and 165 DF,  p-value: 3.423e-10\n\n\nNotice that we do the linear model in the steps:\n\nmodel &lt;- lm(...) fit the model using lm(...), giving the model a name; here, we call it model;\n...lm(mean.acc ~ HLVA...) tell R you want a model of the outcome mean.acc predicted (~) by the predictors listed, HLVA, SHIPLEY, and FACTOR3.\n...data = study.one.gen) tell R that the variables you name in the formula are in the study.one.gen dataset.\nsummary(model) ask R for a summary of the model you called model.\n\nNotice: that we use the variable names as they appear in the dataset, and that each predictor variable is separated from the next by a plus (+) sign.\nNotice: R has a general formula syntax: outcome ~ predictor or y ~ x and uses the same format across a number of different analysis functions.\n\nEach time, the left of the tilde symbol ~ is some output or outcome and the right of the tilde ~ is some input or predictor or set of predictors.\n\n\n\nQuestions: Task 6\nIf you look at the model summary you can answer the following questions\nQ.9. What is the estimate for the coefficient of the effect of the predictor HLVA in this model?\nA.9. 0.017732\nQ.10. Is the effect significant?\nA.10. It is significant, p &lt; .05\nQ.11. What are the values for t and p for the significance test for the coefficient?\nA.11. t = 4.521, p = 1.17e-05\nQ.12. What do you conclude is the answer to the research question, given the linear model results?\nA.12. The model slope estimate 0.017732 suggests that as HLVA scores increase so also do mean.acc scores.\nQ.13. How is the coefficient estimate for the HLVA slope similar or different, comparing this model with multiple predictors to the previous model with one predictor?\nA.13. It can be seen that the HLVA estimate in the two models is different in that it is a bit smaller in the model with multiple predictors compared to the model with one predictor. The HLVA estimate is similar in that it remains positive, it is about the same size.\nNotice that:-\n\nThe estimate of the coefficient of any one predictor can be expected to vary depending on the presence of other predictors.\nThis is one reason why we need to be transparent about why we choose to use the predictors we include in our model.\n\nQ.14. Can you report the estimated effect of SHIPLEY (the measure of vocabulary) using the kind of language you are shown in lecture week 18?\nA.14. The answer to the question can be written like this:\n\nThe effect of vocabulary knowledge (SHIPLEY) on mean accuracy of understanding is significant (estimate = 0.005, t = 2.296, p &lt; .001) indicating that increasing vocabulary knowledge is associated with increasing accuracy of understanding.\n\nQ.15. Can you report the model and the model fit statistics?\nA.15. The answer to the question can be written like this:\n\nWe fitted a linear model with mean comprehension accuracy as the outcome and health literacy (HLVA), reading strategy (FACTOR3), and vocabulary (SHIPLEY) as predictors. The model is significant overall, with F(3, 165) = 18.08, p&lt; .001, and explains 23% of variance (adjusted R2 = 0.23)."
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-5-plot-predictions-from-linear-models-with-multiple-predictors",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-5-plot-predictions-from-linear-models-with-multiple-predictors",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "Step 5: Plot predictions from linear models with multiple predictors",
    "text": "Step 5: Plot predictions from linear models with multiple predictors\n\nIntroduce: make some new moves\n\n\nTask 7 – Plot linear model predictions for one of the predictors\n\nHint: Task 7\nPreviously, we used geom_abline(), specifying intercept and slope estimates, to draw model predictions.\nHere, we use functions that are very helpful when we need to plot model predictions, for models where we have multiple predictors\nWe do this in four steps:\n\nWe first fit a linear model of the outcome, given our predictors.\nWe save information about the model.\nWe use the ggpredict() function from the {ggeffects} library to take the information about the model and create a set of predictions we can use for plotting.\nWe plot the model prediction plots.\n\nThese steps proceed as follows:\n\nWe first fit a linear model of the outcome, given our predictors.\n\nLike this:\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3, data = study.one.gen)\n\nThe code involves these key bits:\n\nmodel &lt;- lm(...) we fit the model using lm(...), giving the model a name; here, we call it model.\n...lm(mean.acc ~ HLVA...) we tell R we want a model of the outcome mean.acc predicted (~) by the predictors HLVA, SHIPLEY, and FACTOR3.\n\nNotice: when we use lm() to fit the model, R creates a set of information about the model, including estimates.\nWe give that set of information a name model, and we use that name, next, to access that information in the plotting step.\n\nWe use the ggpredict() function from the {ggeffects} library to take the information about the model and create a set of predictions we can use for plotting.\n\n\ndat &lt;- ggpredict(model, \"HLVA\")\n\nNotice:\n\ndat &lt;- ggpredict(...) asks R to create a set of predictions, and we give that set of predictions a name dat.\n... ggpredict(model, \"HLVA\") tells R what model information it should use (from model) and which predictor variable we need predictions for \"HLVA\".\n\n\nWe plot the model predictions with:\n\n\nplot(dat)\n\n\n\n\n\n\n\n\n\n\n\nTask 8 – Now produce plots that show the predictions for all the predictor variables in the model"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-6-now-draw-boxplots-to-examine-associations-between-variables",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-6-now-draw-boxplots-to-examine-associations-between-variables",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "Step 6: Now draw boxplots to examine associations between variables",
    "text": "Step 6: Now draw boxplots to examine associations between variables\n\nConsolidation: practice to strengthen skills\n\n\nTask 9 – Create boxplots to examine the association between a continuous numeric outcome variable like mean.acc and a categorical variable like ETHNICITY\nHere, we use geom_boxplot().\n\nHint: Task 9 – We can see where variables are not numeric using summary()\nThe boxplot can be produced using code like this:\n\nggplot(data = study.one.gen, aes(x = ETHNICITY, y = mean.acc)) +\n  geom_boxplot() +\n  theme_bw() +\n  labs(x = \"Ethnicity, ONS categories\", \n       y = \"Mean accuracy of understanding ('mean.acc')\") +\n  ylim(0, 1.1)\n\n\n\n\n\n\n\n\nThe plotting code works bit-by-bit, as described following.\n\nggplot(data = study.one.gen, aes(x = ETHNICITY, y = mean.acc)) defines two aesthetic mappings:\n\n\nx = ETHNICITY, the x variable has to be categorical or nominal, a factor like ETHNICITY with different levels.\ny = mean.acc, the y variable has to be numeric, a set of numbers like mean.acc with different values.\n\n\ngeom_boxplot() then uses that information about category (x = ...) and outcome (y = ...) to draw a box to represent the distribution of outcome scores for each group.\n\nNotice that when you draw the plot:\n\nThe middle line in each box represents the median outcome (here mean.acc) score for each group.\nThe shape of the box represents the distribution or spread of scores.\nThe top of the box represents the 75th percentile, what the score is for the people who are at the top 75% of the sample.\nThe bottom of the box represents the 25th percentile, what the score is for the people at the 25% level of outcomes for the sample.\n\nMore information about boxplots can be found here:\nhttps://ggplot2.tidyverse.org/reference/geom_boxplot.html\nHere is an edit of the plot to make it a bit more effective:\n\nggplot(data = study.one.gen, aes(x = ETHNICITY, y = mean.acc)) +\n  geom_jitter(alpha = .5) +\n  geom_boxplot(outlier.shape = NA, colour = \"red\", alpha = .1) +\n  theme_bw() +\n  labs(x = \"Ethnicity, ONS categories\", \n       y = \"Mean accuracy of understanding ('mean.acc')\") +\n  ylim(0, 1.1)\n\n\n\n\n\n\n\n\nThe plot shows:\n\nboxplots to indicate the average (median) and spread (percentiles) in mean.acc scores for each group;\nplus, with points, individual mean.acc scores for the people in each group.\n\n\n\nWhy are we learning how to do this?\nDrawing plots which show both summaries (like boxplots) and raw data (scores as points) is a common (advanced) professional visualization technique.\n\nIt is effective because these kinds of plots help you to see the pattern or trend and the nature of the underlying sample.\n\nNow you can use the plots to answer questions like the following\n\n\nQuestions: Task 9\nQ.16. What do you notice about the distribution of scores in different groups?\nA.16. The average accuracy of understanding appears to be similar between groups.\nQ.17. Does anything in the plots give you reason to question the nature of the participant sample?\nA.17. This is a leading question: there is plenty in the plots to cause concern.\n\nThe scatter of points shows that we have many more White participants in the sample than participants from other ethnicities.\nBecause we have very few people in the study from ethnic minority (often classed as BAME) groups, we might be concerned about whether the results from our models are representative of what you would see in these groups, or whether the results are representative of the wider population in general.\n\nQ.18. Can you use the ggplot() reference information – see the webpage link – to see how and why I made the code edits I did?\nA.18. You can see example code for each edit in the webpage.\nQ.19. Do you understand what geom_jitter() is doing? – and why I would use it?\nA.19. What the function does, and why I would use it can be found in the reference information webpage:\nhttps://ggplot2.tidyverse.org/reference/geom_jitter.html"
  },
  {
    "objectID": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-6-estimate-the-effects-of-factors-as-well-as-numeric-variables",
    "href": "PSYC122/data/week18/2023-24-PSYC122-w18-how-to.html#step-6-estimate-the-effects-of-factors-as-well-as-numeric-variables",
    "title": "2023-24-PSYC122-w18-how-to",
    "section": "Step 6: Estimate the effects of factors as well as numeric variables",
    "text": "Step 6: Estimate the effects of factors as well as numeric variables\n\nIntroduce: make some new moves\nWe have not yet included any categorical or nominal variables as predictors but we can, and should:\n\nlm() can cope with any kind of variable as a predictor.\n\n\n\nTask 10 – Fit a linear model including both numeric variables and categorical variables as predictors\n\nHint: Task 12\nWe can inspect the data to check what variables are categorical or nominal variables – factors – using summary().\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\nNotice that R shows categorical variables in the summary as having: Class: character.\n\nQ.20. Can you report the estimated effect of ETHNICITY: differences in outcome for people in different self-reported ethnicity groups?\nHint: Q.20. Include the factor ETHNICITY as a predictor:\n\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + ETHNICITY, \n            data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3 + ETHNICITY, \n    data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40261 -0.05322  0.01168  0.07124  0.18391 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.302764   0.095806   3.160  0.00188 ** \nHLVA            0.017391   0.003980   4.370 2.22e-05 ***\nSHIPLEY         0.005249   0.002380   2.206  0.02882 *  \nFACTOR3         0.003495   0.001289   2.711  0.00744 ** \nETHNICITYBlack  0.016600   0.065962   0.252  0.80163    \nETHNICITYMixed -0.016080   0.048371  -0.332  0.74000    \nETHNICITYOther  0.083201   0.108382   0.768  0.44381    \nETHNICITYWhite -0.001006   0.028308  -0.036  0.97168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1043 on 161 degrees of freedom\nMultiple R-squared:  0.2514,    Adjusted R-squared:  0.2188 \nF-statistic: 7.723 on 7 and 161 DF,  p-value: 4.841e-08\n\n\n\nA.20. The effect of ethnicity (ETHNICITY) on mean accuracy of understanding is not significant.\nQ.21. Can you report the model and the model fit statistics?\nA.21. You can report the model and model statistics like this.\n\n\nWe fitted a linear model with mean comprehension accuracy as the outcome and health literacy (HLVA), vocabulary (SHIPLEY), reading strategy (FACTOR3), and ethnicity (ETHNICITY) as predictors. The model is significant overall, with F(7, 161) = 7.72, p&lt; .001, and explains 22% of variance (adjusted R2 = 0.22).\n\n\nQ.22. What changes, when you compare the models with versus without ETHNICITY?\nA.22. If you compare the summaries, for the last two models, you can see that the proportion of variance explained, R-sq, decreases slightly to 22% (Adjusted R-squared = 0.2188), suggesting that adding ETHNICITY as a predictor does not help to predict response accuracy in tests of comprehension of health advice.\n\n\n\nWhy are we learning how to do this?\nR handles factors, by default, by picking one level (here, Asian) as the reference level (or baseline) and comparing outcomes to that baseline, for each other factor level (here, Other).\n\nThus, in this model, the effect of ETHNICITY is estimated as the difference in mean.acc outcome for Asian compared to participants coded as Black, Mixed, Other, White (BAME).\n\nThere are different ways to code factors for analysis. You will learn about these in second year classes.\n\n\n\nTask 13 – Fit a linear model including both numeric variables and categorical variables as predictors, and then plot the predicted effect of the factor (the categorical variable)\n\nHint: Task 12 – We first fit the model, including ETHNICITY, then use the ggpredict() function to get the predictions\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + ETHNICITY, \n            data = study.one.gen)\n\ndat &lt;- ggpredict(model, \"ETHNICITY\")\n\nSome of the focal terms are of type `character`. This may lead to\n  unexpected results. It is recommended to convert these variables to\n  factors before fitting the model.\n  The following variables are of type character: `ETHNICITY`\n\nplot(dat)\n\n\n\n\n\n\n\n\n\nQ.23. Compare the model summary and the prediction plot: what do they show you about the effect of ETHNICITY?\nA.23. If you compare the summary and the plot you can see that:\nthere are some differences in accuracy between people coded as belonging to different ethnic groups;\nbut these differences are very small and are not significant.\n\nNotice that the points in the plot represent model predictions of the average mean.acc accuracy of response for each group.\n\nThe vertical lines on the point represent uncertainty about those estimates and that uncertainty can be seen to be substantial.\nLonger lines represent more uncertainty."
  }
]